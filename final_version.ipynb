{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TMqMNcJFvSAf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMqMNcJFvSAf",
        "outputId": "c1db4ed4-9bb9-4d5e-9d5f-ed6e2c76f7ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: moabb in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.14.0)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.5.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: docstring-inheritance in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: PyYAML<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (6.0.2)\n",
            "Requirement already satisfied: coverage<8.0.0,>=7.0.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (7.9.2)\n",
            "Requirement already satisfied: edfio<0.5.0,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.4.9)\n",
            "Requirement already satisfied: edflib-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.0.8)\n",
            "Requirement already satisfied: memory-profiler<0.62.0,>=0.61.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.61.0)\n",
            "Requirement already satisfied: mne-bids>=0.14 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.16.0)\n",
            "Requirement already satisfied: pooch<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.8.2)\n",
            "Requirement already satisfied: pyriemann<0.8,>=0.7 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn<1.6 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.5.2)\n",
            "Requirement already satisfied: seaborn<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.12.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.15 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.26.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory-profiler<0.62.0,>=0.61.0->moabb) (5.9.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch<2.0.0,>=1.6.0->moabb) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6->moabb) (3.6.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->braindecode) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne->braindecode) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install PyWavelets braindecode moabb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c0474b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from numpy import multiply\n",
        "from braindecode.datasets import MOABBDataset\n",
        "from braindecode.preprocessing import Preprocessor, preprocess, exponential_moving_standardize, create_windows_from_events, SetEEGReference\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff475af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff475af7",
        "outputId": "6e6128fc-1df3-494e-a411-a54d8303d574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A01T.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 42.8M/42.8M [00:00<00:00, 42.8GB/s]\n",
            "SHA256 hash of downloaded file: 054f02e70cf9c4ada1517e9b9864f45407939c1062c6793516585c6f511d0325\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A01E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 43.8M/43.8M [00:00<00:00, 33.6GB/s]\n",
            "SHA256 hash of downloaded file: 53d415f39c3d7b0c88b894d7b08d99bcdfe855ede63831d3691af1a45607fb62\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A02T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 1 → {'accuracy': 0.9824561403508771, 'precision': 0.9666666666666667, 'recall': 1.0, 'specificity': 0.9642857142857143, 'kappa': 0.9648798521256932}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 43.1M/43.1M [00:00<00:00, 43.4GB/s]\n",
            "SHA256 hash of downloaded file: 5ddd5cb520b1692c3ba1363f48d98f58f0e46f3699ee50d749947950fc39db27\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A02E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 44.2M/44.2M [00:00<00:00, 43.7GB/s]\n",
            "SHA256 hash of downloaded file: d63c454005d3a9b41d8440629482e855afc823339bdd0b5721842a7ee9cc7b12\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A03T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 2 → {'accuracy': 0.7894736842105263, 'precision': 0.8148148148148148, 'recall': 0.7586206896551724, 'specificity': 0.8214285714285714, 'kappa': 0.5793357933579336}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 44.1M/44.1M [00:00<00:00, 21.9GB/s]\n",
            "SHA256 hash of downloaded file: 7e731ee8b681d5da6ecb11ae1d4e64b1653c7f15aad5d6b7620b25ce53141e80\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A03E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 42.3M/42.3M [00:00<00:00, 21.1GB/s]\n",
            "SHA256 hash of downloaded file: d4229267ec7624fa8bd3af5cbebac17f415f7c722de6cb676748f8cb3b717d97\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A04T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 3 → {'accuracy': 0.9649122807017544, 'precision': 0.9655172413793104, 'recall': 0.9655172413793104, 'specificity': 0.9642857142857143, 'kappa': 0.9298029556650247}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#############################################| 37.2M/37.2M [00:00<?, ?B/s]\n",
            "SHA256 hash of downloaded file: 15850d81b95fc88cc8b9589eb9b713d49fa071e28adaf32d675b3eaa30591d6e\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A04E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 41.7M/41.7M [00:00<00:00, 19.6GB/s]\n",
            "SHA256 hash of downloaded file: 81916dff2c12997974ba50ffc311da006ea66e525010d010765f0047e771c86a\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A05T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 4 → {'accuracy': 0.8245614035087719, 'precision': 0.8518518518518519, 'recall': 0.7931034482758621, 'specificity': 0.8571428571428571, 'kappa': 0.6494464944649447}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#############################################| 42.5M/42.5M [00:00<?, ?B/s]\n",
            "SHA256 hash of downloaded file: 77387d3b669f4ed9a7c1dac4dcba4c2c40c8910bae20fb961bb7cf5a94912950\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A05E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 44.4M/44.4M [00:00<00:00, 44.1GB/s]\n",
            "SHA256 hash of downloaded file: 8b357470865610c28b2f1d351beac247a56a856f02b2859d650736eb2ef77808\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A06T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 5 → {'accuracy': 0.7192982456140351, 'precision': 0.6857142857142857, 'recall': 0.8275862068965517, 'specificity': 0.6071428571428571, 'kappa': 0.4363411619283065}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 44.6M/44.6M [00:00<00:00, 22.3GB/s]\n",
            "SHA256 hash of downloaded file: 4dc3be1b0d60279134d1220323c73c68cf73799339a7fb224087a3c560a9a7e2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A06E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 43.4M/43.4M [00:00<00:00, 33.3GB/s]\n",
            "SHA256 hash of downloaded file: bf67a40621b74b6af7a986c2f6edfff7fc2bbbca237aadd07b575893032998d1\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A07T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 6 → {'accuracy': 0.8070175438596491, 'precision': 0.7931034482758621, 'recall': 0.8214285714285714, 'specificity': 0.7931034482758621, 'kappa': 0.6141538461538462}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 42.8M/42.8M [00:00<00:00, 41.1GB/s]\n",
            "SHA256 hash of downloaded file: 43b6bbef0be78f0ac2b66cb2d9679091f1f5b7f0a5d4ebef73d2c7cc8e11aa96\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A07E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 42.2M/42.2M [00:00<00:00, 41.9GB/s]\n",
            "SHA256 hash of downloaded file: b9aaec73dcee002fab84ee98e938039a67bf6a3cbf4fc86d5d8df198cfe4c323\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A08T.mat'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100\n",
            "Subject 7 → {'accuracy': 0.9298245614035088, 'precision': 0.9285714285714286, 'recall': 0.9285714285714286, 'specificity': 0.9310344827586207, 'kappa': 0.8596059113300493}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 45.0M/45.0M [00:00<00:00, 32.5GB/s]\n",
            "SHA256 hash of downloaded file: 7a4b3bd602d5bc307d3f4527fca2cf076659e94aca584dd64f6286fd413a82f2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat' to file 'C:\\Users\\alira\\mne_data\\MNE-bnci-data\\database\\data-sets\\001-2014\\A08E.mat'.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lampx.tugraz.at'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "100%|#####################################| 46.3M/46.3M [00:00<00:00, 45.7GB/s]\n",
            "SHA256 hash of downloaded file: 0eedbd89790c7d621c8eef68065ddecf80d437bbbcf60321d9253e2305f294f7\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10\n",
            "Epoch 20\n",
            "Epoch 30\n",
            "Epoch 40\n",
            "Epoch 50\n",
            "Epoch 60\n",
            "Epoch 70\n",
            "Epoch 80\n",
            "Epoch 90\n",
            "Epoch 100\n",
            "Subject 8 → {'accuracy': 0.9122807017543859, 'precision': 0.9285714285714286, 'recall': 0.896551724137931, 'specificity': 0.9285714285714286, 'kappa': 0.8246153846153846}\n",
            "\n",
            "Average Results:\n",
            "accuracy: 0.8662\n",
            "precision: 0.8669\n",
            "recall: 0.8739\n",
            "specificity: 0.8584\n",
            "kappa: 0.7323\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ---------- SpiTranNet  ----------\n",
        "\n",
        "class SpikingNeuronCell(nn.Module):\n",
        "    def __init__(self, threshold=0.3, decay=0.9, temp=1.2):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.decay = decay\n",
        "        self.temp = temp\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not x.requires_grad:\n",
        "           x = x.clone().detach().requires_grad_()\n",
        "\n",
        "        mem_pot = x * self.decay\n",
        "        sigmoid = torch.sigmoid((mem_pot - self.threshold) * self.temp)\n",
        "        spike = torch.where(mem_pot > self.threshold,\n",
        "                            torch.ones_like(mem_pot),\n",
        "                            torch.zeros_like(mem_pot))\n",
        "\n",
        "\n",
        "        spike = spike.clone().detach().requires_grad_()\n",
        "\n",
        "        surrogate_grad = sigmoid * (1.0 - sigmoid) * self.temp\n",
        "        spike.register_hook(lambda grad: grad * surrogate_grad)\n",
        "\n",
        "        return spike\n",
        "\n",
        "\n",
        "\n",
        "class SpikingMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.spike = SpikingNeuronCell()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        return self.spike(attn_output)\n",
        "\n",
        "def positional_encoding(seq_len, d_model, device):\n",
        "    pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    i = torch.arange(d_model // 2, dtype=torch.float32, device=device)\n",
        "    angle_rates = 1 / (10000 ** (2 * i / d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    sin = torch.sin(angle_rads)\n",
        "    cos = torch.cos(angle_rads)\n",
        "    pos_encoding = torch.cat([sin, cos], dim=-1)\n",
        "    return pos_encoding.unsqueeze(0)  # shape: (1, seq_len, d_model)\n",
        "\n",
        "class SpiTranNet(nn.Module):\n",
        "    def __init__(self, input_channels=22, input_length=1000, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.input_length = input_length\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, padding=3)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=4)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=7, padding=3)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, 128, kernel_size=7, padding=3)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=4)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.seq_len = input_length // 64\n",
        "        self.transformer_dim = 128\n",
        "        self.attn = SpikingMultiHeadAttention(embed_dim=self.transformer_dim, num_heads=2)\n",
        "        self.norm1 = nn.LayerNorm(self.transformer_dim)\n",
        "        self.norm2 = nn.LayerNorm(self.transformer_dim)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.transformer_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            SpikingNeuronCell(),\n",
        "            nn.Linear(128, self.transformer_dim)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.seq_len * self.transformer_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        pos_enc = positional_encoding(x.size(1), x.size(2), x.device)\n",
        "        x = x + pos_enc\n",
        "\n",
        "        residual = x\n",
        "        x = self.attn(x)\n",
        "        x = self.norm1(x + residual)\n",
        "\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm2(x + residual)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ---------- general setting ----------\n",
        "sr = 250\n",
        "input_length = 1000\n",
        "input_channels = 22\n",
        "num_classes = 2\n",
        "\n",
        "def load_dataframe(subject_id):\n",
        "    dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])\n",
        "\n",
        "    low_cut_hz = 8.\n",
        "    high_cut_hz = 30.\n",
        "    factor = 1e6\n",
        "    factor_new = 1e-3\n",
        "    init_block_size = 1000\n",
        "\n",
        "    preprocessors = [\n",
        "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),\n",
        "        Preprocessor(lambda data: data * factor),\n",
        "        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
        "        Preprocessor(exponential_moving_standardize, factor_new=factor_new, init_block_size=init_block_size),\n",
        "        SetEEGReference()\n",
        "    ]\n",
        "\n",
        "    preprocess(dataset, preprocessors, n_jobs=-1)\n",
        "    windows_dataset = create_windows_from_events(dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0, preload=True)\n",
        "    XYSet = [windows_dataset.split('session')['0train']]\n",
        "    XYSet_ = [windows_dataset.split('session')['1test']]\n",
        "    return XYSet, XYSet_\n",
        "\n",
        "def extract(XYSet, selected_classes=[0, 1]):\n",
        "    X, Y = [], []\n",
        "    for ds in XYSet:\n",
        "        for window in ds:\n",
        "            x = window[0]\n",
        "            y = window[1]\n",
        "            if isinstance(y, list):\n",
        "                y = y[0]\n",
        "            if int(y) in selected_classes:\n",
        "                X.append(x)\n",
        "                Y.append(int(y))\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def fp(x, y, device):\n",
        "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "    return x, y\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(y_batch.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, average='binary')\n",
        "    rec = recall_score(y_true, y_pred, average='binary')\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    tn, fp_, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    spec = tn / (tn + fp_) if (tn + fp_) > 0 else 0\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"specificity\": spec, \"kappa\": kappa}\n",
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs=100, lr=1e-4, device=\"cuda\"):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    best = {\"accuracy\": 0}\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X_batch), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}\")\n",
        "        metrics = evaluate_model(model, test_loader)\n",
        "        if metrics[\"accuracy\"] > best[\"accuracy\"]:\n",
        "            best = metrics\n",
        "    return best\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    results = []\n",
        "    for i in range(1, 9):\n",
        "        XYSet, XYSet_ = load_dataframe(i)\n",
        "        XTrain, YTrain = extract(XYSet)\n",
        "        XTest, YTest = extract(XYSet_)\n",
        "\n",
        "        XTrain = XTrain.transpose(0, 1, 2) if XTrain.shape[1] < XTrain.shape[2] else XTrain\n",
        "        XTest = XTest.transpose(0, 1, 2) if XTest.shape[1] < XTest.shape[2] else XTest\n",
        "\n",
        "        X_train, y_train = fp(XTrain, YTrain, device)\n",
        "        X_test, y_test = fp(XTest, YTest, device)\n",
        "\n",
        "        X_test_final, X_extra_train, y_test_final, y_extra_train = train_test_split(X_test.cpu().numpy(), y_test.cpu().numpy(), test_size=0.6, stratify=y_test.cpu().numpy())\n",
        "\n",
        "        X_train = torch.cat([X_train, torch.tensor(X_extra_train).to(device)])\n",
        "        y_train = torch.cat([y_train, torch.tensor(y_extra_train).to(device)])\n",
        "        X_test = torch.tensor(X_test_final).to(device)\n",
        "        y_test = torch.tensor(y_test_final).to(device)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
        "        test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=16, shuffle=False)\n",
        "\n",
        "        model = SpiTranNet(input_channels=22, input_length=1000, num_classes=2).to(device)\n",
        "        metrics = train_model(model, train_loader, test_loader, device=device)\n",
        "        print(f\"Subject {i} → {metrics}\")\n",
        "        results.append(metrics)\n",
        "\n",
        "    print(\"\\nAverage Results:\")\n",
        "    for k in results[0]:\n",
        "        avg = sum(r[k] for r in results) / len(results)\n",
        "        print(f\"{k}: {avg:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f664a22",
      "metadata": {},
      "source": [
        "# Second Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53335249",
      "metadata": {},
      "source": [
        "# main code :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2220aea",
      "metadata": {},
      "source": [
        "# both Subject-Wise And all-Subject Training with Metrics Calculation and Visualizations (9subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "343b3318",
      "metadata": {},
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a1461425",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Subject 1 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7079 val_loss=0.6924 train_acc=0.4931 val_acc=0.5972 train_kappa=-0.0139 val_kappa=0.1944\n",
            "[Epoch 2/100] loss=0.7142 val_loss=0.6872 train_acc=0.5278 val_acc=0.5556 train_kappa=0.0556 val_kappa=0.1111\n",
            "[Epoch 3/100] loss=0.7178 val_loss=0.6830 train_acc=0.4792 val_acc=0.5000 train_kappa=-0.0417 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.6843 val_loss=0.6708 train_acc=0.5208 val_acc=0.5139 train_kappa=0.0417 val_kappa=0.0278\n",
            "[Epoch 5/100] loss=0.6922 val_loss=0.6567 train_acc=0.5625 val_acc=0.7917 train_kappa=0.1250 val_kappa=0.5833\n",
            "[Epoch 6/100] loss=0.6702 val_loss=0.6547 train_acc=0.6181 val_acc=0.5347 train_kappa=0.2361 val_kappa=0.0694\n",
            "[Epoch 7/100] loss=0.6922 val_loss=0.6410 train_acc=0.5069 val_acc=0.7986 train_kappa=0.0139 val_kappa=0.5972\n",
            "[Epoch 8/100] loss=0.6923 val_loss=0.6443 train_acc=0.5694 val_acc=0.6458 train_kappa=0.1389 val_kappa=0.2917\n",
            "[Epoch 9/100] loss=0.6700 val_loss=0.6224 train_acc=0.5694 val_acc=0.7778 train_kappa=0.1389 val_kappa=0.5556\n",
            "[Epoch 10/100] loss=0.6345 val_loss=0.6206 train_acc=0.6944 val_acc=0.7917 train_kappa=0.3889 val_kappa=0.5833\n",
            "[Epoch 11/100] loss=0.6515 val_loss=0.6118 train_acc=0.6319 val_acc=0.8125 train_kappa=0.2639 val_kappa=0.6250\n",
            "[Epoch 12/100] loss=0.6367 val_loss=0.6046 train_acc=0.6389 val_acc=0.8056 train_kappa=0.2778 val_kappa=0.6111\n",
            "[Epoch 13/100] loss=0.6750 val_loss=0.6042 train_acc=0.6111 val_acc=0.7847 train_kappa=0.2222 val_kappa=0.5694\n",
            "[Epoch 14/100] loss=0.6421 val_loss=0.5894 train_acc=0.6528 val_acc=0.7986 train_kappa=0.3056 val_kappa=0.5972\n",
            "[Epoch 15/100] loss=0.6406 val_loss=0.5835 train_acc=0.6458 val_acc=0.8125 train_kappa=0.2917 val_kappa=0.6250\n",
            "[Epoch 16/100] loss=0.6337 val_loss=0.5679 train_acc=0.6458 val_acc=0.8264 train_kappa=0.2917 val_kappa=0.6528\n",
            "[Epoch 17/100] loss=0.6226 val_loss=0.5553 train_acc=0.6944 val_acc=0.7917 train_kappa=0.3889 val_kappa=0.5833\n",
            "[Epoch 18/100] loss=0.6393 val_loss=0.5347 train_acc=0.6389 val_acc=0.8403 train_kappa=0.2778 val_kappa=0.6806\n",
            "[Epoch 19/100] loss=0.6011 val_loss=0.4943 train_acc=0.6806 val_acc=0.8194 train_kappa=0.3611 val_kappa=0.6389\n",
            "[Epoch 20/100] loss=0.6218 val_loss=0.4756 train_acc=0.6528 val_acc=0.8264 train_kappa=0.3056 val_kappa=0.6528\n",
            "[Epoch 21/100] loss=0.5818 val_loss=0.4714 train_acc=0.6597 val_acc=0.8056 train_kappa=0.3194 val_kappa=0.6111\n",
            "[Epoch 22/100] loss=0.5716 val_loss=0.4589 train_acc=0.7083 val_acc=0.7917 train_kappa=0.4167 val_kappa=0.5833\n",
            "[Epoch 23/100] loss=0.5624 val_loss=0.4416 train_acc=0.7431 val_acc=0.8125 train_kappa=0.4861 val_kappa=0.6250\n",
            "[Epoch 24/100] loss=0.5625 val_loss=0.4203 train_acc=0.7014 val_acc=0.8333 train_kappa=0.4028 val_kappa=0.6667\n",
            "[Epoch 25/100] loss=0.5578 val_loss=0.4400 train_acc=0.7639 val_acc=0.8611 train_kappa=0.5278 val_kappa=0.7222\n",
            "[Epoch 26/100] loss=0.4661 val_loss=0.4254 train_acc=0.8333 val_acc=0.8403 train_kappa=0.6667 val_kappa=0.6806\n",
            "[Epoch 27/100] loss=0.4946 val_loss=0.4148 train_acc=0.8194 val_acc=0.8403 train_kappa=0.6389 val_kappa=0.6806\n",
            "[Epoch 28/100] loss=0.4484 val_loss=0.3845 train_acc=0.8403 val_acc=0.8472 train_kappa=0.6806 val_kappa=0.6944\n",
            "[Epoch 29/100] loss=0.4321 val_loss=0.3702 train_acc=0.8333 val_acc=0.8264 train_kappa=0.6667 val_kappa=0.6528\n",
            "[Epoch 30/100] loss=0.4363 val_loss=0.3421 train_acc=0.8472 val_acc=0.8264 train_kappa=0.6944 val_kappa=0.6528\n",
            "[Epoch 31/100] loss=0.4169 val_loss=0.3262 train_acc=0.8056 val_acc=0.8472 train_kappa=0.6111 val_kappa=0.6944\n",
            "[Epoch 32/100] loss=0.4134 val_loss=0.3008 train_acc=0.8264 val_acc=0.8472 train_kappa=0.6528 val_kappa=0.6944\n",
            "[Epoch 33/100] loss=0.3520 val_loss=0.2849 train_acc=0.8681 val_acc=0.8542 train_kappa=0.7361 val_kappa=0.7083\n",
            "[Epoch 34/100] loss=0.3668 val_loss=0.2816 train_acc=0.8681 val_acc=0.8542 train_kappa=0.7361 val_kappa=0.7083\n",
            "[Epoch 35/100] loss=0.2952 val_loss=0.2928 train_acc=0.8958 val_acc=0.8681 train_kappa=0.7917 val_kappa=0.7361\n",
            "[Epoch 36/100] loss=0.3430 val_loss=0.2515 train_acc=0.8542 val_acc=0.8750 train_kappa=0.7083 val_kappa=0.7500\n",
            "[Epoch 37/100] loss=0.3003 val_loss=0.2311 train_acc=0.8750 val_acc=0.8819 train_kappa=0.7500 val_kappa=0.7639\n",
            "[Epoch 38/100] loss=0.2787 val_loss=0.2150 train_acc=0.8889 val_acc=0.8819 train_kappa=0.7778 val_kappa=0.7639\n",
            "[Epoch 39/100] loss=0.2570 val_loss=0.2119 train_acc=0.9236 val_acc=0.8819 train_kappa=0.8472 val_kappa=0.7639\n",
            "[Epoch 40/100] loss=0.2682 val_loss=0.2113 train_acc=0.9028 val_acc=0.9375 train_kappa=0.8056 val_kappa=0.8750\n",
            "[Epoch 41/100] loss=0.2232 val_loss=0.1865 train_acc=0.9167 val_acc=0.9028 train_kappa=0.8333 val_kappa=0.8056\n",
            "[Epoch 42/100] loss=0.2489 val_loss=0.1910 train_acc=0.9167 val_acc=0.9306 train_kappa=0.8333 val_kappa=0.8611\n",
            "[Epoch 43/100] loss=0.2598 val_loss=0.1743 train_acc=0.8958 val_acc=0.9097 train_kappa=0.7917 val_kappa=0.8194\n",
            "[Epoch 44/100] loss=0.1949 val_loss=0.1787 train_acc=0.9236 val_acc=0.9375 train_kappa=0.8472 val_kappa=0.8750\n",
            "[Epoch 45/100] loss=0.2180 val_loss=0.1621 train_acc=0.9375 val_acc=0.9236 train_kappa=0.8750 val_kappa=0.8472\n",
            "[Epoch 46/100] loss=0.2117 val_loss=0.1682 train_acc=0.9167 val_acc=0.9444 train_kappa=0.8333 val_kappa=0.8889\n",
            "[Epoch 47/100] loss=0.2053 val_loss=0.1514 train_acc=0.9306 val_acc=0.9375 train_kappa=0.8611 val_kappa=0.8750\n",
            "[Epoch 48/100] loss=0.1538 val_loss=0.1646 train_acc=0.9653 val_acc=0.9306 train_kappa=0.9306 val_kappa=0.8611\n",
            "[Epoch 49/100] loss=0.1889 val_loss=0.1701 train_acc=0.9097 val_acc=0.9306 train_kappa=0.8194 val_kappa=0.8611\n",
            "[Epoch 50/100] loss=0.1778 val_loss=0.1587 train_acc=0.9236 val_acc=0.9306 train_kappa=0.8472 val_kappa=0.8611\n",
            "[Epoch 51/100] loss=0.1743 val_loss=0.1653 train_acc=0.9514 val_acc=0.9097 train_kappa=0.9028 val_kappa=0.8194\n",
            "[Epoch 52/100] loss=0.1319 val_loss=0.1660 train_acc=0.9375 val_acc=0.9236 train_kappa=0.8750 val_kappa=0.8472\n",
            "[Epoch 53/100] loss=0.1716 val_loss=0.1540 train_acc=0.9375 val_acc=0.9236 train_kappa=0.8750 val_kappa=0.8472\n",
            "[Epoch 54/100] loss=0.2015 val_loss=0.1404 train_acc=0.9306 val_acc=0.9375 train_kappa=0.8611 val_kappa=0.8750\n",
            "[Epoch 55/100] loss=0.2018 val_loss=0.1384 train_acc=0.9375 val_acc=0.9444 train_kappa=0.8750 val_kappa=0.8889\n",
            "[Epoch 56/100] loss=0.1923 val_loss=0.1322 train_acc=0.9097 val_acc=0.9514 train_kappa=0.8194 val_kappa=0.9028\n",
            "[Epoch 57/100] loss=0.1628 val_loss=0.1297 train_acc=0.9306 val_acc=0.9514 train_kappa=0.8611 val_kappa=0.9028\n",
            "[Epoch 58/100] loss=0.1610 val_loss=0.1414 train_acc=0.9375 val_acc=0.9375 train_kappa=0.8750 val_kappa=0.8750\n",
            "[Epoch 59/100] loss=0.1364 val_loss=0.1530 train_acc=0.9444 val_acc=0.9514 train_kappa=0.8889 val_kappa=0.9028\n",
            "[Epoch 60/100] loss=0.1820 val_loss=0.1322 train_acc=0.9306 val_acc=0.9444 train_kappa=0.8611 val_kappa=0.8889\n",
            "[Epoch 61/100] loss=0.1105 val_loss=0.1320 train_acc=0.9514 val_acc=0.9653 train_kappa=0.9028 val_kappa=0.9306\n",
            "[Epoch 62/100] loss=0.1377 val_loss=0.1387 train_acc=0.9444 val_acc=0.9375 train_kappa=0.8889 val_kappa=0.8750\n",
            "[Epoch 63/100] loss=0.0843 val_loss=0.1282 train_acc=0.9861 val_acc=0.9583 train_kappa=0.9722 val_kappa=0.9167\n",
            "[Epoch 64/100] loss=0.1196 val_loss=0.1293 train_acc=0.9444 val_acc=0.9583 train_kappa=0.8889 val_kappa=0.9167\n",
            "[Epoch 65/100] loss=0.1087 val_loss=0.1492 train_acc=0.9653 val_acc=0.9444 train_kappa=0.9306 val_kappa=0.8889\n",
            "[Epoch 66/100] loss=0.0988 val_loss=0.1360 train_acc=0.9583 val_acc=0.9653 train_kappa=0.9167 val_kappa=0.9306\n",
            "[Epoch 67/100] loss=0.1225 val_loss=0.1498 train_acc=0.9653 val_acc=0.9375 train_kappa=0.9306 val_kappa=0.8750\n",
            "[Epoch 68/100] loss=0.0904 val_loss=0.1346 train_acc=0.9653 val_acc=0.9444 train_kappa=0.9306 val_kappa=0.8889\n",
            "[Epoch 69/100] loss=0.1038 val_loss=0.1260 train_acc=0.9653 val_acc=0.9583 train_kappa=0.9306 val_kappa=0.9167\n",
            "[Epoch 70/100] loss=0.0893 val_loss=0.1278 train_acc=0.9583 val_acc=0.9514 train_kappa=0.9167 val_kappa=0.9028\n",
            "[Epoch 71/100] loss=0.0775 val_loss=0.1238 train_acc=0.9792 val_acc=0.9514 train_kappa=0.9583 val_kappa=0.9028\n",
            "[Epoch 72/100] loss=0.0789 val_loss=0.1261 train_acc=0.9861 val_acc=0.9583 train_kappa=0.9722 val_kappa=0.9167\n",
            "[Epoch 73/100] loss=0.0741 val_loss=0.1602 train_acc=0.9722 val_acc=0.9444 train_kappa=0.9444 val_kappa=0.8889\n",
            "[Epoch 74/100] loss=0.0755 val_loss=0.1344 train_acc=0.9792 val_acc=0.9583 train_kappa=0.9583 val_kappa=0.9167\n",
            "[Epoch 75/100] loss=0.0929 val_loss=0.1699 train_acc=0.9514 val_acc=0.9444 train_kappa=0.9028 val_kappa=0.8889\n",
            "[Epoch 76/100] loss=0.1026 val_loss=0.1442 train_acc=0.9583 val_acc=0.9583 train_kappa=0.9167 val_kappa=0.9167\n",
            "[Epoch 77/100] loss=0.0930 val_loss=0.1603 train_acc=0.9583 val_acc=0.9583 train_kappa=0.9167 val_kappa=0.9167\n",
            "[Epoch 78/100] loss=0.0716 val_loss=0.1468 train_acc=0.9722 val_acc=0.9653 train_kappa=0.9444 val_kappa=0.9306\n",
            "[Epoch 79/100] loss=0.0568 val_loss=0.1504 train_acc=0.9931 val_acc=0.9583 train_kappa=0.9861 val_kappa=0.9167\n",
            "[Epoch 80/100] loss=0.0866 val_loss=0.1581 train_acc=0.9653 val_acc=0.9514 train_kappa=0.9306 val_kappa=0.9028\n",
            "[Epoch 81/100] loss=0.0941 val_loss=0.1452 train_acc=0.9583 val_acc=0.9653 train_kappa=0.9167 val_kappa=0.9306\n",
            "[Epoch 82/100] loss=0.0914 val_loss=0.1401 train_acc=0.9861 val_acc=0.9653 train_kappa=0.9722 val_kappa=0.9306\n",
            "[Epoch 83/100] loss=0.0387 val_loss=0.1497 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 84/100] loss=0.0371 val_loss=0.1635 train_acc=0.9861 val_acc=0.9444 train_kappa=0.9722 val_kappa=0.8889\n",
            "[Epoch 85/100] loss=0.0737 val_loss=0.1370 train_acc=0.9722 val_acc=0.9514 train_kappa=0.9444 val_kappa=0.9028\n",
            "[Epoch 86/100] loss=0.0580 val_loss=0.1362 train_acc=0.9792 val_acc=0.9653 train_kappa=0.9583 val_kappa=0.9306\n",
            "[Epoch 87/100] loss=0.0383 val_loss=0.1413 train_acc=0.9931 val_acc=0.9653 train_kappa=0.9861 val_kappa=0.9306\n",
            "[Epoch 88/100] loss=0.0564 val_loss=0.1444 train_acc=0.9861 val_acc=0.9653 train_kappa=0.9722 val_kappa=0.9306\n",
            "[Epoch 89/100] loss=0.0616 val_loss=0.1388 train_acc=0.9722 val_acc=0.9653 train_kappa=0.9444 val_kappa=0.9306\n",
            "[Epoch 90/100] loss=0.0647 val_loss=0.1342 train_acc=0.9722 val_acc=0.9583 train_kappa=0.9444 val_kappa=0.9167\n",
            "[Epoch 91/100] loss=0.0628 val_loss=0.1470 train_acc=0.9722 val_acc=0.9514 train_kappa=0.9444 val_kappa=0.9028\n",
            "[Epoch 92/100] loss=0.0466 val_loss=0.1463 train_acc=0.9931 val_acc=0.9722 train_kappa=0.9861 val_kappa=0.9444\n",
            "[Epoch 93/100] loss=0.0317 val_loss=0.1389 train_acc=1.0000 val_acc=0.9653 train_kappa=1.0000 val_kappa=0.9306\n",
            "[Epoch 94/100] loss=0.0452 val_loss=0.1464 train_acc=0.9792 val_acc=0.9583 train_kappa=0.9583 val_kappa=0.9167\n",
            "[Epoch 95/100] loss=0.0446 val_loss=0.1400 train_acc=0.9861 val_acc=0.9653 train_kappa=0.9722 val_kappa=0.9306\n",
            "[Epoch 96/100] loss=0.0734 val_loss=0.1416 train_acc=0.9722 val_acc=0.9583 train_kappa=0.9444 val_kappa=0.9167\n",
            "[Epoch 97/100] loss=0.0311 val_loss=0.1904 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "[Epoch 98/100] loss=0.0518 val_loss=0.1509 train_acc=0.9792 val_acc=0.9583 train_kappa=0.9583 val_kappa=0.9167\n",
            "[Epoch 99/100] loss=0.0355 val_loss=0.1918 train_acc=1.0000 val_acc=0.9514 train_kappa=1.0000 val_kappa=0.9028\n",
            "[Epoch 100/100] loss=0.0267 val_loss=0.1523 train_acc=1.0000 val_acc=0.9583 train_kappa=1.0000 val_kappa=0.9167\n",
            "\n",
            "=== Processing Subject 2 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7039 val_loss=0.6952 train_acc=0.5208 val_acc=0.5000 train_kappa=0.0417 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7030 val_loss=0.6953 train_acc=0.5139 val_acc=0.5000 train_kappa=0.0278 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.6991 val_loss=0.6936 train_acc=0.5069 val_acc=0.5139 train_kappa=0.0139 val_kappa=0.0278\n",
            "[Epoch 4/100] loss=0.7067 val_loss=0.6910 train_acc=0.5208 val_acc=0.5278 train_kappa=0.0417 val_kappa=0.0556\n",
            "[Epoch 5/100] loss=0.7141 val_loss=0.7011 train_acc=0.4792 val_acc=0.5000 train_kappa=-0.0417 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.7008 val_loss=0.6960 train_acc=0.5347 val_acc=0.5069 train_kappa=0.0694 val_kappa=0.0139\n",
            "[Epoch 7/100] loss=0.7011 val_loss=0.6891 train_acc=0.5069 val_acc=0.5625 train_kappa=0.0139 val_kappa=0.1250\n",
            "[Epoch 8/100] loss=0.6879 val_loss=0.6857 train_acc=0.5625 val_acc=0.5556 train_kappa=0.1250 val_kappa=0.1111\n",
            "[Epoch 9/100] loss=0.7046 val_loss=0.6853 train_acc=0.4861 val_acc=0.5278 train_kappa=-0.0278 val_kappa=0.0556\n",
            "[Epoch 10/100] loss=0.6935 val_loss=0.6889 train_acc=0.4861 val_acc=0.5208 train_kappa=-0.0278 val_kappa=0.0417\n",
            "[Epoch 11/100] loss=0.6812 val_loss=0.6875 train_acc=0.5764 val_acc=0.5278 train_kappa=0.1528 val_kappa=0.0556\n",
            "[Epoch 12/100] loss=0.6880 val_loss=0.6900 train_acc=0.5764 val_acc=0.5208 train_kappa=0.1528 val_kappa=0.0417\n",
            "[Epoch 13/100] loss=0.6973 val_loss=0.6843 train_acc=0.5069 val_acc=0.5694 train_kappa=0.0139 val_kappa=0.1389\n",
            "[Epoch 14/100] loss=0.7044 val_loss=0.6980 train_acc=0.4931 val_acc=0.5000 train_kappa=-0.0139 val_kappa=0.0000\n",
            "[Epoch 15/100] loss=0.6840 val_loss=0.6901 train_acc=0.5486 val_acc=0.5764 train_kappa=0.0972 val_kappa=0.1528\n",
            "[Epoch 16/100] loss=0.6915 val_loss=0.6963 train_acc=0.5000 val_acc=0.5347 train_kappa=0.0000 val_kappa=0.0694\n",
            "[Epoch 17/100] loss=0.6818 val_loss=0.6888 train_acc=0.5694 val_acc=0.5278 train_kappa=0.1389 val_kappa=0.0556\n",
            "[Epoch 18/100] loss=0.6817 val_loss=0.6853 train_acc=0.5417 val_acc=0.5278 train_kappa=0.0833 val_kappa=0.0556\n",
            "[Epoch 19/100] loss=0.6772 val_loss=0.7042 train_acc=0.5972 val_acc=0.5000 train_kappa=0.1944 val_kappa=0.0000\n",
            "[Epoch 20/100] loss=0.6990 val_loss=0.6964 train_acc=0.5069 val_acc=0.5000 train_kappa=0.0139 val_kappa=0.0000\n",
            "[Epoch 21/100] loss=0.6736 val_loss=0.6813 train_acc=0.5694 val_acc=0.5347 train_kappa=0.1389 val_kappa=0.0694\n",
            "[Epoch 22/100] loss=0.6820 val_loss=0.6807 train_acc=0.5625 val_acc=0.5417 train_kappa=0.1250 val_kappa=0.0833\n",
            "[Epoch 23/100] loss=0.6897 val_loss=0.6862 train_acc=0.5000 val_acc=0.5556 train_kappa=0.0000 val_kappa=0.1111\n",
            "[Epoch 24/100] loss=0.6653 val_loss=0.6883 train_acc=0.5903 val_acc=0.5625 train_kappa=0.1806 val_kappa=0.1250\n",
            "[Epoch 25/100] loss=0.6683 val_loss=0.6836 train_acc=0.5903 val_acc=0.5278 train_kappa=0.1806 val_kappa=0.0556\n",
            "[Epoch 26/100] loss=0.6664 val_loss=0.6818 train_acc=0.5833 val_acc=0.5556 train_kappa=0.1667 val_kappa=0.1111\n",
            "[Epoch 27/100] loss=0.6536 val_loss=0.6851 train_acc=0.5903 val_acc=0.5556 train_kappa=0.1806 val_kappa=0.1111\n",
            "[Epoch 28/100] loss=0.6420 val_loss=0.7097 train_acc=0.5833 val_acc=0.5139 train_kappa=0.1667 val_kappa=0.0278\n",
            "[Epoch 29/100] loss=0.6814 val_loss=0.7040 train_acc=0.5556 val_acc=0.5625 train_kappa=0.1111 val_kappa=0.1250\n",
            "[Epoch 30/100] loss=0.6909 val_loss=0.6878 train_acc=0.5208 val_acc=0.5486 train_kappa=0.0417 val_kappa=0.0972\n",
            "[Epoch 31/100] loss=0.6513 val_loss=0.7076 train_acc=0.6042 val_acc=0.5486 train_kappa=0.2083 val_kappa=0.0972\n",
            "[Epoch 32/100] loss=0.6612 val_loss=0.7038 train_acc=0.6319 val_acc=0.5556 train_kappa=0.2639 val_kappa=0.1111\n",
            "[Epoch 33/100] loss=0.6664 val_loss=0.6764 train_acc=0.6181 val_acc=0.5347 train_kappa=0.2361 val_kappa=0.0694\n",
            "[Epoch 34/100] loss=0.6376 val_loss=0.7176 train_acc=0.6319 val_acc=0.5486 train_kappa=0.2639 val_kappa=0.0972\n",
            "[Epoch 35/100] loss=0.6263 val_loss=0.7197 train_acc=0.6597 val_acc=0.5903 train_kappa=0.3194 val_kappa=0.1806\n",
            "[Epoch 36/100] loss=0.6473 val_loss=0.6997 train_acc=0.6319 val_acc=0.5625 train_kappa=0.2639 val_kappa=0.1250\n",
            "[Epoch 37/100] loss=0.6515 val_loss=0.7365 train_acc=0.5764 val_acc=0.5833 train_kappa=0.1528 val_kappa=0.1667\n",
            "[Epoch 38/100] loss=0.6500 val_loss=0.7809 train_acc=0.6181 val_acc=0.5347 train_kappa=0.2361 val_kappa=0.0694\n",
            "[Epoch 39/100] loss=0.6145 val_loss=0.7366 train_acc=0.6597 val_acc=0.5764 train_kappa=0.3194 val_kappa=0.1528\n",
            "[Epoch 40/100] loss=0.6198 val_loss=0.7248 train_acc=0.6458 val_acc=0.5764 train_kappa=0.2917 val_kappa=0.1528\n",
            "[Epoch 41/100] loss=0.6151 val_loss=0.8098 train_acc=0.7292 val_acc=0.5486 train_kappa=0.4583 val_kappa=0.0972\n",
            "[Epoch 42/100] loss=0.6334 val_loss=0.7696 train_acc=0.6250 val_acc=0.5694 train_kappa=0.2500 val_kappa=0.1389\n",
            "[Epoch 43/100] loss=0.6050 val_loss=0.7245 train_acc=0.6875 val_acc=0.5764 train_kappa=0.3750 val_kappa=0.1528\n",
            "[Epoch 44/100] loss=0.6174 val_loss=0.7650 train_acc=0.6875 val_acc=0.5903 train_kappa=0.3750 val_kappa=0.1806\n",
            "[Epoch 45/100] loss=0.6190 val_loss=0.8297 train_acc=0.6389 val_acc=0.5417 train_kappa=0.2778 val_kappa=0.0833\n",
            "[Epoch 46/100] loss=0.5839 val_loss=0.7523 train_acc=0.7222 val_acc=0.5556 train_kappa=0.4444 val_kappa=0.1111\n",
            "[Epoch 47/100] loss=0.6072 val_loss=0.8156 train_acc=0.6667 val_acc=0.5764 train_kappa=0.3333 val_kappa=0.1528\n",
            "[Epoch 48/100] loss=0.5867 val_loss=0.8148 train_acc=0.7083 val_acc=0.5625 train_kappa=0.4167 val_kappa=0.1250\n",
            "[Epoch 49/100] loss=0.5921 val_loss=0.7434 train_acc=0.6597 val_acc=0.5764 train_kappa=0.3194 val_kappa=0.1528\n",
            "[Epoch 50/100] loss=0.5558 val_loss=0.8289 train_acc=0.7222 val_acc=0.5625 train_kappa=0.4444 val_kappa=0.1250\n",
            "[Epoch 51/100] loss=0.5605 val_loss=0.7708 train_acc=0.7639 val_acc=0.5764 train_kappa=0.5278 val_kappa=0.1528\n",
            "[Epoch 52/100] loss=0.5598 val_loss=0.8194 train_acc=0.7222 val_acc=0.5486 train_kappa=0.4444 val_kappa=0.0972\n",
            "[Epoch 53/100] loss=0.5466 val_loss=0.8978 train_acc=0.7361 val_acc=0.5347 train_kappa=0.4722 val_kappa=0.0694\n",
            "[Epoch 54/100] loss=0.4913 val_loss=0.7776 train_acc=0.8333 val_acc=0.6042 train_kappa=0.6667 val_kappa=0.2083\n",
            "[Epoch 55/100] loss=0.4840 val_loss=0.8583 train_acc=0.7986 val_acc=0.5694 train_kappa=0.5972 val_kappa=0.1389\n",
            "[Epoch 56/100] loss=0.4829 val_loss=0.8651 train_acc=0.8333 val_acc=0.5833 train_kappa=0.6667 val_kappa=0.1667\n",
            "[Epoch 57/100] loss=0.4663 val_loss=0.8547 train_acc=0.7778 val_acc=0.6042 train_kappa=0.5556 val_kappa=0.2083\n",
            "[Epoch 58/100] loss=0.4378 val_loss=0.8826 train_acc=0.8403 val_acc=0.6042 train_kappa=0.6806 val_kappa=0.2083\n",
            "[Epoch 59/100] loss=0.4179 val_loss=0.8937 train_acc=0.8611 val_acc=0.5903 train_kappa=0.7222 val_kappa=0.1806\n",
            "[Epoch 60/100] loss=0.3945 val_loss=0.8243 train_acc=0.8681 val_acc=0.6111 train_kappa=0.7361 val_kappa=0.2222\n",
            "[Epoch 61/100] loss=0.4103 val_loss=0.7850 train_acc=0.8681 val_acc=0.6250 train_kappa=0.7361 val_kappa=0.2500\n",
            "[Epoch 62/100] loss=0.4087 val_loss=0.8243 train_acc=0.8264 val_acc=0.6250 train_kappa=0.6528 val_kappa=0.2500\n",
            "[Epoch 63/100] loss=0.3731 val_loss=0.8716 train_acc=0.8750 val_acc=0.6111 train_kappa=0.7500 val_kappa=0.2222\n",
            "[Epoch 64/100] loss=0.3530 val_loss=0.9260 train_acc=0.8958 val_acc=0.5972 train_kappa=0.7917 val_kappa=0.1944\n",
            "[Epoch 65/100] loss=0.3856 val_loss=0.7253 train_acc=0.8194 val_acc=0.6597 train_kappa=0.6389 val_kappa=0.3194\n",
            "[Epoch 66/100] loss=0.3244 val_loss=1.0467 train_acc=0.8681 val_acc=0.6042 train_kappa=0.7361 val_kappa=0.2083\n",
            "[Epoch 67/100] loss=0.3065 val_loss=0.7981 train_acc=0.9028 val_acc=0.6528 train_kappa=0.8056 val_kappa=0.3056\n",
            "[Epoch 68/100] loss=0.2932 val_loss=0.9159 train_acc=0.9167 val_acc=0.6458 train_kappa=0.8333 val_kappa=0.2917\n",
            "[Epoch 69/100] loss=0.2636 val_loss=0.7925 train_acc=0.9444 val_acc=0.6597 train_kappa=0.8889 val_kappa=0.3194\n",
            "[Epoch 70/100] loss=0.2044 val_loss=0.9763 train_acc=0.9722 val_acc=0.6389 train_kappa=0.9444 val_kappa=0.2778\n",
            "[Epoch 71/100] loss=0.2275 val_loss=0.8410 train_acc=0.9375 val_acc=0.6736 train_kappa=0.8750 val_kappa=0.3472\n",
            "[Epoch 72/100] loss=0.1937 val_loss=0.9177 train_acc=0.9653 val_acc=0.6528 train_kappa=0.9306 val_kappa=0.3056\n",
            "[Epoch 73/100] loss=0.1676 val_loss=0.8439 train_acc=0.9653 val_acc=0.6528 train_kappa=0.9306 val_kappa=0.3056\n",
            "[Epoch 74/100] loss=0.2083 val_loss=0.7705 train_acc=0.9375 val_acc=0.7014 train_kappa=0.8750 val_kappa=0.4028\n",
            "[Epoch 75/100] loss=0.1693 val_loss=0.9116 train_acc=0.9653 val_acc=0.6736 train_kappa=0.9306 val_kappa=0.3472\n",
            "[Epoch 76/100] loss=0.1361 val_loss=0.8218 train_acc=0.9653 val_acc=0.7083 train_kappa=0.9306 val_kappa=0.4167\n",
            "[Epoch 77/100] loss=0.1763 val_loss=0.7760 train_acc=0.9653 val_acc=0.7500 train_kappa=0.9306 val_kappa=0.5000\n",
            "[Epoch 78/100] loss=0.1796 val_loss=1.0925 train_acc=0.9444 val_acc=0.6458 train_kappa=0.8889 val_kappa=0.2917\n",
            "[Epoch 79/100] loss=0.1785 val_loss=0.8018 train_acc=0.9375 val_acc=0.7569 train_kappa=0.8750 val_kappa=0.5139\n",
            "[Epoch 80/100] loss=0.1218 val_loss=1.0888 train_acc=0.9653 val_acc=0.6528 train_kappa=0.9306 val_kappa=0.3056\n",
            "[Epoch 81/100] loss=0.0978 val_loss=0.8356 train_acc=0.9861 val_acc=0.7014 train_kappa=0.9722 val_kappa=0.4028\n",
            "[Epoch 82/100] loss=0.0904 val_loss=1.0397 train_acc=0.9861 val_acc=0.6667 train_kappa=0.9722 val_kappa=0.3333\n",
            "[Epoch 83/100] loss=0.0934 val_loss=0.8213 train_acc=0.9792 val_acc=0.7153 train_kappa=0.9583 val_kappa=0.4306\n",
            "[Epoch 84/100] loss=0.0824 val_loss=0.9032 train_acc=1.0000 val_acc=0.6806 train_kappa=1.0000 val_kappa=0.3611\n",
            "[Epoch 85/100] loss=0.0726 val_loss=0.9312 train_acc=0.9861 val_acc=0.6875 train_kappa=0.9722 val_kappa=0.3750\n",
            "[Epoch 86/100] loss=0.0585 val_loss=0.9017 train_acc=1.0000 val_acc=0.6806 train_kappa=1.0000 val_kappa=0.3611\n",
            "[Epoch 87/100] loss=0.0755 val_loss=0.9162 train_acc=0.9792 val_acc=0.6944 train_kappa=0.9583 val_kappa=0.3889\n",
            "[Epoch 88/100] loss=0.0868 val_loss=0.9204 train_acc=0.9792 val_acc=0.7083 train_kappa=0.9583 val_kappa=0.4167\n",
            "[Epoch 89/100] loss=0.0669 val_loss=0.9891 train_acc=0.9792 val_acc=0.6736 train_kappa=0.9583 val_kappa=0.3472\n",
            "[Epoch 90/100] loss=0.0559 val_loss=1.0502 train_acc=0.9931 val_acc=0.6806 train_kappa=0.9861 val_kappa=0.3611\n",
            "[Epoch 91/100] loss=0.0430 val_loss=0.8897 train_acc=1.0000 val_acc=0.7361 train_kappa=1.0000 val_kappa=0.4722\n",
            "[Epoch 92/100] loss=0.0711 val_loss=1.1195 train_acc=0.9861 val_acc=0.6806 train_kappa=0.9722 val_kappa=0.3611\n",
            "[Epoch 93/100] loss=0.0614 val_loss=0.9175 train_acc=0.9861 val_acc=0.7708 train_kappa=0.9722 val_kappa=0.5417\n",
            "[Epoch 94/100] loss=0.0686 val_loss=1.0653 train_acc=0.9792 val_acc=0.7014 train_kappa=0.9583 val_kappa=0.4028\n",
            "[Epoch 95/100] loss=0.0345 val_loss=1.0652 train_acc=1.0000 val_acc=0.7153 train_kappa=1.0000 val_kappa=0.4306\n",
            "[Epoch 96/100] loss=0.0393 val_loss=0.9969 train_acc=1.0000 val_acc=0.7500 train_kappa=1.0000 val_kappa=0.5000\n",
            "[Epoch 97/100] loss=0.0556 val_loss=1.0601 train_acc=0.9792 val_acc=0.7222 train_kappa=0.9583 val_kappa=0.4444\n",
            "[Epoch 98/100] loss=0.0372 val_loss=1.0335 train_acc=1.0000 val_acc=0.7361 train_kappa=1.0000 val_kappa=0.4722\n",
            "[Epoch 99/100] loss=0.0515 val_loss=1.0176 train_acc=0.9861 val_acc=0.7361 train_kappa=0.9722 val_kappa=0.4722\n",
            "[Epoch 100/100] loss=0.0332 val_loss=1.1339 train_acc=1.0000 val_acc=0.6875 train_kappa=1.0000 val_kappa=0.3750\n",
            "\n",
            "=== Processing Subject 3 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7552 val_loss=0.6921 train_acc=0.4375 val_acc=0.5000 train_kappa=-0.1250 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7240 val_loss=0.7120 train_acc=0.4722 val_acc=0.5000 train_kappa=-0.0556 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.7012 val_loss=0.6928 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.7232 val_loss=0.7090 train_acc=0.4931 val_acc=0.5000 train_kappa=-0.0139 val_kappa=0.0000\n",
            "[Epoch 5/100] loss=0.6910 val_loss=0.6926 train_acc=0.5972 val_acc=0.5000 train_kappa=0.1944 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.7126 val_loss=0.7239 train_acc=0.5139 val_acc=0.5000 train_kappa=0.0278 val_kappa=0.0000\n",
            "[Epoch 7/100] loss=0.6882 val_loss=0.6919 train_acc=0.5347 val_acc=0.5833 train_kappa=0.0694 val_kappa=0.1667\n",
            "[Epoch 8/100] loss=0.6690 val_loss=0.7016 train_acc=0.6111 val_acc=0.5000 train_kappa=0.2222 val_kappa=0.0000\n",
            "[Epoch 9/100] loss=0.6892 val_loss=0.6993 train_acc=0.5069 val_acc=0.5000 train_kappa=0.0139 val_kappa=0.0000\n",
            "[Epoch 10/100] loss=0.6890 val_loss=0.6914 train_acc=0.5417 val_acc=0.5694 train_kappa=0.0833 val_kappa=0.1389\n",
            "[Epoch 11/100] loss=0.7088 val_loss=0.6943 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 12/100] loss=0.7084 val_loss=0.6918 train_acc=0.5139 val_acc=0.5486 train_kappa=0.0278 val_kappa=0.0972\n",
            "[Epoch 13/100] loss=0.6828 val_loss=0.6954 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 14/100] loss=0.7032 val_loss=0.6921 train_acc=0.5417 val_acc=0.5000 train_kappa=0.0833 val_kappa=0.0000\n",
            "[Epoch 15/100] loss=0.7001 val_loss=0.6875 train_acc=0.4931 val_acc=0.5833 train_kappa=-0.0139 val_kappa=0.1667\n",
            "[Epoch 16/100] loss=0.6997 val_loss=0.6863 train_acc=0.4653 val_acc=0.6875 train_kappa=-0.0694 val_kappa=0.3750\n",
            "[Epoch 17/100] loss=0.6922 val_loss=0.6880 train_acc=0.4931 val_acc=0.5000 train_kappa=-0.0139 val_kappa=0.0000\n",
            "[Epoch 18/100] loss=0.7072 val_loss=0.6851 train_acc=0.4861 val_acc=0.6736 train_kappa=-0.0278 val_kappa=0.3472\n",
            "[Epoch 19/100] loss=0.6946 val_loss=0.6905 train_acc=0.5417 val_acc=0.5069 train_kappa=0.0833 val_kappa=0.0139\n",
            "[Epoch 20/100] loss=0.7063 val_loss=0.6811 train_acc=0.4653 val_acc=0.6111 train_kappa=-0.0694 val_kappa=0.2222\n",
            "[Epoch 21/100] loss=0.6996 val_loss=0.6835 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 22/100] loss=0.6932 val_loss=0.6767 train_acc=0.4861 val_acc=0.6736 train_kappa=-0.0278 val_kappa=0.3472\n",
            "[Epoch 23/100] loss=0.6753 val_loss=0.6753 train_acc=0.5903 val_acc=0.6319 train_kappa=0.1806 val_kappa=0.2639\n",
            "[Epoch 24/100] loss=0.6638 val_loss=0.6721 train_acc=0.6181 val_acc=0.6250 train_kappa=0.2361 val_kappa=0.2500\n",
            "[Epoch 25/100] loss=0.6716 val_loss=0.6689 train_acc=0.6042 val_acc=0.6667 train_kappa=0.2083 val_kappa=0.3333\n",
            "[Epoch 26/100] loss=0.6722 val_loss=0.6846 train_acc=0.5764 val_acc=0.5069 train_kappa=0.1528 val_kappa=0.0139\n",
            "[Epoch 27/100] loss=0.6806 val_loss=0.6634 train_acc=0.5556 val_acc=0.6319 train_kappa=0.1111 val_kappa=0.2639\n",
            "[Epoch 28/100] loss=0.6783 val_loss=0.6592 train_acc=0.5833 val_acc=0.5903 train_kappa=0.1667 val_kappa=0.1806\n",
            "[Epoch 29/100] loss=0.6637 val_loss=0.6469 train_acc=0.6111 val_acc=0.7500 train_kappa=0.2222 val_kappa=0.5000\n",
            "[Epoch 30/100] loss=0.6726 val_loss=0.6485 train_acc=0.5556 val_acc=0.5833 train_kappa=0.1111 val_kappa=0.1667\n",
            "[Epoch 31/100] loss=0.6793 val_loss=0.6444 train_acc=0.5833 val_acc=0.6111 train_kappa=0.1667 val_kappa=0.2222\n",
            "[Epoch 32/100] loss=0.6625 val_loss=0.6329 train_acc=0.6458 val_acc=0.6944 train_kappa=0.2917 val_kappa=0.3889\n",
            "[Epoch 33/100] loss=0.6694 val_loss=0.6219 train_acc=0.5764 val_acc=0.7986 train_kappa=0.1528 val_kappa=0.5972\n",
            "[Epoch 34/100] loss=0.6524 val_loss=0.6246 train_acc=0.6389 val_acc=0.6944 train_kappa=0.2778 val_kappa=0.3889\n",
            "[Epoch 35/100] loss=0.6594 val_loss=0.6244 train_acc=0.6389 val_acc=0.5903 train_kappa=0.2778 val_kappa=0.1806\n",
            "[Epoch 36/100] loss=0.6612 val_loss=0.6050 train_acc=0.5972 val_acc=0.7361 train_kappa=0.1944 val_kappa=0.4722\n",
            "[Epoch 37/100] loss=0.6407 val_loss=0.6082 train_acc=0.6667 val_acc=0.7292 train_kappa=0.3333 val_kappa=0.4583\n",
            "[Epoch 38/100] loss=0.6066 val_loss=0.5960 train_acc=0.7014 val_acc=0.7361 train_kappa=0.4028 val_kappa=0.4722\n",
            "[Epoch 39/100] loss=0.6347 val_loss=0.5861 train_acc=0.6389 val_acc=0.7222 train_kappa=0.2778 val_kappa=0.4444\n",
            "[Epoch 40/100] loss=0.6025 val_loss=0.5667 train_acc=0.6806 val_acc=0.7639 train_kappa=0.3611 val_kappa=0.5278\n",
            "[Epoch 41/100] loss=0.5796 val_loss=0.5610 train_acc=0.7431 val_acc=0.8125 train_kappa=0.4861 val_kappa=0.6250\n",
            "[Epoch 42/100] loss=0.6150 val_loss=0.5464 train_acc=0.6458 val_acc=0.8264 train_kappa=0.2917 val_kappa=0.6528\n",
            "[Epoch 43/100] loss=0.5824 val_loss=0.5230 train_acc=0.7292 val_acc=0.8264 train_kappa=0.4583 val_kappa=0.6528\n",
            "[Epoch 44/100] loss=0.5468 val_loss=0.5007 train_acc=0.7778 val_acc=0.8403 train_kappa=0.5556 val_kappa=0.6806\n",
            "[Epoch 45/100] loss=0.5549 val_loss=0.5021 train_acc=0.7500 val_acc=0.7708 train_kappa=0.5000 val_kappa=0.5417\n",
            "[Epoch 46/100] loss=0.5367 val_loss=0.4648 train_acc=0.7361 val_acc=0.8264 train_kappa=0.4722 val_kappa=0.6528\n",
            "[Epoch 47/100] loss=0.5244 val_loss=0.4603 train_acc=0.7431 val_acc=0.7847 train_kappa=0.4861 val_kappa=0.5694\n",
            "[Epoch 48/100] loss=0.5142 val_loss=0.4212 train_acc=0.7639 val_acc=0.8750 train_kappa=0.5278 val_kappa=0.7500\n",
            "[Epoch 49/100] loss=0.5071 val_loss=0.4024 train_acc=0.7639 val_acc=0.8472 train_kappa=0.5278 val_kappa=0.6944\n",
            "[Epoch 50/100] loss=0.4629 val_loss=0.4747 train_acc=0.8264 val_acc=0.7361 train_kappa=0.6528 val_kappa=0.4722\n",
            "[Epoch 51/100] loss=0.4582 val_loss=0.3414 train_acc=0.7986 val_acc=0.8819 train_kappa=0.5972 val_kappa=0.7639\n",
            "[Epoch 52/100] loss=0.4428 val_loss=0.3923 train_acc=0.8056 val_acc=0.7986 train_kappa=0.6111 val_kappa=0.5972\n",
            "[Epoch 53/100] loss=0.4269 val_loss=0.3009 train_acc=0.7986 val_acc=0.8819 train_kappa=0.5972 val_kappa=0.7639\n",
            "[Epoch 54/100] loss=0.3701 val_loss=0.2993 train_acc=0.8681 val_acc=0.8958 train_kappa=0.7361 val_kappa=0.7917\n",
            "[Epoch 55/100] loss=0.3768 val_loss=0.3367 train_acc=0.8819 val_acc=0.8264 train_kappa=0.7639 val_kappa=0.6528\n",
            "[Epoch 56/100] loss=0.3513 val_loss=0.2649 train_acc=0.8542 val_acc=0.9167 train_kappa=0.7083 val_kappa=0.8333\n",
            "[Epoch 57/100] loss=0.3949 val_loss=0.3423 train_acc=0.8264 val_acc=0.8194 train_kappa=0.6528 val_kappa=0.6389\n",
            "[Epoch 58/100] loss=0.3313 val_loss=0.2605 train_acc=0.8819 val_acc=0.8889 train_kappa=0.7639 val_kappa=0.7778\n",
            "[Epoch 59/100] loss=0.3567 val_loss=0.3067 train_acc=0.8056 val_acc=0.8681 train_kappa=0.6111 val_kappa=0.7361\n",
            "[Epoch 60/100] loss=0.2912 val_loss=0.2474 train_acc=0.9236 val_acc=0.9097 train_kappa=0.8472 val_kappa=0.8194\n",
            "[Epoch 61/100] loss=0.3203 val_loss=0.3046 train_acc=0.8611 val_acc=0.8472 train_kappa=0.7222 val_kappa=0.6944\n",
            "[Epoch 62/100] loss=0.3386 val_loss=0.2295 train_acc=0.8681 val_acc=0.9028 train_kappa=0.7361 val_kappa=0.8056\n",
            "[Epoch 63/100] loss=0.2808 val_loss=0.2444 train_acc=0.8542 val_acc=0.8750 train_kappa=0.7083 val_kappa=0.7500\n",
            "[Epoch 64/100] loss=0.2683 val_loss=0.2600 train_acc=0.8958 val_acc=0.8681 train_kappa=0.7917 val_kappa=0.7361\n",
            "[Epoch 65/100] loss=0.2218 val_loss=0.2590 train_acc=0.9236 val_acc=0.8681 train_kappa=0.8472 val_kappa=0.7361\n",
            "[Epoch 66/100] loss=0.2331 val_loss=0.2502 train_acc=0.9167 val_acc=0.8750 train_kappa=0.8333 val_kappa=0.7500\n",
            "[Epoch 67/100] loss=0.1953 val_loss=0.2239 train_acc=0.9444 val_acc=0.9097 train_kappa=0.8889 val_kappa=0.8194\n",
            "[Epoch 68/100] loss=0.2039 val_loss=0.2320 train_acc=0.9236 val_acc=0.8958 train_kappa=0.8472 val_kappa=0.7917\n",
            "[Epoch 69/100] loss=0.2050 val_loss=0.2502 train_acc=0.9583 val_acc=0.8750 train_kappa=0.9167 val_kappa=0.7500\n",
            "[Epoch 70/100] loss=0.2006 val_loss=0.2114 train_acc=0.9583 val_acc=0.9028 train_kappa=0.9167 val_kappa=0.8056\n",
            "[Epoch 71/100] loss=0.1740 val_loss=0.2210 train_acc=0.9444 val_acc=0.8958 train_kappa=0.8889 val_kappa=0.7917\n",
            "[Epoch 72/100] loss=0.1523 val_loss=0.2075 train_acc=0.9583 val_acc=0.9097 train_kappa=0.9167 val_kappa=0.8194\n",
            "[Epoch 73/100] loss=0.1652 val_loss=0.2195 train_acc=0.9653 val_acc=0.9028 train_kappa=0.9306 val_kappa=0.8056\n",
            "[Epoch 74/100] loss=0.1437 val_loss=0.2083 train_acc=0.9514 val_acc=0.9167 train_kappa=0.9028 val_kappa=0.8333\n",
            "[Epoch 75/100] loss=0.1096 val_loss=0.2002 train_acc=0.9583 val_acc=0.9236 train_kappa=0.9167 val_kappa=0.8472\n",
            "[Epoch 76/100] loss=0.1150 val_loss=0.2771 train_acc=0.9792 val_acc=0.8611 train_kappa=0.9583 val_kappa=0.7222\n",
            "[Epoch 77/100] loss=0.1683 val_loss=0.1989 train_acc=0.9444 val_acc=0.9167 train_kappa=0.8889 val_kappa=0.8333\n",
            "[Epoch 78/100] loss=0.1108 val_loss=0.2516 train_acc=0.9583 val_acc=0.8750 train_kappa=0.9167 val_kappa=0.7500\n",
            "[Epoch 79/100] loss=0.1079 val_loss=0.2052 train_acc=0.9722 val_acc=0.9236 train_kappa=0.9444 val_kappa=0.8472\n",
            "[Epoch 80/100] loss=0.1200 val_loss=0.1888 train_acc=0.9722 val_acc=0.9306 train_kappa=0.9444 val_kappa=0.8611\n",
            "[Epoch 81/100] loss=0.1123 val_loss=0.1863 train_acc=0.9375 val_acc=0.9306 train_kappa=0.8750 val_kappa=0.8611\n",
            "[Epoch 82/100] loss=0.1273 val_loss=0.2269 train_acc=0.9444 val_acc=0.9097 train_kappa=0.8889 val_kappa=0.8194\n",
            "[Epoch 83/100] loss=0.1213 val_loss=0.2128 train_acc=0.9583 val_acc=0.9167 train_kappa=0.9167 val_kappa=0.8333\n",
            "[Epoch 84/100] loss=0.0948 val_loss=0.2451 train_acc=0.9653 val_acc=0.8958 train_kappa=0.9306 val_kappa=0.7917\n",
            "[Epoch 85/100] loss=0.1048 val_loss=0.2037 train_acc=0.9514 val_acc=0.9375 train_kappa=0.9028 val_kappa=0.8750\n",
            "[Epoch 86/100] loss=0.1171 val_loss=0.2431 train_acc=0.9653 val_acc=0.8958 train_kappa=0.9306 val_kappa=0.7917\n",
            "[Epoch 87/100] loss=0.1013 val_loss=0.1943 train_acc=0.9653 val_acc=0.9236 train_kappa=0.9306 val_kappa=0.8472\n",
            "[Epoch 88/100] loss=0.0974 val_loss=0.2399 train_acc=0.9583 val_acc=0.9028 train_kappa=0.9167 val_kappa=0.8056\n",
            "[Epoch 89/100] loss=0.0965 val_loss=0.2100 train_acc=0.9653 val_acc=0.9236 train_kappa=0.9306 val_kappa=0.8472\n",
            "[Epoch 90/100] loss=0.0881 val_loss=0.2513 train_acc=0.9792 val_acc=0.8889 train_kappa=0.9583 val_kappa=0.7778\n",
            "[Epoch 91/100] loss=0.0726 val_loss=0.2084 train_acc=0.9722 val_acc=0.9167 train_kappa=0.9444 val_kappa=0.8333\n",
            "[Epoch 92/100] loss=0.0752 val_loss=0.2136 train_acc=0.9792 val_acc=0.9167 train_kappa=0.9583 val_kappa=0.8333\n",
            "[Epoch 93/100] loss=0.0683 val_loss=0.2087 train_acc=0.9792 val_acc=0.8958 train_kappa=0.9583 val_kappa=0.7917\n",
            "[Epoch 94/100] loss=0.0880 val_loss=0.2112 train_acc=0.9792 val_acc=0.9236 train_kappa=0.9583 val_kappa=0.8472\n",
            "[Epoch 95/100] loss=0.0843 val_loss=0.2122 train_acc=0.9722 val_acc=0.9028 train_kappa=0.9444 val_kappa=0.8056\n",
            "[Epoch 96/100] loss=0.0810 val_loss=0.2149 train_acc=0.9722 val_acc=0.9097 train_kappa=0.9444 val_kappa=0.8194\n",
            "[Epoch 97/100] loss=0.0796 val_loss=0.2538 train_acc=0.9722 val_acc=0.8889 train_kappa=0.9444 val_kappa=0.7778\n",
            "[Epoch 98/100] loss=0.0354 val_loss=0.2276 train_acc=1.0000 val_acc=0.9236 train_kappa=1.0000 val_kappa=0.8472\n",
            "[Epoch 99/100] loss=0.0710 val_loss=0.3027 train_acc=0.9861 val_acc=0.8889 train_kappa=0.9722 val_kappa=0.7778\n",
            "[Epoch 100/100] loss=0.0775 val_loss=0.2197 train_acc=0.9792 val_acc=0.9306 train_kappa=0.9583 val_kappa=0.8611\n",
            "\n",
            "=== Processing Subject 4 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7273 val_loss=0.7222 train_acc=0.5139 val_acc=0.5000 train_kappa=0.0278 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7280 val_loss=0.6944 train_acc=0.4722 val_acc=0.5000 train_kappa=-0.0556 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.7085 val_loss=0.6907 train_acc=0.4375 val_acc=0.5208 train_kappa=-0.1250 val_kappa=0.0417\n",
            "[Epoch 4/100] loss=0.6946 val_loss=0.6874 train_acc=0.5486 val_acc=0.5000 train_kappa=0.0972 val_kappa=0.0000\n",
            "[Epoch 5/100] loss=0.6806 val_loss=0.6858 train_acc=0.5278 val_acc=0.5000 train_kappa=0.0556 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.7055 val_loss=0.6826 train_acc=0.4514 val_acc=0.5278 train_kappa=-0.0972 val_kappa=0.0556\n",
            "[Epoch 7/100] loss=0.6896 val_loss=0.6758 train_acc=0.5069 val_acc=0.6319 train_kappa=0.0139 val_kappa=0.2639\n",
            "[Epoch 8/100] loss=0.7032 val_loss=0.6709 train_acc=0.5208 val_acc=0.7361 train_kappa=0.0417 val_kappa=0.4722\n",
            "[Epoch 9/100] loss=0.6946 val_loss=0.6763 train_acc=0.5556 val_acc=0.5556 train_kappa=0.1111 val_kappa=0.1111\n",
            "[Epoch 10/100] loss=0.6932 val_loss=0.6784 train_acc=0.5556 val_acc=0.5625 train_kappa=0.1111 val_kappa=0.1250\n",
            "[Epoch 11/100] loss=0.6812 val_loss=0.6855 train_acc=0.5278 val_acc=0.6181 train_kappa=0.0556 val_kappa=0.2361\n",
            "[Epoch 12/100] loss=0.6955 val_loss=0.6849 train_acc=0.5000 val_acc=0.6042 train_kappa=0.0000 val_kappa=0.2083\n",
            "[Epoch 13/100] loss=0.6729 val_loss=0.6801 train_acc=0.5417 val_acc=0.7222 train_kappa=0.0833 val_kappa=0.4444\n",
            "[Epoch 14/100] loss=0.6864 val_loss=0.6779 train_acc=0.5833 val_acc=0.5417 train_kappa=0.1667 val_kappa=0.0833\n",
            "[Epoch 15/100] loss=0.6852 val_loss=0.6737 train_acc=0.5625 val_acc=0.6806 train_kappa=0.1250 val_kappa=0.3611\n",
            "[Epoch 16/100] loss=0.6734 val_loss=0.6735 train_acc=0.5833 val_acc=0.6667 train_kappa=0.1667 val_kappa=0.3333\n",
            "[Epoch 17/100] loss=0.6694 val_loss=0.6674 train_acc=0.6458 val_acc=0.7431 train_kappa=0.2917 val_kappa=0.4861\n",
            "[Epoch 18/100] loss=0.6902 val_loss=0.6634 train_acc=0.5139 val_acc=0.6319 train_kappa=0.0278 val_kappa=0.2639\n",
            "[Epoch 19/100] loss=0.6925 val_loss=0.6641 train_acc=0.5625 val_acc=0.5833 train_kappa=0.1250 val_kappa=0.1667\n",
            "[Epoch 20/100] loss=0.6936 val_loss=0.6698 train_acc=0.6042 val_acc=0.5486 train_kappa=0.2083 val_kappa=0.0972\n",
            "[Epoch 21/100] loss=0.6768 val_loss=0.6404 train_acc=0.5833 val_acc=0.6736 train_kappa=0.1667 val_kappa=0.3472\n",
            "[Epoch 22/100] loss=0.6511 val_loss=0.6380 train_acc=0.6458 val_acc=0.7708 train_kappa=0.2917 val_kappa=0.5417\n",
            "[Epoch 23/100] loss=0.6564 val_loss=0.6465 train_acc=0.6458 val_acc=0.7986 train_kappa=0.2917 val_kappa=0.5972\n",
            "[Epoch 24/100] loss=0.6478 val_loss=0.6615 train_acc=0.6111 val_acc=0.5903 train_kappa=0.2222 val_kappa=0.1806\n",
            "[Epoch 25/100] loss=0.6720 val_loss=0.6732 train_acc=0.5972 val_acc=0.5069 train_kappa=0.1944 val_kappa=0.0139\n",
            "[Epoch 26/100] loss=0.6575 val_loss=0.6380 train_acc=0.6111 val_acc=0.8125 train_kappa=0.2222 val_kappa=0.6250\n",
            "[Epoch 27/100] loss=0.6633 val_loss=0.6321 train_acc=0.6111 val_acc=0.7083 train_kappa=0.2222 val_kappa=0.4167\n",
            "[Epoch 28/100] loss=0.6549 val_loss=0.6318 train_acc=0.5972 val_acc=0.7361 train_kappa=0.1944 val_kappa=0.4722\n",
            "[Epoch 29/100] loss=0.6532 val_loss=0.6428 train_acc=0.6181 val_acc=0.6111 train_kappa=0.2361 val_kappa=0.2222\n",
            "[Epoch 30/100] loss=0.6425 val_loss=0.6367 train_acc=0.6319 val_acc=0.7917 train_kappa=0.2639 val_kappa=0.5833\n",
            "[Epoch 31/100] loss=0.6645 val_loss=0.6317 train_acc=0.6042 val_acc=0.7639 train_kappa=0.2083 val_kappa=0.5278\n",
            "[Epoch 32/100] loss=0.6267 val_loss=0.6227 train_acc=0.6667 val_acc=0.7917 train_kappa=0.3333 val_kappa=0.5833\n",
            "[Epoch 33/100] loss=0.6610 val_loss=0.6135 train_acc=0.6042 val_acc=0.7431 train_kappa=0.2083 val_kappa=0.4861\n",
            "[Epoch 34/100] loss=0.6534 val_loss=0.6024 train_acc=0.5694 val_acc=0.7917 train_kappa=0.1389 val_kappa=0.5833\n",
            "[Epoch 35/100] loss=0.6503 val_loss=0.6086 train_acc=0.6250 val_acc=0.6944 train_kappa=0.2500 val_kappa=0.3889\n",
            "[Epoch 36/100] loss=0.6529 val_loss=0.6179 train_acc=0.6319 val_acc=0.7361 train_kappa=0.2639 val_kappa=0.4722\n",
            "[Epoch 37/100] loss=0.6254 val_loss=0.6055 train_acc=0.6389 val_acc=0.8056 train_kappa=0.2778 val_kappa=0.6111\n",
            "[Epoch 38/100] loss=0.6058 val_loss=0.6082 train_acc=0.7153 val_acc=0.6806 train_kappa=0.4306 val_kappa=0.3611\n",
            "[Epoch 39/100] loss=0.6391 val_loss=0.6007 train_acc=0.6389 val_acc=0.7083 train_kappa=0.2778 val_kappa=0.4167\n",
            "[Epoch 40/100] loss=0.6191 val_loss=0.5806 train_acc=0.6528 val_acc=0.8403 train_kappa=0.3056 val_kappa=0.6806\n",
            "[Epoch 41/100] loss=0.6209 val_loss=0.5811 train_acc=0.6597 val_acc=0.7708 train_kappa=0.3194 val_kappa=0.5417\n",
            "[Epoch 42/100] loss=0.6242 val_loss=0.5862 train_acc=0.6389 val_acc=0.7292 train_kappa=0.2778 val_kappa=0.4583\n",
            "[Epoch 43/100] loss=0.5905 val_loss=0.5727 train_acc=0.7569 val_acc=0.7569 train_kappa=0.5139 val_kappa=0.5139\n",
            "[Epoch 44/100] loss=0.5712 val_loss=0.5514 train_acc=0.7361 val_acc=0.7778 train_kappa=0.4722 val_kappa=0.5556\n",
            "[Epoch 45/100] loss=0.5688 val_loss=0.5284 train_acc=0.7500 val_acc=0.8333 train_kappa=0.5000 val_kappa=0.6667\n",
            "[Epoch 46/100] loss=0.5753 val_loss=0.5655 train_acc=0.7083 val_acc=0.7222 train_kappa=0.4167 val_kappa=0.4444\n",
            "[Epoch 47/100] loss=0.5832 val_loss=0.5575 train_acc=0.7153 val_acc=0.7639 train_kappa=0.4306 val_kappa=0.5278\n",
            "[Epoch 48/100] loss=0.5648 val_loss=0.5544 train_acc=0.7500 val_acc=0.7292 train_kappa=0.5000 val_kappa=0.4583\n",
            "[Epoch 49/100] loss=0.5588 val_loss=0.5452 train_acc=0.7778 val_acc=0.7986 train_kappa=0.5556 val_kappa=0.5972\n",
            "[Epoch 50/100] loss=0.4937 val_loss=0.5383 train_acc=0.8333 val_acc=0.7569 train_kappa=0.6667 val_kappa=0.5139\n",
            "[Epoch 51/100] loss=0.5633 val_loss=0.5107 train_acc=0.7083 val_acc=0.7500 train_kappa=0.4167 val_kappa=0.5000\n",
            "[Epoch 52/100] loss=0.5189 val_loss=0.4881 train_acc=0.7500 val_acc=0.8403 train_kappa=0.5000 val_kappa=0.6806\n",
            "[Epoch 53/100] loss=0.5284 val_loss=0.5298 train_acc=0.7569 val_acc=0.7361 train_kappa=0.5139 val_kappa=0.4722\n",
            "[Epoch 54/100] loss=0.5115 val_loss=0.4607 train_acc=0.7361 val_acc=0.8681 train_kappa=0.4722 val_kappa=0.7361\n",
            "[Epoch 55/100] loss=0.5026 val_loss=0.4921 train_acc=0.7708 val_acc=0.7639 train_kappa=0.5417 val_kappa=0.5278\n",
            "[Epoch 56/100] loss=0.4805 val_loss=0.5127 train_acc=0.7708 val_acc=0.7222 train_kappa=0.5417 val_kappa=0.4444\n",
            "[Epoch 57/100] loss=0.5268 val_loss=0.4505 train_acc=0.7222 val_acc=0.8403 train_kappa=0.4444 val_kappa=0.6806\n",
            "[Epoch 58/100] loss=0.4630 val_loss=0.4734 train_acc=0.8264 val_acc=0.7569 train_kappa=0.6528 val_kappa=0.5139\n",
            "[Epoch 59/100] loss=0.4024 val_loss=0.4765 train_acc=0.8750 val_acc=0.7639 train_kappa=0.7500 val_kappa=0.5278\n",
            "[Epoch 60/100] loss=0.4498 val_loss=0.4220 train_acc=0.8125 val_acc=0.8542 train_kappa=0.6250 val_kappa=0.7083\n",
            "[Epoch 61/100] loss=0.4292 val_loss=0.4335 train_acc=0.8264 val_acc=0.8194 train_kappa=0.6528 val_kappa=0.6389\n",
            "[Epoch 62/100] loss=0.4177 val_loss=0.4220 train_acc=0.8333 val_acc=0.8542 train_kappa=0.6667 val_kappa=0.7083\n",
            "[Epoch 63/100] loss=0.3775 val_loss=0.4361 train_acc=0.8750 val_acc=0.7986 train_kappa=0.7500 val_kappa=0.5972\n",
            "[Epoch 64/100] loss=0.3736 val_loss=0.4185 train_acc=0.8264 val_acc=0.8472 train_kappa=0.6528 val_kappa=0.6944\n",
            "[Epoch 65/100] loss=0.3922 val_loss=0.4207 train_acc=0.8472 val_acc=0.8403 train_kappa=0.6944 val_kappa=0.6806\n",
            "[Epoch 66/100] loss=0.4004 val_loss=0.4089 train_acc=0.8403 val_acc=0.8333 train_kappa=0.6806 val_kappa=0.6667\n",
            "[Epoch 67/100] loss=0.3945 val_loss=0.4293 train_acc=0.8472 val_acc=0.7986 train_kappa=0.6944 val_kappa=0.5972\n",
            "[Epoch 68/100] loss=0.3376 val_loss=0.4147 train_acc=0.8889 val_acc=0.8264 train_kappa=0.7778 val_kappa=0.6528\n",
            "[Epoch 69/100] loss=0.3710 val_loss=0.4310 train_acc=0.8542 val_acc=0.7986 train_kappa=0.7083 val_kappa=0.5972\n",
            "[Epoch 70/100] loss=0.3111 val_loss=0.3914 train_acc=0.8958 val_acc=0.8403 train_kappa=0.7917 val_kappa=0.6806\n",
            "[Epoch 71/100] loss=0.3277 val_loss=0.4106 train_acc=0.8819 val_acc=0.8056 train_kappa=0.7639 val_kappa=0.6111\n",
            "[Epoch 72/100] loss=0.3208 val_loss=0.4015 train_acc=0.8889 val_acc=0.8264 train_kappa=0.7778 val_kappa=0.6528\n",
            "[Epoch 73/100] loss=0.3610 val_loss=0.3947 train_acc=0.8333 val_acc=0.8264 train_kappa=0.6667 val_kappa=0.6528\n",
            "[Epoch 74/100] loss=0.2497 val_loss=0.4263 train_acc=0.9236 val_acc=0.7917 train_kappa=0.8472 val_kappa=0.5833\n",
            "[Epoch 75/100] loss=0.2635 val_loss=0.4161 train_acc=0.9306 val_acc=0.8403 train_kappa=0.8611 val_kappa=0.6806\n",
            "[Epoch 76/100] loss=0.2807 val_loss=0.4499 train_acc=0.8750 val_acc=0.7847 train_kappa=0.7500 val_kappa=0.5694\n",
            "[Epoch 77/100] loss=0.2758 val_loss=0.3918 train_acc=0.8889 val_acc=0.8333 train_kappa=0.7778 val_kappa=0.6667\n",
            "[Epoch 78/100] loss=0.2779 val_loss=0.3903 train_acc=0.8958 val_acc=0.8194 train_kappa=0.7917 val_kappa=0.6389\n",
            "[Epoch 79/100] loss=0.2524 val_loss=0.3891 train_acc=0.9097 val_acc=0.8264 train_kappa=0.8194 val_kappa=0.6528\n",
            "[Epoch 80/100] loss=0.1867 val_loss=0.3915 train_acc=0.9583 val_acc=0.8403 train_kappa=0.9167 val_kappa=0.6806\n",
            "[Epoch 81/100] loss=0.2389 val_loss=0.4407 train_acc=0.9097 val_acc=0.7778 train_kappa=0.8194 val_kappa=0.5556\n",
            "[Epoch 82/100] loss=0.2281 val_loss=0.4045 train_acc=0.9375 val_acc=0.8333 train_kappa=0.8750 val_kappa=0.6667\n",
            "[Epoch 83/100] loss=0.2245 val_loss=0.4002 train_acc=0.9236 val_acc=0.8125 train_kappa=0.8472 val_kappa=0.6250\n",
            "[Epoch 84/100] loss=0.2117 val_loss=0.4101 train_acc=0.9444 val_acc=0.7986 train_kappa=0.8889 val_kappa=0.5972\n",
            "[Epoch 85/100] loss=0.1719 val_loss=0.4244 train_acc=0.9444 val_acc=0.8264 train_kappa=0.8889 val_kappa=0.6528\n",
            "[Epoch 86/100] loss=0.2086 val_loss=0.4216 train_acc=0.9306 val_acc=0.8056 train_kappa=0.8611 val_kappa=0.6111\n",
            "[Epoch 87/100] loss=0.1636 val_loss=0.3995 train_acc=0.9583 val_acc=0.8403 train_kappa=0.9167 val_kappa=0.6806\n",
            "[Epoch 88/100] loss=0.1404 val_loss=0.3934 train_acc=0.9722 val_acc=0.8125 train_kappa=0.9444 val_kappa=0.6250\n",
            "[Epoch 89/100] loss=0.1655 val_loss=0.3769 train_acc=0.9444 val_acc=0.8403 train_kappa=0.8889 val_kappa=0.6806\n",
            "[Epoch 90/100] loss=0.1962 val_loss=0.4107 train_acc=0.9306 val_acc=0.8056 train_kappa=0.8611 val_kappa=0.6111\n",
            "[Epoch 91/100] loss=0.1735 val_loss=0.3996 train_acc=0.9444 val_acc=0.8333 train_kappa=0.8889 val_kappa=0.6667\n",
            "[Epoch 92/100] loss=0.1481 val_loss=0.4468 train_acc=0.9722 val_acc=0.7986 train_kappa=0.9444 val_kappa=0.5972\n",
            "[Epoch 93/100] loss=0.1489 val_loss=0.4304 train_acc=0.9444 val_acc=0.7986 train_kappa=0.8889 val_kappa=0.5972\n",
            "[Epoch 94/100] loss=0.1218 val_loss=0.4080 train_acc=0.9653 val_acc=0.8264 train_kappa=0.9306 val_kappa=0.6528\n",
            "[Epoch 95/100] loss=0.1194 val_loss=0.4044 train_acc=0.9653 val_acc=0.8264 train_kappa=0.9306 val_kappa=0.6528\n",
            "[Epoch 96/100] loss=0.1370 val_loss=0.4245 train_acc=0.9722 val_acc=0.8125 train_kappa=0.9444 val_kappa=0.6250\n",
            "[Epoch 97/100] loss=0.1236 val_loss=0.4225 train_acc=0.9653 val_acc=0.8125 train_kappa=0.9306 val_kappa=0.6250\n",
            "[Epoch 98/100] loss=0.1322 val_loss=0.4058 train_acc=0.9514 val_acc=0.8264 train_kappa=0.9028 val_kappa=0.6528\n",
            "[Epoch 99/100] loss=0.1178 val_loss=0.4111 train_acc=0.9792 val_acc=0.8194 train_kappa=0.9583 val_kappa=0.6389\n",
            "[Epoch 100/100] loss=0.0975 val_loss=0.4018 train_acc=0.9792 val_acc=0.8194 train_kappa=0.9583 val_kappa=0.6389\n",
            "\n",
            "=== Processing Subject 5 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7533 val_loss=0.7177 train_acc=0.4514 val_acc=0.5000 train_kappa=-0.0972 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.6987 val_loss=0.6989 train_acc=0.5486 val_acc=0.5000 train_kappa=0.0972 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.7048 val_loss=0.6935 train_acc=0.5278 val_acc=0.5000 train_kappa=0.0556 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.7143 val_loss=0.7211 train_acc=0.4444 val_acc=0.5000 train_kappa=-0.1111 val_kappa=0.0000\n",
            "[Epoch 5/100] loss=0.6964 val_loss=0.6964 train_acc=0.5556 val_acc=0.5000 train_kappa=0.1111 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.7244 val_loss=0.6935 train_acc=0.4514 val_acc=0.5347 train_kappa=-0.0972 val_kappa=0.0694\n",
            "[Epoch 7/100] loss=0.6794 val_loss=0.6928 train_acc=0.5486 val_acc=0.5556 train_kappa=0.0972 val_kappa=0.1111\n",
            "[Epoch 8/100] loss=0.6964 val_loss=0.7002 train_acc=0.5069 val_acc=0.5000 train_kappa=0.0139 val_kappa=0.0000\n",
            "[Epoch 9/100] loss=0.7176 val_loss=0.7012 train_acc=0.5069 val_acc=0.5000 train_kappa=0.0139 val_kappa=0.0000\n",
            "[Epoch 10/100] loss=0.6935 val_loss=0.6907 train_acc=0.4931 val_acc=0.5278 train_kappa=-0.0139 val_kappa=0.0556\n",
            "[Epoch 11/100] loss=0.7109 val_loss=0.6938 train_acc=0.4931 val_acc=0.5208 train_kappa=-0.0139 val_kappa=0.0417\n",
            "[Epoch 12/100] loss=0.6953 val_loss=0.6926 train_acc=0.5625 val_acc=0.5417 train_kappa=0.1250 val_kappa=0.0833\n",
            "[Epoch 13/100] loss=0.6838 val_loss=0.6896 train_acc=0.5486 val_acc=0.5417 train_kappa=0.0972 val_kappa=0.0833\n",
            "[Epoch 14/100] loss=0.6871 val_loss=0.6929 train_acc=0.5278 val_acc=0.5069 train_kappa=0.0556 val_kappa=0.0139\n",
            "[Epoch 15/100] loss=0.7021 val_loss=0.6924 train_acc=0.4931 val_acc=0.5347 train_kappa=-0.0139 val_kappa=0.0694\n",
            "[Epoch 16/100] loss=0.7028 val_loss=0.6878 train_acc=0.5069 val_acc=0.5208 train_kappa=0.0139 val_kappa=0.0417\n",
            "[Epoch 17/100] loss=0.7141 val_loss=0.6970 train_acc=0.4306 val_acc=0.5000 train_kappa=-0.1389 val_kappa=0.0000\n",
            "[Epoch 18/100] loss=0.7070 val_loss=0.6943 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 19/100] loss=0.7004 val_loss=0.6982 train_acc=0.4861 val_acc=0.5000 train_kappa=-0.0278 val_kappa=0.0000\n",
            "[Epoch 20/100] loss=0.7124 val_loss=0.6875 train_acc=0.4792 val_acc=0.5972 train_kappa=-0.0417 val_kappa=0.1944\n",
            "[Epoch 21/100] loss=0.6956 val_loss=0.6870 train_acc=0.5347 val_acc=0.5833 train_kappa=0.0694 val_kappa=0.1667\n",
            "[Epoch 22/100] loss=0.7085 val_loss=0.6880 train_acc=0.5139 val_acc=0.5486 train_kappa=0.0278 val_kappa=0.0972\n",
            "[Epoch 23/100] loss=0.6839 val_loss=0.6912 train_acc=0.5000 val_acc=0.4931 train_kappa=0.0000 val_kappa=-0.0139\n",
            "[Epoch 24/100] loss=0.6916 val_loss=0.6869 train_acc=0.5417 val_acc=0.5347 train_kappa=0.0833 val_kappa=0.0694\n",
            "[Epoch 25/100] loss=0.6826 val_loss=0.6879 train_acc=0.5833 val_acc=0.5278 train_kappa=0.1667 val_kappa=0.0556\n",
            "[Epoch 26/100] loss=0.6815 val_loss=0.6888 train_acc=0.5069 val_acc=0.5278 train_kappa=0.0139 val_kappa=0.0556\n",
            "[Epoch 27/100] loss=0.6914 val_loss=0.7031 train_acc=0.5694 val_acc=0.5000 train_kappa=0.1389 val_kappa=0.0000\n",
            "[Epoch 28/100] loss=0.6726 val_loss=0.6869 train_acc=0.5278 val_acc=0.5833 train_kappa=0.0556 val_kappa=0.1667\n",
            "[Epoch 29/100] loss=0.6685 val_loss=0.6907 train_acc=0.5972 val_acc=0.5000 train_kappa=0.1944 val_kappa=0.0000\n",
            "[Epoch 30/100] loss=0.6879 val_loss=0.6862 train_acc=0.5278 val_acc=0.5694 train_kappa=0.0556 val_kappa=0.1389\n",
            "[Epoch 31/100] loss=0.6816 val_loss=0.6934 train_acc=0.5972 val_acc=0.5000 train_kappa=0.1944 val_kappa=0.0000\n",
            "[Epoch 32/100] loss=0.6812 val_loss=0.6869 train_acc=0.5347 val_acc=0.5278 train_kappa=0.0694 val_kappa=0.0556\n",
            "[Epoch 33/100] loss=0.6952 val_loss=0.6869 train_acc=0.5556 val_acc=0.5000 train_kappa=0.1111 val_kappa=0.0000\n",
            "[Epoch 34/100] loss=0.6654 val_loss=0.6832 train_acc=0.5833 val_acc=0.5833 train_kappa=0.1667 val_kappa=0.1667\n",
            "[Epoch 35/100] loss=0.6763 val_loss=0.6862 train_acc=0.5764 val_acc=0.5278 train_kappa=0.1528 val_kappa=0.0556\n",
            "[Epoch 36/100] loss=0.6624 val_loss=0.6863 train_acc=0.5694 val_acc=0.5278 train_kappa=0.1389 val_kappa=0.0556\n",
            "[Epoch 37/100] loss=0.6750 val_loss=0.6928 train_acc=0.5625 val_acc=0.5069 train_kappa=0.1250 val_kappa=0.0139\n",
            "[Epoch 38/100] loss=0.6672 val_loss=0.6917 train_acc=0.6111 val_acc=0.5139 train_kappa=0.2222 val_kappa=0.0278\n",
            "[Epoch 39/100] loss=0.6819 val_loss=0.6973 train_acc=0.5903 val_acc=0.5139 train_kappa=0.1806 val_kappa=0.0278\n",
            "[Epoch 40/100] loss=0.6832 val_loss=0.6829 train_acc=0.5625 val_acc=0.5139 train_kappa=0.1250 val_kappa=0.0278\n",
            "[Epoch 41/100] loss=0.6744 val_loss=0.6824 train_acc=0.6250 val_acc=0.5417 train_kappa=0.2500 val_kappa=0.0833\n",
            "[Epoch 42/100] loss=0.6993 val_loss=0.6867 train_acc=0.5208 val_acc=0.4931 train_kappa=0.0417 val_kappa=-0.0139\n",
            "[Epoch 43/100] loss=0.6541 val_loss=0.6836 train_acc=0.6181 val_acc=0.5139 train_kappa=0.2361 val_kappa=0.0278\n",
            "[Epoch 44/100] loss=0.6594 val_loss=0.6868 train_acc=0.6319 val_acc=0.5069 train_kappa=0.2639 val_kappa=0.0139\n",
            "[Epoch 45/100] loss=0.6932 val_loss=0.6914 train_acc=0.5278 val_acc=0.5139 train_kappa=0.0556 val_kappa=0.0278\n",
            "[Epoch 46/100] loss=0.6515 val_loss=0.6821 train_acc=0.6667 val_acc=0.6111 train_kappa=0.3333 val_kappa=0.2222\n",
            "[Epoch 47/100] loss=0.6927 val_loss=0.6832 train_acc=0.5556 val_acc=0.5069 train_kappa=0.1111 val_kappa=0.0139\n",
            "[Epoch 48/100] loss=0.6544 val_loss=0.6970 train_acc=0.5694 val_acc=0.5139 train_kappa=0.1389 val_kappa=0.0278\n",
            "[Epoch 49/100] loss=0.6504 val_loss=0.6797 train_acc=0.6181 val_acc=0.5347 train_kappa=0.2361 val_kappa=0.0694\n",
            "[Epoch 50/100] loss=0.6753 val_loss=0.6795 train_acc=0.5486 val_acc=0.5903 train_kappa=0.0972 val_kappa=0.1806\n",
            "[Epoch 51/100] loss=0.6619 val_loss=0.7075 train_acc=0.6111 val_acc=0.5069 train_kappa=0.2222 val_kappa=0.0139\n",
            "[Epoch 52/100] loss=0.6596 val_loss=0.6907 train_acc=0.6042 val_acc=0.5069 train_kappa=0.2083 val_kappa=0.0139\n",
            "[Epoch 53/100] loss=0.6752 val_loss=0.6802 train_acc=0.5972 val_acc=0.5347 train_kappa=0.1944 val_kappa=0.0694\n",
            "[Epoch 54/100] loss=0.6371 val_loss=0.6809 train_acc=0.6528 val_acc=0.5069 train_kappa=0.3056 val_kappa=0.0139\n",
            "[Epoch 55/100] loss=0.6458 val_loss=0.6833 train_acc=0.6319 val_acc=0.4931 train_kappa=0.2639 val_kappa=-0.0139\n",
            "[Epoch 56/100] loss=0.6438 val_loss=0.6733 train_acc=0.6389 val_acc=0.5625 train_kappa=0.2778 val_kappa=0.1250\n",
            "[Epoch 57/100] loss=0.6439 val_loss=0.6877 train_acc=0.6875 val_acc=0.5069 train_kappa=0.3750 val_kappa=0.0139\n",
            "[Epoch 58/100] loss=0.6191 val_loss=0.6815 train_acc=0.6458 val_acc=0.5069 train_kappa=0.2917 val_kappa=0.0139\n",
            "[Epoch 59/100] loss=0.6269 val_loss=0.6738 train_acc=0.6875 val_acc=0.5347 train_kappa=0.3750 val_kappa=0.0694\n",
            "[Epoch 60/100] loss=0.6349 val_loss=0.6740 train_acc=0.6111 val_acc=0.5556 train_kappa=0.2222 val_kappa=0.1111\n",
            "[Epoch 61/100] loss=0.6222 val_loss=0.7088 train_acc=0.6597 val_acc=0.5139 train_kappa=0.3194 val_kappa=0.0278\n",
            "[Epoch 62/100] loss=0.5937 val_loss=0.6865 train_acc=0.7153 val_acc=0.5139 train_kappa=0.4306 val_kappa=0.0278\n",
            "[Epoch 63/100] loss=0.6226 val_loss=0.6734 train_acc=0.6319 val_acc=0.5694 train_kappa=0.2639 val_kappa=0.1389\n",
            "[Epoch 64/100] loss=0.6534 val_loss=0.6815 train_acc=0.6181 val_acc=0.5208 train_kappa=0.2361 val_kappa=0.0417\n",
            "[Epoch 65/100] loss=0.6011 val_loss=0.7218 train_acc=0.7014 val_acc=0.5139 train_kappa=0.4028 val_kappa=0.0278\n",
            "[Epoch 66/100] loss=0.6189 val_loss=0.7310 train_acc=0.6597 val_acc=0.5208 train_kappa=0.3194 val_kappa=0.0417\n",
            "[Epoch 67/100] loss=0.5914 val_loss=0.7168 train_acc=0.7083 val_acc=0.5208 train_kappa=0.4167 val_kappa=0.0417\n",
            "[Epoch 68/100] loss=0.5940 val_loss=0.7098 train_acc=0.7153 val_acc=0.5069 train_kappa=0.4306 val_kappa=0.0139\n",
            "[Epoch 69/100] loss=0.5952 val_loss=0.6856 train_acc=0.6667 val_acc=0.5278 train_kappa=0.3333 val_kappa=0.0556\n",
            "[Epoch 70/100] loss=0.5956 val_loss=0.8011 train_acc=0.6806 val_acc=0.5139 train_kappa=0.3611 val_kappa=0.0278\n",
            "[Epoch 71/100] loss=0.6175 val_loss=0.6854 train_acc=0.6597 val_acc=0.5278 train_kappa=0.3194 val_kappa=0.0556\n",
            "[Epoch 72/100] loss=0.5909 val_loss=0.6657 train_acc=0.6597 val_acc=0.5903 train_kappa=0.3194 val_kappa=0.1806\n",
            "[Epoch 73/100] loss=0.5738 val_loss=0.7023 train_acc=0.7083 val_acc=0.5278 train_kappa=0.4167 val_kappa=0.0556\n",
            "[Epoch 74/100] loss=0.5933 val_loss=0.6991 train_acc=0.6528 val_acc=0.5208 train_kappa=0.3056 val_kappa=0.0417\n",
            "[Epoch 75/100] loss=0.5557 val_loss=0.7261 train_acc=0.7222 val_acc=0.5000 train_kappa=0.4444 val_kappa=0.0000\n",
            "[Epoch 76/100] loss=0.5636 val_loss=0.6580 train_acc=0.6875 val_acc=0.6389 train_kappa=0.3750 val_kappa=0.2778\n",
            "[Epoch 77/100] loss=0.5471 val_loss=0.7044 train_acc=0.6875 val_acc=0.5000 train_kappa=0.3750 val_kappa=0.0000\n",
            "[Epoch 78/100] loss=0.5464 val_loss=0.6901 train_acc=0.7569 val_acc=0.5208 train_kappa=0.5139 val_kappa=0.0417\n",
            "[Epoch 79/100] loss=0.5354 val_loss=0.6584 train_acc=0.7361 val_acc=0.6250 train_kappa=0.4722 val_kappa=0.2500\n",
            "[Epoch 80/100] loss=0.5723 val_loss=0.7625 train_acc=0.7153 val_acc=0.5069 train_kappa=0.4306 val_kappa=0.0139\n",
            "[Epoch 81/100] loss=0.5409 val_loss=0.6526 train_acc=0.7153 val_acc=0.6042 train_kappa=0.4306 val_kappa=0.2083\n",
            "[Epoch 82/100] loss=0.5391 val_loss=0.7062 train_acc=0.7014 val_acc=0.5208 train_kappa=0.4028 val_kappa=0.0417\n",
            "[Epoch 83/100] loss=0.4912 val_loss=0.7164 train_acc=0.7708 val_acc=0.5069 train_kappa=0.5417 val_kappa=0.0139\n",
            "[Epoch 84/100] loss=0.5209 val_loss=0.6689 train_acc=0.7569 val_acc=0.5625 train_kappa=0.5139 val_kappa=0.1250\n",
            "[Epoch 85/100] loss=0.4946 val_loss=0.7234 train_acc=0.7500 val_acc=0.5069 train_kappa=0.5000 val_kappa=0.0139\n",
            "[Epoch 86/100] loss=0.4998 val_loss=0.7099 train_acc=0.7778 val_acc=0.5278 train_kappa=0.5556 val_kappa=0.0556\n",
            "[Epoch 87/100] loss=0.5069 val_loss=0.6687 train_acc=0.7292 val_acc=0.5694 train_kappa=0.4583 val_kappa=0.1389\n",
            "[Epoch 88/100] loss=0.4410 val_loss=0.6908 train_acc=0.8542 val_acc=0.5556 train_kappa=0.7083 val_kappa=0.1111\n",
            "[Epoch 89/100] loss=0.4682 val_loss=0.6888 train_acc=0.8056 val_acc=0.5625 train_kappa=0.6111 val_kappa=0.1250\n",
            "[Epoch 90/100] loss=0.4603 val_loss=0.6605 train_acc=0.8194 val_acc=0.5903 train_kappa=0.6389 val_kappa=0.1806\n",
            "[Epoch 91/100] loss=0.4609 val_loss=0.6872 train_acc=0.8194 val_acc=0.5417 train_kappa=0.6389 val_kappa=0.0833\n",
            "[Epoch 92/100] loss=0.4374 val_loss=0.6536 train_acc=0.8542 val_acc=0.6111 train_kappa=0.7083 val_kappa=0.2222\n",
            "[Epoch 93/100] loss=0.4447 val_loss=0.7168 train_acc=0.8403 val_acc=0.5347 train_kappa=0.6806 val_kappa=0.0694\n",
            "[Epoch 94/100] loss=0.4177 val_loss=0.6558 train_acc=0.8542 val_acc=0.5764 train_kappa=0.7083 val_kappa=0.1528\n",
            "[Epoch 95/100] loss=0.4896 val_loss=0.7014 train_acc=0.7639 val_acc=0.5417 train_kappa=0.5278 val_kappa=0.0833\n",
            "[Epoch 96/100] loss=0.4298 val_loss=0.6742 train_acc=0.7778 val_acc=0.5903 train_kappa=0.5556 val_kappa=0.1806\n",
            "[Epoch 97/100] loss=0.4141 val_loss=0.6605 train_acc=0.8333 val_acc=0.5903 train_kappa=0.6667 val_kappa=0.1806\n",
            "[Epoch 98/100] loss=0.3696 val_loss=0.6807 train_acc=0.8750 val_acc=0.5764 train_kappa=0.7500 val_kappa=0.1528\n",
            "[Epoch 99/100] loss=0.3486 val_loss=0.6770 train_acc=0.8819 val_acc=0.5903 train_kappa=0.7639 val_kappa=0.1806\n",
            "[Epoch 100/100] loss=0.3640 val_loss=0.6752 train_acc=0.8889 val_acc=0.5764 train_kappa=0.7778 val_kappa=0.1528\n",
            "\n",
            "=== Processing Subject 6 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7020 val_loss=0.6962 train_acc=0.5208 val_acc=0.5000 train_kappa=0.0417 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7246 val_loss=0.6916 train_acc=0.4861 val_acc=0.5000 train_kappa=-0.0278 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.6911 val_loss=0.7100 train_acc=0.5556 val_acc=0.5000 train_kappa=0.1111 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.7252 val_loss=0.6910 train_acc=0.5278 val_acc=0.5000 train_kappa=0.0556 val_kappa=0.0000\n",
            "[Epoch 5/100] loss=0.6904 val_loss=0.6889 train_acc=0.5486 val_acc=0.5000 train_kappa=0.0972 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.6720 val_loss=0.6875 train_acc=0.5625 val_acc=0.6042 train_kappa=0.1250 val_kappa=0.2083\n",
            "[Epoch 7/100] loss=0.6707 val_loss=0.6942 train_acc=0.5833 val_acc=0.5000 train_kappa=0.1667 val_kappa=0.0000\n",
            "[Epoch 8/100] loss=0.6957 val_loss=0.6831 train_acc=0.5139 val_acc=0.5347 train_kappa=0.0278 val_kappa=0.0694\n",
            "[Epoch 9/100] loss=0.6962 val_loss=0.6851 train_acc=0.4792 val_acc=0.5139 train_kappa=-0.0417 val_kappa=0.0278\n",
            "[Epoch 10/100] loss=0.6719 val_loss=0.6777 train_acc=0.6042 val_acc=0.6042 train_kappa=0.2083 val_kappa=0.2083\n",
            "[Epoch 11/100] loss=0.6629 val_loss=0.6774 train_acc=0.5972 val_acc=0.6319 train_kappa=0.1944 val_kappa=0.2639\n",
            "[Epoch 12/100] loss=0.6865 val_loss=0.6804 train_acc=0.5556 val_acc=0.5556 train_kappa=0.1111 val_kappa=0.1111\n",
            "[Epoch 13/100] loss=0.6720 val_loss=0.6763 train_acc=0.5833 val_acc=0.5764 train_kappa=0.1667 val_kappa=0.1528\n",
            "[Epoch 14/100] loss=0.6943 val_loss=0.6784 train_acc=0.5486 val_acc=0.5486 train_kappa=0.0972 val_kappa=0.0972\n",
            "[Epoch 15/100] loss=0.6958 val_loss=0.6728 train_acc=0.5278 val_acc=0.5694 train_kappa=0.0556 val_kappa=0.1389\n",
            "[Epoch 16/100] loss=0.6964 val_loss=0.6761 train_acc=0.5347 val_acc=0.5625 train_kappa=0.0694 val_kappa=0.1250\n",
            "[Epoch 17/100] loss=0.6864 val_loss=0.6667 train_acc=0.5556 val_acc=0.5972 train_kappa=0.1111 val_kappa=0.1944\n",
            "[Epoch 18/100] loss=0.7149 val_loss=0.6710 train_acc=0.4375 val_acc=0.6667 train_kappa=-0.1250 val_kappa=0.3333\n",
            "[Epoch 19/100] loss=0.6909 val_loss=0.6818 train_acc=0.5486 val_acc=0.5208 train_kappa=0.0972 val_kappa=0.0417\n",
            "[Epoch 20/100] loss=0.6812 val_loss=0.6799 train_acc=0.5833 val_acc=0.5347 train_kappa=0.1667 val_kappa=0.0694\n",
            "[Epoch 21/100] loss=0.6594 val_loss=0.6768 train_acc=0.5972 val_acc=0.6250 train_kappa=0.1944 val_kappa=0.2500\n",
            "[Epoch 22/100] loss=0.6892 val_loss=0.6745 train_acc=0.5694 val_acc=0.6736 train_kappa=0.1389 val_kappa=0.3472\n",
            "[Epoch 23/100] loss=0.6698 val_loss=0.6789 train_acc=0.5833 val_acc=0.5278 train_kappa=0.1667 val_kappa=0.0556\n",
            "[Epoch 24/100] loss=0.6933 val_loss=0.6777 train_acc=0.5347 val_acc=0.5972 train_kappa=0.0694 val_kappa=0.1944\n",
            "[Epoch 25/100] loss=0.6751 val_loss=0.6759 train_acc=0.5972 val_acc=0.5694 train_kappa=0.1944 val_kappa=0.1389\n",
            "[Epoch 26/100] loss=0.6705 val_loss=0.6748 train_acc=0.6389 val_acc=0.5764 train_kappa=0.2778 val_kappa=0.1528\n",
            "[Epoch 27/100] loss=0.6435 val_loss=0.6749 train_acc=0.5972 val_acc=0.5694 train_kappa=0.1944 val_kappa=0.1389\n",
            "[Epoch 28/100] loss=0.6355 val_loss=0.6596 train_acc=0.6111 val_acc=0.5764 train_kappa=0.2222 val_kappa=0.1528\n",
            "[Epoch 29/100] loss=0.6472 val_loss=0.6520 train_acc=0.5903 val_acc=0.6111 train_kappa=0.1806 val_kappa=0.2222\n",
            "[Epoch 30/100] loss=0.6616 val_loss=0.6551 train_acc=0.5972 val_acc=0.6250 train_kappa=0.1944 val_kappa=0.2500\n",
            "[Epoch 31/100] loss=0.6111 val_loss=0.6616 train_acc=0.6667 val_acc=0.5764 train_kappa=0.3333 val_kappa=0.1528\n",
            "[Epoch 32/100] loss=0.6699 val_loss=0.6511 train_acc=0.6181 val_acc=0.6250 train_kappa=0.2361 val_kappa=0.2500\n",
            "[Epoch 33/100] loss=0.6237 val_loss=0.6399 train_acc=0.6528 val_acc=0.6389 train_kappa=0.3056 val_kappa=0.2778\n",
            "[Epoch 34/100] loss=0.6098 val_loss=0.6489 train_acc=0.7014 val_acc=0.6111 train_kappa=0.4028 val_kappa=0.2222\n",
            "[Epoch 35/100] loss=0.6215 val_loss=0.6411 train_acc=0.6597 val_acc=0.6250 train_kappa=0.3194 val_kappa=0.2500\n",
            "[Epoch 36/100] loss=0.6226 val_loss=0.6381 train_acc=0.7083 val_acc=0.6250 train_kappa=0.4167 val_kappa=0.2500\n",
            "[Epoch 37/100] loss=0.6222 val_loss=0.6451 train_acc=0.6944 val_acc=0.6181 train_kappa=0.3889 val_kappa=0.2361\n",
            "[Epoch 38/100] loss=0.6284 val_loss=0.6311 train_acc=0.6667 val_acc=0.6458 train_kappa=0.3333 val_kappa=0.2917\n",
            "[Epoch 39/100] loss=0.6133 val_loss=0.6293 train_acc=0.6667 val_acc=0.6389 train_kappa=0.3333 val_kappa=0.2778\n",
            "[Epoch 40/100] loss=0.6413 val_loss=0.6430 train_acc=0.5972 val_acc=0.5833 train_kappa=0.1944 val_kappa=0.1667\n",
            "[Epoch 41/100] loss=0.5575 val_loss=0.6346 train_acc=0.7292 val_acc=0.6181 train_kappa=0.4583 val_kappa=0.2361\n",
            "[Epoch 42/100] loss=0.5952 val_loss=0.6363 train_acc=0.6806 val_acc=0.6667 train_kappa=0.3611 val_kappa=0.3333\n",
            "[Epoch 43/100] loss=0.5949 val_loss=0.6347 train_acc=0.6528 val_acc=0.6181 train_kappa=0.3056 val_kappa=0.2361\n",
            "[Epoch 44/100] loss=0.6509 val_loss=0.6289 train_acc=0.6111 val_acc=0.6319 train_kappa=0.2222 val_kappa=0.2639\n",
            "[Epoch 45/100] loss=0.6110 val_loss=0.6264 train_acc=0.6667 val_acc=0.6389 train_kappa=0.3333 val_kappa=0.2778\n",
            "[Epoch 46/100] loss=0.6151 val_loss=0.6213 train_acc=0.6250 val_acc=0.6597 train_kappa=0.2500 val_kappa=0.3194\n",
            "[Epoch 47/100] loss=0.5591 val_loss=0.6250 train_acc=0.7083 val_acc=0.6389 train_kappa=0.4167 val_kappa=0.2778\n",
            "[Epoch 48/100] loss=0.5560 val_loss=0.6209 train_acc=0.7361 val_acc=0.6667 train_kappa=0.4722 val_kappa=0.3333\n",
            "[Epoch 49/100] loss=0.6049 val_loss=0.6124 train_acc=0.7222 val_acc=0.6736 train_kappa=0.4444 val_kappa=0.3472\n",
            "[Epoch 50/100] loss=0.5666 val_loss=0.6105 train_acc=0.6875 val_acc=0.6806 train_kappa=0.3750 val_kappa=0.3611\n",
            "[Epoch 51/100] loss=0.5270 val_loss=0.6134 train_acc=0.7431 val_acc=0.6319 train_kappa=0.4861 val_kappa=0.2639\n",
            "[Epoch 52/100] loss=0.5742 val_loss=0.6334 train_acc=0.7153 val_acc=0.6250 train_kappa=0.4306 val_kappa=0.2500\n",
            "[Epoch 53/100] loss=0.5767 val_loss=0.6262 train_acc=0.6736 val_acc=0.6389 train_kappa=0.3472 val_kappa=0.2778\n",
            "[Epoch 54/100] loss=0.5528 val_loss=0.6125 train_acc=0.7153 val_acc=0.6597 train_kappa=0.4306 val_kappa=0.3194\n",
            "[Epoch 55/100] loss=0.5387 val_loss=0.6081 train_acc=0.7431 val_acc=0.6875 train_kappa=0.4861 val_kappa=0.3750\n",
            "[Epoch 56/100] loss=0.5400 val_loss=0.6548 train_acc=0.6944 val_acc=0.5833 train_kappa=0.3889 val_kappa=0.1667\n",
            "[Epoch 57/100] loss=0.5410 val_loss=0.6158 train_acc=0.7361 val_acc=0.6736 train_kappa=0.4722 val_kappa=0.3472\n",
            "[Epoch 58/100] loss=0.4858 val_loss=0.6617 train_acc=0.7778 val_acc=0.5972 train_kappa=0.5556 val_kappa=0.1944\n",
            "[Epoch 59/100] loss=0.5128 val_loss=0.6140 train_acc=0.7639 val_acc=0.6389 train_kappa=0.5278 val_kappa=0.2778\n",
            "[Epoch 60/100] loss=0.4969 val_loss=0.6172 train_acc=0.7639 val_acc=0.6458 train_kappa=0.5278 val_kappa=0.2917\n",
            "[Epoch 61/100] loss=0.4984 val_loss=0.6194 train_acc=0.7708 val_acc=0.6528 train_kappa=0.5417 val_kappa=0.3056\n",
            "[Epoch 62/100] loss=0.5411 val_loss=0.6176 train_acc=0.7014 val_acc=0.6806 train_kappa=0.4028 val_kappa=0.3611\n",
            "[Epoch 63/100] loss=0.5808 val_loss=0.6298 train_acc=0.6458 val_acc=0.6319 train_kappa=0.2917 val_kappa=0.2639\n",
            "[Epoch 64/100] loss=0.5564 val_loss=0.7085 train_acc=0.6806 val_acc=0.6042 train_kappa=0.3611 val_kappa=0.2083\n",
            "[Epoch 65/100] loss=0.5969 val_loss=0.7659 train_acc=0.6458 val_acc=0.5694 train_kappa=0.2917 val_kappa=0.1389\n",
            "[Epoch 66/100] loss=0.5095 val_loss=0.6873 train_acc=0.7222 val_acc=0.6181 train_kappa=0.4444 val_kappa=0.2361\n",
            "[Epoch 67/100] loss=0.5051 val_loss=0.6512 train_acc=0.7153 val_acc=0.5972 train_kappa=0.4306 val_kappa=0.1944\n",
            "[Epoch 68/100] loss=0.5108 val_loss=0.6073 train_acc=0.7500 val_acc=0.6736 train_kappa=0.5000 val_kappa=0.3472\n",
            "[Epoch 69/100] loss=0.4457 val_loss=0.5999 train_acc=0.7847 val_acc=0.6597 train_kappa=0.5694 val_kappa=0.3194\n",
            "[Epoch 70/100] loss=0.4593 val_loss=0.5895 train_acc=0.7986 val_acc=0.6667 train_kappa=0.5972 val_kappa=0.3333\n",
            "[Epoch 71/100] loss=0.4664 val_loss=0.5809 train_acc=0.8194 val_acc=0.7153 train_kappa=0.6389 val_kappa=0.4306\n",
            "[Epoch 72/100] loss=0.4315 val_loss=0.6133 train_acc=0.7778 val_acc=0.6528 train_kappa=0.5556 val_kappa=0.3056\n",
            "[Epoch 73/100] loss=0.4451 val_loss=0.6274 train_acc=0.7917 val_acc=0.6111 train_kappa=0.5833 val_kappa=0.2222\n",
            "[Epoch 74/100] loss=0.4126 val_loss=0.6698 train_acc=0.8403 val_acc=0.6181 train_kappa=0.6806 val_kappa=0.2361\n",
            "[Epoch 75/100] loss=0.4441 val_loss=0.5752 train_acc=0.7917 val_acc=0.7014 train_kappa=0.5833 val_kappa=0.4028\n",
            "[Epoch 76/100] loss=0.3701 val_loss=0.5847 train_acc=0.8611 val_acc=0.7153 train_kappa=0.7222 val_kappa=0.4306\n",
            "[Epoch 77/100] loss=0.3836 val_loss=0.5833 train_acc=0.8472 val_acc=0.6944 train_kappa=0.6944 val_kappa=0.3889\n",
            "[Epoch 78/100] loss=0.3752 val_loss=0.5760 train_acc=0.8611 val_acc=0.7083 train_kappa=0.7222 val_kappa=0.4167\n",
            "[Epoch 79/100] loss=0.3374 val_loss=0.6106 train_acc=0.9097 val_acc=0.6458 train_kappa=0.8194 val_kappa=0.2917\n",
            "[Epoch 80/100] loss=0.3613 val_loss=0.5845 train_acc=0.8611 val_acc=0.7014 train_kappa=0.7222 val_kappa=0.4028\n",
            "[Epoch 81/100] loss=0.3460 val_loss=0.5821 train_acc=0.8611 val_acc=0.7153 train_kappa=0.7222 val_kappa=0.4306\n",
            "[Epoch 82/100] loss=0.3356 val_loss=0.5931 train_acc=0.8681 val_acc=0.7083 train_kappa=0.7361 val_kappa=0.4167\n",
            "[Epoch 83/100] loss=0.3524 val_loss=0.6125 train_acc=0.8611 val_acc=0.6736 train_kappa=0.7222 val_kappa=0.3472\n",
            "[Epoch 84/100] loss=0.2917 val_loss=0.6290 train_acc=0.9167 val_acc=0.6667 train_kappa=0.8333 val_kappa=0.3333\n",
            "[Epoch 85/100] loss=0.3064 val_loss=0.6101 train_acc=0.8958 val_acc=0.7083 train_kappa=0.7917 val_kappa=0.4167\n",
            "[Epoch 86/100] loss=0.2906 val_loss=0.6151 train_acc=0.8889 val_acc=0.7361 train_kappa=0.7778 val_kappa=0.4722\n",
            "[Epoch 87/100] loss=0.3020 val_loss=0.6049 train_acc=0.8819 val_acc=0.6944 train_kappa=0.7639 val_kappa=0.3889\n",
            "[Epoch 88/100] loss=0.2471 val_loss=0.6462 train_acc=0.9236 val_acc=0.6458 train_kappa=0.8472 val_kappa=0.2917\n",
            "[Epoch 89/100] loss=0.2941 val_loss=0.6316 train_acc=0.8750 val_acc=0.6806 train_kappa=0.7500 val_kappa=0.3611\n",
            "[Epoch 90/100] loss=0.2699 val_loss=0.6021 train_acc=0.9236 val_acc=0.7014 train_kappa=0.8472 val_kappa=0.4028\n",
            "[Epoch 91/100] loss=0.2598 val_loss=0.5840 train_acc=0.8958 val_acc=0.7292 train_kappa=0.7917 val_kappa=0.4583\n",
            "[Epoch 92/100] loss=0.2244 val_loss=0.5834 train_acc=0.9444 val_acc=0.7222 train_kappa=0.8889 val_kappa=0.4444\n",
            "[Epoch 93/100] loss=0.2237 val_loss=0.5848 train_acc=0.9306 val_acc=0.7222 train_kappa=0.8611 val_kappa=0.4444\n",
            "[Epoch 94/100] loss=0.2436 val_loss=0.5840 train_acc=0.9306 val_acc=0.7153 train_kappa=0.8611 val_kappa=0.4306\n",
            "[Epoch 95/100] loss=0.1980 val_loss=0.6037 train_acc=0.9444 val_acc=0.7153 train_kappa=0.8889 val_kappa=0.4306\n",
            "[Epoch 96/100] loss=0.1899 val_loss=0.6653 train_acc=0.9653 val_acc=0.7083 train_kappa=0.9306 val_kappa=0.4167\n",
            "[Epoch 97/100] loss=0.1969 val_loss=0.6299 train_acc=0.9444 val_acc=0.6944 train_kappa=0.8889 val_kappa=0.3889\n",
            "[Epoch 98/100] loss=0.2208 val_loss=0.6180 train_acc=0.9444 val_acc=0.6875 train_kappa=0.8889 val_kappa=0.3750\n",
            "[Epoch 99/100] loss=0.2302 val_loss=0.5838 train_acc=0.8958 val_acc=0.7153 train_kappa=0.7917 val_kappa=0.4306\n",
            "[Epoch 100/100] loss=0.2010 val_loss=0.5770 train_acc=0.9444 val_acc=0.7083 train_kappa=0.8889 val_kappa=0.4167\n",
            "\n",
            "=== Processing Subject 7 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7689 val_loss=0.7091 train_acc=0.4236 val_acc=0.5000 train_kappa=-0.1528 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7328 val_loss=0.7212 train_acc=0.4375 val_acc=0.5000 train_kappa=-0.1250 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.7032 val_loss=0.6903 train_acc=0.5417 val_acc=0.5486 train_kappa=0.0833 val_kappa=0.0972\n",
            "[Epoch 4/100] loss=0.6981 val_loss=0.7066 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 5/100] loss=0.7112 val_loss=0.6915 train_acc=0.4792 val_acc=0.5000 train_kappa=-0.0417 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.6872 val_loss=0.6907 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 7/100] loss=0.7000 val_loss=0.6907 train_acc=0.5139 val_acc=0.5417 train_kappa=0.0278 val_kappa=0.0833\n",
            "[Epoch 8/100] loss=0.7036 val_loss=0.6963 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 9/100] loss=0.6959 val_loss=0.6907 train_acc=0.4861 val_acc=0.5486 train_kappa=-0.0278 val_kappa=0.0972\n",
            "[Epoch 10/100] loss=0.6842 val_loss=0.6962 train_acc=0.5208 val_acc=0.5000 train_kappa=0.0417 val_kappa=0.0000\n",
            "[Epoch 11/100] loss=0.7066 val_loss=0.6898 train_acc=0.4722 val_acc=0.5417 train_kappa=-0.0556 val_kappa=0.0833\n",
            "[Epoch 12/100] loss=0.7063 val_loss=0.6912 train_acc=0.4722 val_acc=0.5069 train_kappa=-0.0556 val_kappa=0.0139\n",
            "[Epoch 13/100] loss=0.6995 val_loss=0.6890 train_acc=0.5556 val_acc=0.5278 train_kappa=0.1111 val_kappa=0.0556\n",
            "[Epoch 14/100] loss=0.6867 val_loss=0.6921 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 15/100] loss=0.7012 val_loss=0.6888 train_acc=0.4722 val_acc=0.5278 train_kappa=-0.0556 val_kappa=0.0556\n",
            "[Epoch 16/100] loss=0.6734 val_loss=0.6899 train_acc=0.5903 val_acc=0.5139 train_kappa=0.1806 val_kappa=0.0278\n",
            "[Epoch 17/100] loss=0.7071 val_loss=0.6887 train_acc=0.4306 val_acc=0.5417 train_kappa=-0.1389 val_kappa=0.0833\n",
            "[Epoch 18/100] loss=0.6903 val_loss=0.6891 train_acc=0.5069 val_acc=0.5347 train_kappa=0.0139 val_kappa=0.0694\n",
            "[Epoch 19/100] loss=0.6785 val_loss=0.6880 train_acc=0.6111 val_acc=0.6042 train_kappa=0.2222 val_kappa=0.2083\n",
            "[Epoch 20/100] loss=0.6716 val_loss=0.6903 train_acc=0.6389 val_acc=0.5347 train_kappa=0.2778 val_kappa=0.0694\n",
            "[Epoch 21/100] loss=0.6915 val_loss=0.6861 train_acc=0.5417 val_acc=0.5486 train_kappa=0.0833 val_kappa=0.0972\n",
            "[Epoch 22/100] loss=0.6872 val_loss=0.6856 train_acc=0.5556 val_acc=0.5972 train_kappa=0.1111 val_kappa=0.1944\n",
            "[Epoch 23/100] loss=0.6668 val_loss=0.6896 train_acc=0.5972 val_acc=0.5208 train_kappa=0.1944 val_kappa=0.0417\n",
            "[Epoch 24/100] loss=0.6873 val_loss=0.6901 train_acc=0.5764 val_acc=0.5556 train_kappa=0.1528 val_kappa=0.1111\n",
            "[Epoch 25/100] loss=0.6731 val_loss=0.6911 train_acc=0.5556 val_acc=0.5625 train_kappa=0.1111 val_kappa=0.1250\n",
            "[Epoch 26/100] loss=0.6541 val_loss=0.6970 train_acc=0.5972 val_acc=0.5139 train_kappa=0.1944 val_kappa=0.0278\n",
            "[Epoch 27/100] loss=0.6675 val_loss=0.6841 train_acc=0.5972 val_acc=0.5347 train_kappa=0.1944 val_kappa=0.0694\n",
            "[Epoch 28/100] loss=0.6855 val_loss=0.6843 train_acc=0.5556 val_acc=0.5417 train_kappa=0.1111 val_kappa=0.0833\n",
            "[Epoch 29/100] loss=0.6612 val_loss=0.6835 train_acc=0.5833 val_acc=0.5486 train_kappa=0.1667 val_kappa=0.0972\n",
            "[Epoch 30/100] loss=0.6398 val_loss=0.6994 train_acc=0.6528 val_acc=0.5625 train_kappa=0.3056 val_kappa=0.1250\n",
            "[Epoch 31/100] loss=0.6837 val_loss=0.6861 train_acc=0.5278 val_acc=0.5278 train_kappa=0.0556 val_kappa=0.0556\n",
            "[Epoch 32/100] loss=0.6908 val_loss=0.7539 train_acc=0.5556 val_acc=0.5000 train_kappa=0.1111 val_kappa=0.0000\n",
            "[Epoch 33/100] loss=0.6750 val_loss=0.7046 train_acc=0.5833 val_acc=0.5417 train_kappa=0.1667 val_kappa=0.0833\n",
            "[Epoch 34/100] loss=0.6592 val_loss=0.6964 train_acc=0.5972 val_acc=0.5556 train_kappa=0.1944 val_kappa=0.1111\n",
            "[Epoch 35/100] loss=0.6205 val_loss=0.6925 train_acc=0.7153 val_acc=0.5278 train_kappa=0.4306 val_kappa=0.0556\n",
            "[Epoch 36/100] loss=0.6636 val_loss=0.6834 train_acc=0.6250 val_acc=0.5556 train_kappa=0.2500 val_kappa=0.1111\n",
            "[Epoch 37/100] loss=0.5864 val_loss=0.6856 train_acc=0.7639 val_acc=0.5486 train_kappa=0.5278 val_kappa=0.0972\n",
            "[Epoch 38/100] loss=0.6318 val_loss=0.6830 train_acc=0.6528 val_acc=0.5625 train_kappa=0.3056 val_kappa=0.1250\n",
            "[Epoch 39/100] loss=0.6315 val_loss=0.6913 train_acc=0.6736 val_acc=0.5347 train_kappa=0.3472 val_kappa=0.0694\n",
            "[Epoch 40/100] loss=0.6325 val_loss=0.6943 train_acc=0.6806 val_acc=0.5347 train_kappa=0.3611 val_kappa=0.0694\n",
            "[Epoch 41/100] loss=0.6321 val_loss=0.6865 train_acc=0.6250 val_acc=0.5625 train_kappa=0.2500 val_kappa=0.1250\n",
            "[Epoch 42/100] loss=0.6176 val_loss=0.6994 train_acc=0.6667 val_acc=0.5139 train_kappa=0.3333 val_kappa=0.0278\n",
            "[Epoch 43/100] loss=0.6213 val_loss=0.6743 train_acc=0.6319 val_acc=0.5833 train_kappa=0.2639 val_kappa=0.1667\n",
            "[Epoch 44/100] loss=0.6280 val_loss=0.6745 train_acc=0.6597 val_acc=0.6667 train_kappa=0.3194 val_kappa=0.3333\n",
            "[Epoch 45/100] loss=0.5985 val_loss=0.6758 train_acc=0.7083 val_acc=0.5347 train_kappa=0.4167 val_kappa=0.0694\n",
            "[Epoch 46/100] loss=0.6046 val_loss=0.6673 train_acc=0.7014 val_acc=0.6181 train_kappa=0.4028 val_kappa=0.2361\n",
            "[Epoch 47/100] loss=0.5871 val_loss=0.6772 train_acc=0.7083 val_acc=0.5486 train_kappa=0.4167 val_kappa=0.0972\n",
            "[Epoch 48/100] loss=0.5745 val_loss=0.6687 train_acc=0.7222 val_acc=0.5903 train_kappa=0.4444 val_kappa=0.1806\n",
            "[Epoch 49/100] loss=0.5492 val_loss=0.6591 train_acc=0.7847 val_acc=0.5625 train_kappa=0.5694 val_kappa=0.1250\n",
            "[Epoch 50/100] loss=0.5953 val_loss=0.6708 train_acc=0.6528 val_acc=0.5625 train_kappa=0.3056 val_kappa=0.1250\n",
            "[Epoch 51/100] loss=0.5919 val_loss=0.6518 train_acc=0.6944 val_acc=0.5972 train_kappa=0.3889 val_kappa=0.1944\n",
            "[Epoch 52/100] loss=0.5831 val_loss=0.6477 train_acc=0.6875 val_acc=0.6458 train_kappa=0.3750 val_kappa=0.2917\n",
            "[Epoch 53/100] loss=0.5586 val_loss=0.6910 train_acc=0.7222 val_acc=0.5903 train_kappa=0.4444 val_kappa=0.1806\n",
            "[Epoch 54/100] loss=0.5146 val_loss=0.6528 train_acc=0.7778 val_acc=0.6111 train_kappa=0.5556 val_kappa=0.2222\n",
            "[Epoch 55/100] loss=0.5164 val_loss=0.7356 train_acc=0.7986 val_acc=0.5764 train_kappa=0.5972 val_kappa=0.1528\n",
            "[Epoch 56/100] loss=0.5615 val_loss=0.6135 train_acc=0.7292 val_acc=0.6944 train_kappa=0.4583 val_kappa=0.3889\n",
            "[Epoch 57/100] loss=0.4764 val_loss=0.6408 train_acc=0.7847 val_acc=0.6181 train_kappa=0.5694 val_kappa=0.2361\n",
            "[Epoch 58/100] loss=0.4696 val_loss=0.6331 train_acc=0.7986 val_acc=0.6181 train_kappa=0.5972 val_kappa=0.2361\n",
            "[Epoch 59/100] loss=0.4750 val_loss=0.6561 train_acc=0.7917 val_acc=0.6181 train_kappa=0.5833 val_kappa=0.2361\n",
            "[Epoch 60/100] loss=0.4672 val_loss=0.6238 train_acc=0.7847 val_acc=0.6875 train_kappa=0.5694 val_kappa=0.3750\n",
            "[Epoch 61/100] loss=0.4541 val_loss=0.7545 train_acc=0.7986 val_acc=0.5903 train_kappa=0.5972 val_kappa=0.1806\n",
            "[Epoch 62/100] loss=0.4170 val_loss=0.6363 train_acc=0.8472 val_acc=0.7014 train_kappa=0.6944 val_kappa=0.4028\n",
            "[Epoch 63/100] loss=0.4479 val_loss=0.6405 train_acc=0.8194 val_acc=0.6458 train_kappa=0.6389 val_kappa=0.2917\n",
            "[Epoch 64/100] loss=0.3929 val_loss=0.6164 train_acc=0.8542 val_acc=0.6806 train_kappa=0.7083 val_kappa=0.3611\n",
            "[Epoch 65/100] loss=0.4001 val_loss=0.6903 train_acc=0.8264 val_acc=0.6250 train_kappa=0.6528 val_kappa=0.2500\n",
            "[Epoch 66/100] loss=0.3390 val_loss=0.6103 train_acc=0.9097 val_acc=0.6806 train_kappa=0.8194 val_kappa=0.3611\n",
            "[Epoch 67/100] loss=0.3740 val_loss=0.6144 train_acc=0.8681 val_acc=0.6667 train_kappa=0.7361 val_kappa=0.3333\n",
            "[Epoch 68/100] loss=0.3429 val_loss=0.6249 train_acc=0.9028 val_acc=0.6667 train_kappa=0.8056 val_kappa=0.3333\n",
            "[Epoch 69/100] loss=0.3277 val_loss=0.6488 train_acc=0.9167 val_acc=0.6736 train_kappa=0.8333 val_kappa=0.3472\n",
            "[Epoch 70/100] loss=0.3501 val_loss=0.6151 train_acc=0.8681 val_acc=0.6944 train_kappa=0.7361 val_kappa=0.3889\n",
            "[Epoch 71/100] loss=0.4246 val_loss=0.6556 train_acc=0.8056 val_acc=0.6806 train_kappa=0.6111 val_kappa=0.3611\n",
            "[Epoch 72/100] loss=0.3793 val_loss=0.7037 train_acc=0.8125 val_acc=0.6667 train_kappa=0.6250 val_kappa=0.3333\n",
            "[Epoch 73/100] loss=0.3948 val_loss=0.5987 train_acc=0.7986 val_acc=0.7153 train_kappa=0.5972 val_kappa=0.4306\n",
            "[Epoch 74/100] loss=0.3459 val_loss=0.6566 train_acc=0.8681 val_acc=0.6667 train_kappa=0.7361 val_kappa=0.3333\n",
            "[Epoch 75/100] loss=0.2643 val_loss=0.5820 train_acc=0.9097 val_acc=0.7431 train_kappa=0.8194 val_kappa=0.4861\n",
            "[Epoch 76/100] loss=0.2969 val_loss=0.5905 train_acc=0.8819 val_acc=0.7361 train_kappa=0.7639 val_kappa=0.4722\n",
            "[Epoch 77/100] loss=0.2602 val_loss=0.6323 train_acc=0.9306 val_acc=0.7153 train_kappa=0.8611 val_kappa=0.4306\n",
            "[Epoch 78/100] loss=0.3087 val_loss=0.7503 train_acc=0.8750 val_acc=0.6111 train_kappa=0.7500 val_kappa=0.2222\n",
            "[Epoch 79/100] loss=0.3170 val_loss=0.7785 train_acc=0.8542 val_acc=0.7014 train_kappa=0.7083 val_kappa=0.4028\n",
            "[Epoch 80/100] loss=0.2278 val_loss=0.7528 train_acc=0.9028 val_acc=0.6111 train_kappa=0.8056 val_kappa=0.2222\n",
            "[Epoch 81/100] loss=0.2323 val_loss=0.6745 train_acc=0.9306 val_acc=0.7778 train_kappa=0.8611 val_kappa=0.5556\n",
            "[Epoch 82/100] loss=0.2654 val_loss=0.6522 train_acc=0.8819 val_acc=0.7014 train_kappa=0.7639 val_kappa=0.4028\n",
            "[Epoch 83/100] loss=0.1965 val_loss=0.6927 train_acc=0.9444 val_acc=0.7222 train_kappa=0.8889 val_kappa=0.4444\n",
            "[Epoch 84/100] loss=0.1913 val_loss=0.5707 train_acc=0.9306 val_acc=0.7500 train_kappa=0.8611 val_kappa=0.5000\n",
            "[Epoch 85/100] loss=0.2135 val_loss=0.5983 train_acc=0.9236 val_acc=0.7292 train_kappa=0.8472 val_kappa=0.4583\n",
            "[Epoch 86/100] loss=0.2229 val_loss=0.6230 train_acc=0.9444 val_acc=0.7361 train_kappa=0.8889 val_kappa=0.4722\n",
            "[Epoch 87/100] loss=0.1547 val_loss=0.5643 train_acc=0.9583 val_acc=0.7361 train_kappa=0.9167 val_kappa=0.4722\n",
            "[Epoch 88/100] loss=0.1560 val_loss=0.7078 train_acc=0.9653 val_acc=0.6806 train_kappa=0.9306 val_kappa=0.3611\n",
            "[Epoch 89/100] loss=0.1766 val_loss=0.6202 train_acc=0.9514 val_acc=0.7500 train_kappa=0.9028 val_kappa=0.5000\n",
            "[Epoch 90/100] loss=0.1858 val_loss=0.7794 train_acc=0.9097 val_acc=0.6944 train_kappa=0.8194 val_kappa=0.3889\n",
            "[Epoch 91/100] loss=0.1604 val_loss=0.6685 train_acc=0.9375 val_acc=0.7639 train_kappa=0.8750 val_kappa=0.5278\n",
            "[Epoch 92/100] loss=0.2011 val_loss=0.6284 train_acc=0.9167 val_acc=0.7431 train_kappa=0.8333 val_kappa=0.4861\n",
            "[Epoch 93/100] loss=0.1764 val_loss=0.6377 train_acc=0.9236 val_acc=0.7361 train_kappa=0.8472 val_kappa=0.4722\n",
            "[Epoch 94/100] loss=0.1146 val_loss=0.7537 train_acc=0.9583 val_acc=0.7292 train_kappa=0.9167 val_kappa=0.4583\n",
            "[Epoch 95/100] loss=0.1559 val_loss=0.5994 train_acc=0.9236 val_acc=0.7639 train_kappa=0.8472 val_kappa=0.5278\n",
            "[Epoch 96/100] loss=0.1166 val_loss=0.5774 train_acc=0.9583 val_acc=0.8056 train_kappa=0.9167 val_kappa=0.6111\n",
            "[Epoch 97/100] loss=0.1201 val_loss=0.6408 train_acc=0.9722 val_acc=0.7639 train_kappa=0.9444 val_kappa=0.5278\n",
            "[Epoch 98/100] loss=0.1570 val_loss=0.7495 train_acc=0.9444 val_acc=0.6736 train_kappa=0.8889 val_kappa=0.3472\n",
            "[Epoch 99/100] loss=0.2050 val_loss=0.7603 train_acc=0.8958 val_acc=0.7361 train_kappa=0.7917 val_kappa=0.4722\n",
            "[Epoch 100/100] loss=0.1146 val_loss=0.6343 train_acc=0.9722 val_acc=0.7292 train_kappa=0.9444 val_kappa=0.4583\n",
            "\n",
            "=== Processing Subject 8 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7103 val_loss=0.6990 train_acc=0.5139 val_acc=0.5000 train_kappa=0.0278 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.6958 val_loss=0.6945 train_acc=0.5000 val_acc=0.4931 train_kappa=0.0000 val_kappa=-0.0139\n",
            "[Epoch 3/100] loss=0.7000 val_loss=0.6938 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.6915 val_loss=0.6920 train_acc=0.5208 val_acc=0.5139 train_kappa=0.0417 val_kappa=0.0278\n",
            "[Epoch 5/100] loss=0.6976 val_loss=0.6921 train_acc=0.5694 val_acc=0.5347 train_kappa=0.1389 val_kappa=0.0694\n",
            "[Epoch 6/100] loss=0.7183 val_loss=0.6917 train_acc=0.4653 val_acc=0.5000 train_kappa=-0.0694 val_kappa=0.0000\n",
            "[Epoch 7/100] loss=0.6825 val_loss=0.6911 train_acc=0.5625 val_acc=0.5000 train_kappa=0.1250 val_kappa=0.0000\n",
            "[Epoch 8/100] loss=0.6852 val_loss=0.6866 train_acc=0.5347 val_acc=0.5278 train_kappa=0.0694 val_kappa=0.0556\n",
            "[Epoch 9/100] loss=0.7175 val_loss=0.6893 train_acc=0.5000 val_acc=0.5000 train_kappa=0.0000 val_kappa=0.0000\n",
            "[Epoch 10/100] loss=0.6922 val_loss=0.6778 train_acc=0.5903 val_acc=0.6319 train_kappa=0.1806 val_kappa=0.2639\n",
            "[Epoch 11/100] loss=0.6864 val_loss=0.6745 train_acc=0.6111 val_acc=0.6319 train_kappa=0.2222 val_kappa=0.2639\n",
            "[Epoch 12/100] loss=0.7031 val_loss=0.6706 train_acc=0.5347 val_acc=0.5833 train_kappa=0.0694 val_kappa=0.1667\n",
            "[Epoch 13/100] loss=0.6946 val_loss=0.6729 train_acc=0.5208 val_acc=0.5764 train_kappa=0.0417 val_kappa=0.1528\n",
            "[Epoch 14/100] loss=0.6908 val_loss=0.6748 train_acc=0.5417 val_acc=0.5347 train_kappa=0.0833 val_kappa=0.0694\n",
            "[Epoch 15/100] loss=0.6833 val_loss=0.6680 train_acc=0.5347 val_acc=0.6667 train_kappa=0.0694 val_kappa=0.3333\n",
            "[Epoch 16/100] loss=0.6717 val_loss=0.6599 train_acc=0.5903 val_acc=0.7083 train_kappa=0.1806 val_kappa=0.4167\n",
            "[Epoch 17/100] loss=0.6670 val_loss=0.6570 train_acc=0.5625 val_acc=0.6597 train_kappa=0.1250 val_kappa=0.3194\n",
            "[Epoch 18/100] loss=0.6793 val_loss=0.6496 train_acc=0.5694 val_acc=0.6944 train_kappa=0.1389 val_kappa=0.3889\n",
            "[Epoch 19/100] loss=0.6499 val_loss=0.6606 train_acc=0.6250 val_acc=0.5347 train_kappa=0.2500 val_kappa=0.0694\n",
            "[Epoch 20/100] loss=0.6592 val_loss=0.6626 train_acc=0.6042 val_acc=0.5417 train_kappa=0.2083 val_kappa=0.0833\n",
            "[Epoch 21/100] loss=0.6559 val_loss=0.6632 train_acc=0.5903 val_acc=0.7500 train_kappa=0.1806 val_kappa=0.5000\n",
            "[Epoch 22/100] loss=0.6683 val_loss=0.6691 train_acc=0.5417 val_acc=0.7569 train_kappa=0.0833 val_kappa=0.5139\n",
            "[Epoch 23/100] loss=0.6705 val_loss=0.6823 train_acc=0.6042 val_acc=0.5000 train_kappa=0.2083 val_kappa=0.0000\n",
            "[Epoch 24/100] loss=0.6446 val_loss=0.6685 train_acc=0.5972 val_acc=0.5139 train_kappa=0.1944 val_kappa=0.0278\n",
            "[Epoch 25/100] loss=0.6299 val_loss=0.6526 train_acc=0.6875 val_acc=0.7014 train_kappa=0.3750 val_kappa=0.4028\n",
            "[Epoch 26/100] loss=0.6353 val_loss=0.6373 train_acc=0.6528 val_acc=0.7361 train_kappa=0.3056 val_kappa=0.4722\n",
            "[Epoch 27/100] loss=0.6469 val_loss=0.6372 train_acc=0.6181 val_acc=0.5347 train_kappa=0.2361 val_kappa=0.0694\n",
            "[Epoch 28/100] loss=0.6581 val_loss=0.6187 train_acc=0.6111 val_acc=0.7986 train_kappa=0.2222 val_kappa=0.5972\n",
            "[Epoch 29/100] loss=0.6216 val_loss=0.5998 train_acc=0.6736 val_acc=0.7986 train_kappa=0.3472 val_kappa=0.5972\n",
            "[Epoch 30/100] loss=0.6052 val_loss=0.5915 train_acc=0.7014 val_acc=0.7708 train_kappa=0.4028 val_kappa=0.5417\n",
            "[Epoch 31/100] loss=0.5992 val_loss=0.5743 train_acc=0.6736 val_acc=0.7847 train_kappa=0.3472 val_kappa=0.5694\n",
            "[Epoch 32/100] loss=0.6125 val_loss=0.5678 train_acc=0.7083 val_acc=0.7292 train_kappa=0.4167 val_kappa=0.4583\n",
            "[Epoch 33/100] loss=0.5723 val_loss=0.5319 train_acc=0.7431 val_acc=0.7778 train_kappa=0.4861 val_kappa=0.5556\n",
            "[Epoch 34/100] loss=0.5899 val_loss=0.5127 train_acc=0.6667 val_acc=0.7917 train_kappa=0.3333 val_kappa=0.5833\n",
            "[Epoch 35/100] loss=0.6361 val_loss=0.5527 train_acc=0.6250 val_acc=0.7083 train_kappa=0.2500 val_kappa=0.4167\n",
            "[Epoch 36/100] loss=0.5763 val_loss=0.4989 train_acc=0.6875 val_acc=0.7917 train_kappa=0.3750 val_kappa=0.5833\n",
            "[Epoch 37/100] loss=0.5342 val_loss=0.5245 train_acc=0.7778 val_acc=0.7222 train_kappa=0.5556 val_kappa=0.4444\n",
            "[Epoch 38/100] loss=0.5078 val_loss=0.4688 train_acc=0.8194 val_acc=0.8194 train_kappa=0.6389 val_kappa=0.6389\n",
            "[Epoch 39/100] loss=0.4975 val_loss=0.4608 train_acc=0.7778 val_acc=0.7917 train_kappa=0.5556 val_kappa=0.5833\n",
            "[Epoch 40/100] loss=0.5060 val_loss=0.4639 train_acc=0.7569 val_acc=0.7778 train_kappa=0.5139 val_kappa=0.5556\n",
            "[Epoch 41/100] loss=0.4622 val_loss=0.4451 train_acc=0.7639 val_acc=0.7986 train_kappa=0.5278 val_kappa=0.5972\n",
            "[Epoch 42/100] loss=0.4257 val_loss=0.5025 train_acc=0.8403 val_acc=0.7153 train_kappa=0.6806 val_kappa=0.4306\n",
            "[Epoch 43/100] loss=0.4554 val_loss=0.4300 train_acc=0.7986 val_acc=0.8194 train_kappa=0.5972 val_kappa=0.6389\n",
            "[Epoch 44/100] loss=0.4493 val_loss=0.4550 train_acc=0.8125 val_acc=0.7569 train_kappa=0.6250 val_kappa=0.5139\n",
            "[Epoch 45/100] loss=0.4125 val_loss=0.3962 train_acc=0.8194 val_acc=0.8333 train_kappa=0.6389 val_kappa=0.6667\n",
            "[Epoch 46/100] loss=0.4321 val_loss=0.4708 train_acc=0.7917 val_acc=0.7361 train_kappa=0.5833 val_kappa=0.4722\n",
            "[Epoch 47/100] loss=0.4560 val_loss=0.3762 train_acc=0.7361 val_acc=0.8194 train_kappa=0.4722 val_kappa=0.6389\n",
            "[Epoch 48/100] loss=0.4109 val_loss=0.3812 train_acc=0.7986 val_acc=0.8403 train_kappa=0.5972 val_kappa=0.6806\n",
            "[Epoch 49/100] loss=0.4335 val_loss=0.4642 train_acc=0.7847 val_acc=0.7569 train_kappa=0.5694 val_kappa=0.5139\n",
            "[Epoch 50/100] loss=0.4212 val_loss=0.4805 train_acc=0.8056 val_acc=0.7569 train_kappa=0.6111 val_kappa=0.5139\n",
            "[Epoch 51/100] loss=0.4216 val_loss=0.4279 train_acc=0.8333 val_acc=0.7708 train_kappa=0.6667 val_kappa=0.5417\n",
            "[Epoch 52/100] loss=0.3436 val_loss=0.3613 train_acc=0.8472 val_acc=0.8194 train_kappa=0.6944 val_kappa=0.6389\n",
            "[Epoch 53/100] loss=0.3359 val_loss=0.3606 train_acc=0.8681 val_acc=0.8194 train_kappa=0.7361 val_kappa=0.6389\n",
            "[Epoch 54/100] loss=0.3277 val_loss=0.3556 train_acc=0.9097 val_acc=0.8125 train_kappa=0.8194 val_kappa=0.6250\n",
            "[Epoch 55/100] loss=0.3077 val_loss=0.3539 train_acc=0.8889 val_acc=0.8403 train_kappa=0.7778 val_kappa=0.6806\n",
            "[Epoch 56/100] loss=0.3065 val_loss=0.3592 train_acc=0.8819 val_acc=0.8125 train_kappa=0.7639 val_kappa=0.6250\n",
            "[Epoch 57/100] loss=0.2833 val_loss=0.3421 train_acc=0.9097 val_acc=0.8333 train_kappa=0.8194 val_kappa=0.6667\n",
            "[Epoch 58/100] loss=0.2938 val_loss=0.3524 train_acc=0.9167 val_acc=0.8264 train_kappa=0.8333 val_kappa=0.6528\n",
            "[Epoch 59/100] loss=0.2775 val_loss=0.3444 train_acc=0.8819 val_acc=0.8681 train_kappa=0.7639 val_kappa=0.7361\n",
            "[Epoch 60/100] loss=0.3242 val_loss=0.3564 train_acc=0.8542 val_acc=0.8264 train_kappa=0.7083 val_kappa=0.6528\n",
            "[Epoch 61/100] loss=0.3025 val_loss=0.3381 train_acc=0.8958 val_acc=0.8403 train_kappa=0.7917 val_kappa=0.6806\n",
            "[Epoch 62/100] loss=0.2578 val_loss=0.3361 train_acc=0.9167 val_acc=0.8542 train_kappa=0.8333 val_kappa=0.7083\n",
            "[Epoch 63/100] loss=0.2673 val_loss=0.3753 train_acc=0.8958 val_acc=0.8125 train_kappa=0.7917 val_kappa=0.6250\n",
            "[Epoch 64/100] loss=0.3008 val_loss=0.3390 train_acc=0.8681 val_acc=0.8472 train_kappa=0.7361 val_kappa=0.6944\n",
            "[Epoch 65/100] loss=0.2621 val_loss=0.3377 train_acc=0.9028 val_acc=0.8472 train_kappa=0.8056 val_kappa=0.6944\n",
            "[Epoch 66/100] loss=0.2642 val_loss=0.3451 train_acc=0.8889 val_acc=0.8472 train_kappa=0.7778 val_kappa=0.6944\n",
            "[Epoch 67/100] loss=0.2781 val_loss=0.3596 train_acc=0.8889 val_acc=0.8264 train_kappa=0.7778 val_kappa=0.6528\n",
            "[Epoch 68/100] loss=0.2640 val_loss=0.3276 train_acc=0.9028 val_acc=0.8681 train_kappa=0.8056 val_kappa=0.7361\n",
            "[Epoch 69/100] loss=0.2717 val_loss=0.3200 train_acc=0.8819 val_acc=0.8472 train_kappa=0.7639 val_kappa=0.6944\n",
            "[Epoch 70/100] loss=0.2083 val_loss=0.3365 train_acc=0.9236 val_acc=0.8333 train_kappa=0.8472 val_kappa=0.6667\n",
            "[Epoch 71/100] loss=0.2708 val_loss=0.3264 train_acc=0.8819 val_acc=0.8333 train_kappa=0.7639 val_kappa=0.6667\n",
            "[Epoch 72/100] loss=0.2624 val_loss=0.3394 train_acc=0.8819 val_acc=0.8403 train_kappa=0.7639 val_kappa=0.6806\n",
            "[Epoch 73/100] loss=0.2746 val_loss=0.3449 train_acc=0.8681 val_acc=0.8542 train_kappa=0.7361 val_kappa=0.7083\n",
            "[Epoch 74/100] loss=0.1885 val_loss=0.4279 train_acc=0.9444 val_acc=0.7917 train_kappa=0.8889 val_kappa=0.5833\n",
            "[Epoch 75/100] loss=0.2382 val_loss=0.3636 train_acc=0.9028 val_acc=0.8472 train_kappa=0.8056 val_kappa=0.6944\n",
            "[Epoch 76/100] loss=0.2050 val_loss=0.3240 train_acc=0.9167 val_acc=0.8542 train_kappa=0.8333 val_kappa=0.7083\n",
            "[Epoch 77/100] loss=0.2247 val_loss=0.3614 train_acc=0.8958 val_acc=0.8472 train_kappa=0.7917 val_kappa=0.6944\n",
            "[Epoch 78/100] loss=0.1824 val_loss=0.3392 train_acc=0.9444 val_acc=0.8611 train_kappa=0.8889 val_kappa=0.7222\n",
            "[Epoch 79/100] loss=0.1766 val_loss=0.3324 train_acc=0.9583 val_acc=0.8681 train_kappa=0.9167 val_kappa=0.7361\n",
            "[Epoch 80/100] loss=0.1963 val_loss=0.3319 train_acc=0.9444 val_acc=0.8681 train_kappa=0.8889 val_kappa=0.7361\n",
            "[Epoch 81/100] loss=0.1890 val_loss=0.3357 train_acc=0.9306 val_acc=0.8611 train_kappa=0.8611 val_kappa=0.7222\n",
            "[Epoch 82/100] loss=0.1804 val_loss=0.3611 train_acc=0.9444 val_acc=0.8333 train_kappa=0.8889 val_kappa=0.6667\n",
            "[Epoch 83/100] loss=0.1424 val_loss=0.3539 train_acc=0.9514 val_acc=0.8611 train_kappa=0.9028 val_kappa=0.7222\n",
            "[Epoch 84/100] loss=0.1606 val_loss=0.3538 train_acc=0.9514 val_acc=0.8472 train_kappa=0.9028 val_kappa=0.6944\n",
            "[Epoch 85/100] loss=0.1334 val_loss=0.3310 train_acc=0.9722 val_acc=0.8611 train_kappa=0.9444 val_kappa=0.7222\n",
            "[Epoch 86/100] loss=0.1630 val_loss=0.3309 train_acc=0.9375 val_acc=0.8611 train_kappa=0.8750 val_kappa=0.7222\n",
            "[Epoch 87/100] loss=0.1619 val_loss=0.3446 train_acc=0.9306 val_acc=0.8403 train_kappa=0.8611 val_kappa=0.6806\n",
            "[Epoch 88/100] loss=0.1379 val_loss=0.3291 train_acc=0.9653 val_acc=0.8681 train_kappa=0.9306 val_kappa=0.7361\n",
            "[Epoch 89/100] loss=0.1338 val_loss=0.3301 train_acc=0.9653 val_acc=0.8681 train_kappa=0.9306 val_kappa=0.7361\n",
            "[Epoch 90/100] loss=0.1137 val_loss=0.3308 train_acc=0.9722 val_acc=0.8681 train_kappa=0.9444 val_kappa=0.7361\n",
            "[Epoch 91/100] loss=0.1327 val_loss=0.3290 train_acc=0.9722 val_acc=0.8611 train_kappa=0.9444 val_kappa=0.7222\n",
            "[Epoch 92/100] loss=0.1387 val_loss=0.3352 train_acc=0.9653 val_acc=0.8611 train_kappa=0.9306 val_kappa=0.7222\n",
            "[Epoch 93/100] loss=0.1392 val_loss=0.3367 train_acc=0.9653 val_acc=0.8611 train_kappa=0.9306 val_kappa=0.7222\n",
            "[Epoch 94/100] loss=0.1059 val_loss=0.3394 train_acc=0.9722 val_acc=0.8611 train_kappa=0.9444 val_kappa=0.7222\n",
            "[Epoch 95/100] loss=0.1019 val_loss=0.3397 train_acc=0.9861 val_acc=0.8611 train_kappa=0.9722 val_kappa=0.7222\n",
            "[Epoch 96/100] loss=0.1447 val_loss=0.3600 train_acc=0.9306 val_acc=0.8472 train_kappa=0.8611 val_kappa=0.6944\n",
            "[Epoch 97/100] loss=0.1697 val_loss=0.3475 train_acc=0.9444 val_acc=0.8611 train_kappa=0.8889 val_kappa=0.7222\n",
            "[Epoch 98/100] loss=0.1272 val_loss=0.3751 train_acc=0.9514 val_acc=0.8542 train_kappa=0.9028 val_kappa=0.7083\n",
            "[Epoch 99/100] loss=0.1161 val_loss=0.4268 train_acc=0.9514 val_acc=0.8264 train_kappa=0.9028 val_kappa=0.6528\n",
            "[Epoch 100/100] loss=0.1148 val_loss=0.3986 train_acc=0.9722 val_acc=0.8611 train_kappa=0.9444 val_kappa=0.7222\n",
            "\n",
            "=== Processing Subject 9 ===\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\braindecode\\preprocessing\\preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "[Epoch 1/100] loss=0.7107 val_loss=0.7072 train_acc=0.4861 val_acc=0.5000 train_kappa=-0.0278 val_kappa=0.0000\n",
            "[Epoch 2/100] loss=0.7057 val_loss=0.6979 train_acc=0.5347 val_acc=0.5000 train_kappa=0.0694 val_kappa=0.0000\n",
            "[Epoch 3/100] loss=0.6887 val_loss=0.6914 train_acc=0.5278 val_acc=0.5000 train_kappa=0.0556 val_kappa=0.0000\n",
            "[Epoch 4/100] loss=0.7153 val_loss=0.6761 train_acc=0.4931 val_acc=0.8264 train_kappa=-0.0139 val_kappa=0.6528\n",
            "[Epoch 5/100] loss=0.7027 val_loss=0.6980 train_acc=0.5417 val_acc=0.5000 train_kappa=0.0833 val_kappa=0.0000\n",
            "[Epoch 6/100] loss=0.6776 val_loss=0.6706 train_acc=0.5625 val_acc=0.6944 train_kappa=0.1250 val_kappa=0.3889\n",
            "[Epoch 7/100] loss=0.6890 val_loss=0.6657 train_acc=0.5764 val_acc=0.5833 train_kappa=0.1528 val_kappa=0.1667\n",
            "[Epoch 8/100] loss=0.6860 val_loss=0.6574 train_acc=0.5278 val_acc=0.8125 train_kappa=0.0556 val_kappa=0.6250\n",
            "[Epoch 9/100] loss=0.6683 val_loss=0.6495 train_acc=0.5486 val_acc=0.5000 train_kappa=0.0972 val_kappa=0.0000\n",
            "[Epoch 10/100] loss=0.6968 val_loss=0.6519 train_acc=0.5625 val_acc=0.5000 train_kappa=0.1250 val_kappa=0.0000\n",
            "[Epoch 11/100] loss=0.6486 val_loss=0.6052 train_acc=0.6250 val_acc=0.8611 train_kappa=0.2500 val_kappa=0.7222\n",
            "[Epoch 12/100] loss=0.6302 val_loss=0.5812 train_acc=0.6944 val_acc=0.8889 train_kappa=0.3889 val_kappa=0.7778\n",
            "[Epoch 13/100] loss=0.6437 val_loss=0.5755 train_acc=0.6319 val_acc=0.8542 train_kappa=0.2639 val_kappa=0.7083\n",
            "[Epoch 14/100] loss=0.6012 val_loss=0.5608 train_acc=0.7292 val_acc=0.7292 train_kappa=0.4583 val_kappa=0.4583\n",
            "[Epoch 15/100] loss=0.6064 val_loss=0.5203 train_acc=0.7153 val_acc=0.8750 train_kappa=0.4306 val_kappa=0.7500\n",
            "[Epoch 16/100] loss=0.5954 val_loss=0.4928 train_acc=0.7569 val_acc=0.9097 train_kappa=0.5139 val_kappa=0.8194\n",
            "[Epoch 17/100] loss=0.5931 val_loss=0.4697 train_acc=0.7222 val_acc=0.8889 train_kappa=0.4444 val_kappa=0.7778\n",
            "[Epoch 18/100] loss=0.5542 val_loss=0.4567 train_acc=0.7222 val_acc=0.8819 train_kappa=0.4444 val_kappa=0.7639\n",
            "[Epoch 19/100] loss=0.4974 val_loss=0.4180 train_acc=0.7917 val_acc=0.9097 train_kappa=0.5833 val_kappa=0.8194\n",
            "[Epoch 20/100] loss=0.5039 val_loss=0.3745 train_acc=0.8056 val_acc=0.9167 train_kappa=0.6111 val_kappa=0.8333\n",
            "[Epoch 21/100] loss=0.4958 val_loss=0.3270 train_acc=0.7917 val_acc=0.9375 train_kappa=0.5833 val_kappa=0.8750\n",
            "[Epoch 22/100] loss=0.4690 val_loss=0.2930 train_acc=0.8056 val_acc=0.9306 train_kappa=0.6111 val_kappa=0.8611\n",
            "[Epoch 23/100] loss=0.4174 val_loss=0.2646 train_acc=0.8333 val_acc=0.9236 train_kappa=0.6667 val_kappa=0.8472\n",
            "[Epoch 24/100] loss=0.4031 val_loss=0.2353 train_acc=0.8611 val_acc=0.9306 train_kappa=0.7222 val_kappa=0.8611\n",
            "[Epoch 25/100] loss=0.3896 val_loss=0.2146 train_acc=0.8264 val_acc=0.9236 train_kappa=0.6528 val_kappa=0.8472\n",
            "[Epoch 26/100] loss=0.3659 val_loss=0.1998 train_acc=0.8472 val_acc=0.9167 train_kappa=0.6944 val_kappa=0.8333\n",
            "[Epoch 27/100] loss=0.3306 val_loss=0.1859 train_acc=0.8819 val_acc=0.9375 train_kappa=0.7639 val_kappa=0.8750\n",
            "[Epoch 28/100] loss=0.3081 val_loss=0.1807 train_acc=0.8958 val_acc=0.9444 train_kappa=0.7917 val_kappa=0.8889\n",
            "[Epoch 29/100] loss=0.2961 val_loss=0.2075 train_acc=0.9097 val_acc=0.9167 train_kappa=0.8194 val_kappa=0.8333\n",
            "[Epoch 30/100] loss=0.2962 val_loss=0.1693 train_acc=0.8681 val_acc=0.9375 train_kappa=0.7361 val_kappa=0.8750\n",
            "[Epoch 31/100] loss=0.2586 val_loss=0.1628 train_acc=0.8958 val_acc=0.9236 train_kappa=0.7917 val_kappa=0.8472\n",
            "[Epoch 32/100] loss=0.2316 val_loss=0.1583 train_acc=0.9097 val_acc=0.9236 train_kappa=0.8194 val_kappa=0.8472\n",
            "[Epoch 33/100] loss=0.2468 val_loss=0.1621 train_acc=0.8819 val_acc=0.9167 train_kappa=0.7639 val_kappa=0.8333\n",
            "[Epoch 34/100] loss=0.2097 val_loss=0.1702 train_acc=0.9444 val_acc=0.9167 train_kappa=0.8889 val_kappa=0.8333\n",
            "[Epoch 35/100] loss=0.2158 val_loss=0.1830 train_acc=0.9167 val_acc=0.9306 train_kappa=0.8333 val_kappa=0.8611\n",
            "[Epoch 36/100] loss=0.1778 val_loss=0.1493 train_acc=0.9306 val_acc=0.9236 train_kappa=0.8611 val_kappa=0.8472\n",
            "[Epoch 37/100] loss=0.1841 val_loss=0.1723 train_acc=0.9444 val_acc=0.9306 train_kappa=0.8889 val_kappa=0.8611\n",
            "[Epoch 38/100] loss=0.1650 val_loss=0.1602 train_acc=0.9514 val_acc=0.9306 train_kappa=0.9028 val_kappa=0.8611\n",
            "[Epoch 39/100] loss=0.2038 val_loss=0.1529 train_acc=0.9444 val_acc=0.9306 train_kappa=0.8889 val_kappa=0.8611\n",
            "[Epoch 40/100] loss=0.1592 val_loss=0.1694 train_acc=0.9444 val_acc=0.9236 train_kappa=0.8889 val_kappa=0.8472\n",
            "[Epoch 41/100] loss=0.1670 val_loss=0.1635 train_acc=0.9375 val_acc=0.9167 train_kappa=0.8750 val_kappa=0.8333\n",
            "[Epoch 42/100] loss=0.1642 val_loss=0.2048 train_acc=0.9514 val_acc=0.9236 train_kappa=0.9028 val_kappa=0.8472\n",
            "[Epoch 43/100] loss=0.1878 val_loss=0.1307 train_acc=0.9444 val_acc=0.9444 train_kappa=0.8889 val_kappa=0.8889\n",
            "[Epoch 44/100] loss=0.1444 val_loss=0.2302 train_acc=0.9514 val_acc=0.9236 train_kappa=0.9028 val_kappa=0.8472\n",
            "[Epoch 45/100] loss=0.1108 val_loss=0.1494 train_acc=0.9722 val_acc=0.9444 train_kappa=0.9444 val_kappa=0.8889\n",
            "[Epoch 46/100] loss=0.1395 val_loss=0.1791 train_acc=0.9444 val_acc=0.9375 train_kappa=0.8889 val_kappa=0.8750\n",
            "[Epoch 47/100] loss=0.1093 val_loss=0.1537 train_acc=0.9653 val_acc=0.9444 train_kappa=0.9306 val_kappa=0.8889\n",
            "[Epoch 48/100] loss=0.1084 val_loss=0.1319 train_acc=0.9583 val_acc=0.9514 train_kappa=0.9167 val_kappa=0.9028\n",
            "[Epoch 49/100] loss=0.1220 val_loss=0.1540 train_acc=0.9444 val_acc=0.9514 train_kappa=0.8889 val_kappa=0.9028\n",
            "[Epoch 50/100] loss=0.1091 val_loss=0.2062 train_acc=0.9583 val_acc=0.9306 train_kappa=0.9167 val_kappa=0.8611\n",
            "[Epoch 51/100] loss=0.0913 val_loss=0.1191 train_acc=0.9722 val_acc=0.9514 train_kappa=0.9444 val_kappa=0.9028\n",
            "[Epoch 52/100] loss=0.0990 val_loss=0.1419 train_acc=0.9653 val_acc=0.9514 train_kappa=0.9306 val_kappa=0.9028\n",
            "[Epoch 53/100] loss=0.0999 val_loss=0.2228 train_acc=0.9653 val_acc=0.9375 train_kappa=0.9306 val_kappa=0.8750\n",
            "[Epoch 54/100] loss=0.1264 val_loss=0.1550 train_acc=0.9444 val_acc=0.9444 train_kappa=0.8889 val_kappa=0.8889\n",
            "[Epoch 55/100] loss=0.0740 val_loss=0.1742 train_acc=0.9792 val_acc=0.9444 train_kappa=0.9583 val_kappa=0.8889\n",
            "[Epoch 56/100] loss=0.1020 val_loss=0.1785 train_acc=0.9653 val_acc=0.9444 train_kappa=0.9306 val_kappa=0.8889\n",
            "[Epoch 57/100] loss=0.0860 val_loss=0.1392 train_acc=0.9792 val_acc=0.9444 train_kappa=0.9583 val_kappa=0.8889\n",
            "[Epoch 58/100] loss=0.0780 val_loss=0.1650 train_acc=0.9653 val_acc=0.9444 train_kappa=0.9306 val_kappa=0.8889\n",
            "[Epoch 59/100] loss=0.1009 val_loss=0.2191 train_acc=0.9514 val_acc=0.9444 train_kappa=0.9028 val_kappa=0.8889\n",
            "[Epoch 60/100] loss=0.0966 val_loss=0.1359 train_acc=0.9722 val_acc=0.9444 train_kappa=0.9444 val_kappa=0.8889\n",
            "[Epoch 61/100] loss=0.0800 val_loss=0.2029 train_acc=0.9792 val_acc=0.9514 train_kappa=0.9583 val_kappa=0.9028\n",
            "[Epoch 62/100] loss=0.0575 val_loss=0.1591 train_acc=0.9861 val_acc=0.9444 train_kappa=0.9722 val_kappa=0.8889\n",
            "[Epoch 63/100] loss=0.0991 val_loss=0.1843 train_acc=0.9583 val_acc=0.9444 train_kappa=0.9167 val_kappa=0.8889\n",
            "[Epoch 64/100] loss=0.0727 val_loss=0.2096 train_acc=0.9722 val_acc=0.9444 train_kappa=0.9444 val_kappa=0.8889\n",
            "[Epoch 65/100] loss=0.0681 val_loss=0.1576 train_acc=0.9583 val_acc=0.9514 train_kappa=0.9167 val_kappa=0.9028\n",
            "[Epoch 66/100] loss=0.0486 val_loss=0.1576 train_acc=0.9792 val_acc=0.9514 train_kappa=0.9583 val_kappa=0.9028\n",
            "[Epoch 67/100] loss=0.0512 val_loss=0.2070 train_acc=0.9792 val_acc=0.9444 train_kappa=0.9583 val_kappa=0.8889\n",
            "[Epoch 68/100] loss=0.0559 val_loss=0.1529 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 69/100] loss=0.0519 val_loss=0.1529 train_acc=0.9931 val_acc=0.9583 train_kappa=0.9861 val_kappa=0.9167\n",
            "[Epoch 70/100] loss=0.0541 val_loss=0.2073 train_acc=0.9792 val_acc=0.9444 train_kappa=0.9583 val_kappa=0.8889\n",
            "[Epoch 71/100] loss=0.0538 val_loss=0.1462 train_acc=0.9861 val_acc=0.9583 train_kappa=0.9722 val_kappa=0.9167\n",
            "[Epoch 72/100] loss=0.0459 val_loss=0.1704 train_acc=0.9861 val_acc=0.9583 train_kappa=0.9722 val_kappa=0.9167\n",
            "[Epoch 73/100] loss=0.0405 val_loss=0.2142 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "[Epoch 74/100] loss=0.0470 val_loss=0.1216 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 75/100] loss=0.0592 val_loss=0.1985 train_acc=0.9792 val_acc=0.9444 train_kappa=0.9583 val_kappa=0.8889\n",
            "[Epoch 76/100] loss=0.0378 val_loss=0.1815 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "[Epoch 77/100] loss=0.0481 val_loss=0.1651 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 78/100] loss=0.0475 val_loss=0.2518 train_acc=0.9792 val_acc=0.9375 train_kappa=0.9583 val_kappa=0.8750\n",
            "[Epoch 79/100] loss=0.0275 val_loss=0.1352 train_acc=1.0000 val_acc=0.9514 train_kappa=1.0000 val_kappa=0.9028\n",
            "[Epoch 80/100] loss=0.0415 val_loss=0.2465 train_acc=0.9931 val_acc=0.9375 train_kappa=0.9861 val_kappa=0.8750\n",
            "[Epoch 81/100] loss=0.0250 val_loss=0.1930 train_acc=1.0000 val_acc=0.9444 train_kappa=1.0000 val_kappa=0.8889\n",
            "[Epoch 82/100] loss=0.0424 val_loss=0.1949 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "[Epoch 83/100] loss=0.0350 val_loss=0.2527 train_acc=1.0000 val_acc=0.9444 train_kappa=1.0000 val_kappa=0.8889\n",
            "[Epoch 84/100] loss=0.0401 val_loss=0.1765 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 85/100] loss=0.0420 val_loss=0.2151 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 86/100] loss=0.0303 val_loss=0.2395 train_acc=1.0000 val_acc=0.9444 train_kappa=1.0000 val_kappa=0.8889\n",
            "[Epoch 87/100] loss=0.0270 val_loss=0.1947 train_acc=1.0000 val_acc=0.9514 train_kappa=1.0000 val_kappa=0.9028\n",
            "[Epoch 88/100] loss=0.0338 val_loss=0.1893 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 89/100] loss=0.0277 val_loss=0.2179 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 90/100] loss=0.0315 val_loss=0.2407 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 91/100] loss=0.0238 val_loss=0.2707 train_acc=1.0000 val_acc=0.9375 train_kappa=1.0000 val_kappa=0.8750\n",
            "[Epoch 92/100] loss=0.0260 val_loss=0.2342 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 93/100] loss=0.0200 val_loss=0.2229 train_acc=1.0000 val_acc=0.9514 train_kappa=1.0000 val_kappa=0.9028\n",
            "[Epoch 94/100] loss=0.0286 val_loss=0.2421 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "[Epoch 95/100] loss=0.0268 val_loss=0.1269 train_acc=1.0000 val_acc=0.9514 train_kappa=1.0000 val_kappa=0.9028\n",
            "[Epoch 96/100] loss=0.0293 val_loss=0.4054 train_acc=1.0000 val_acc=0.9097 train_kappa=1.0000 val_kappa=0.8194\n",
            "[Epoch 97/100] loss=0.0444 val_loss=0.1664 train_acc=0.9861 val_acc=0.9514 train_kappa=0.9722 val_kappa=0.9028\n",
            "[Epoch 98/100] loss=0.0349 val_loss=0.2051 train_acc=0.9931 val_acc=0.9514 train_kappa=0.9861 val_kappa=0.9028\n",
            "[Epoch 99/100] loss=0.0254 val_loss=0.3214 train_acc=0.9931 val_acc=0.9375 train_kappa=0.9861 val_kappa=0.8750\n",
            "[Epoch 100/100] loss=0.0247 val_loss=0.2276 train_acc=0.9931 val_acc=0.9444 train_kappa=0.9861 val_kappa=0.8889\n",
            "\n",
            "=== Computing Combined Metrics Across All Subjects ===\n",
            "\n",
            "\n",
            "Per-subject metrics: [{'accuracy': 0.9722222222222222, 'precision': 0.9857142857142858, 'recall': 0.9583333333333334, 'f1_score': 0.971830985915493, 'kappa': 0.9444444444444444, 'specificity': 0.9861111111111112, 'tn': 71, 'fp': 1, 'fn': 3, 'tp': 69, 'roc_auc': 0.9909336419753086, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.05555555555555555, 0.05555555555555555, 0.09722222222222222, 0.09722222222222222, 0.2777777777777778, 0.2777777777777778, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.7361111111111112, 0.7361111111111112, 0.9583333333333334, 0.9583333333333334, 0.9722222222222222, 0.9722222222222222, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.7708333333333334, 'precision': 0.7294117647058823, 'recall': 0.8611111111111112, 'f1_score': 0.7898089171974522, 'kappa': 0.5416666666666667, 'specificity': 0.6805555555555556, 'tn': 49, 'fp': 23, 'fn': 10, 'tp': 62, 'roc_auc': 0.8150077160493827, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.08333333333333333, 0.08333333333333333, 0.1111111111111111, 0.1111111111111111, 0.1388888888888889, 0.1388888888888889, 0.1527777777777778, 0.1527777777777778, 0.16666666666666666, 0.16666666666666666, 0.18055555555555555, 0.18055555555555555, 0.19444444444444445, 0.19444444444444445, 0.20833333333333334, 0.20833333333333334, 0.2222222222222222, 0.2222222222222222, 0.2361111111111111, 0.2361111111111111, 0.2916666666666667, 0.2916666666666667, 0.3194444444444444, 0.3194444444444444, 0.4166666666666667, 0.4166666666666667, 0.4305555555555556, 0.4305555555555556, 0.4861111111111111, 0.4861111111111111, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.6527777777777778, 0.6527777777777778, 0.9444444444444444, 0.9444444444444444, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.1527777777777778, 0.1527777777777778, 0.2361111111111111, 0.2361111111111111, 0.3194444444444444, 0.3194444444444444, 0.3611111111111111, 0.3611111111111111, 0.375, 0.375, 0.4027777777777778, 0.4027777777777778, 0.4166666666666667, 0.4166666666666667, 0.4444444444444444, 0.4444444444444444, 0.4861111111111111, 0.4861111111111111, 0.5694444444444444, 0.5694444444444444, 0.5833333333333334, 0.5833333333333334, 0.625, 0.625, 0.6666666666666666, 0.6666666666666666, 0.7361111111111112, 0.7361111111111112, 0.7916666666666666, 0.7916666666666666, 0.8055555555555556, 0.8055555555555556, 0.8611111111111112, 0.8611111111111112, 0.875, 0.875, 0.8888888888888888, 0.8888888888888888, 0.9027777777777778, 0.9027777777777778, 0.9444444444444444, 0.9444444444444444, 0.9583333333333334, 0.9583333333333334, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.9375, 'precision': 0.92, 'recall': 0.9583333333333334, 'f1_score': 0.9387755102040817, 'kappa': 0.875, 'specificity': 0.9166666666666666, 'tn': 66, 'fp': 6, 'fn': 3, 'tp': 69, 'roc_auc': 0.9766589506172839, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.06944444444444445, 0.06944444444444445, 0.08333333333333333, 0.08333333333333333, 0.1388888888888889, 0.1388888888888889, 0.20833333333333334, 0.20833333333333334, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.3472222222222222, 0.3472222222222222, 0.7916666666666666, 0.7916666666666666, 0.8194444444444444, 0.8194444444444444, 0.8888888888888888, 0.8888888888888888, 0.9166666666666666, 0.9166666666666666, 0.9583333333333334, 0.9583333333333334, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.8680555555555556, 'precision': 0.8441558441558441, 'recall': 0.9027777777777778, 'f1_score': 0.87248322147651, 'kappa': 0.7361111111111112, 'specificity': 0.8333333333333334, 'tn': 60, 'fp': 12, 'fn': 7, 'tp': 65, 'roc_auc': 0.9149305555555556, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.06944444444444445, 0.06944444444444445, 0.08333333333333333, 0.08333333333333333, 0.09722222222222222, 0.09722222222222222, 0.1111111111111111, 0.1111111111111111, 0.125, 0.125, 0.1527777777777778, 0.1527777777777778, 0.16666666666666666, 0.16666666666666666, 0.19444444444444445, 0.19444444444444445, 0.20833333333333334, 0.20833333333333334, 0.25, 0.25, 0.4305555555555556, 0.4305555555555556, 0.4444444444444444, 0.4444444444444444, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.3055555555555556, 0.3055555555555556, 0.375, 0.375, 0.4305555555555556, 0.4305555555555556, 0.5277777777777778, 0.5277777777777778, 0.5694444444444444, 0.5694444444444444, 0.6666666666666666, 0.6666666666666666, 0.7361111111111112, 0.7361111111111112, 0.75, 0.75, 0.8055555555555556, 0.8055555555555556, 0.8888888888888888, 0.8888888888888888, 0.9027777777777778, 0.9027777777777778, 0.9166666666666666, 0.9166666666666666, 0.9444444444444444, 0.9444444444444444, 0.9583333333333334, 0.9583333333333334, 0.9722222222222222, 0.9722222222222222, 1.0, 1.0]}, {'accuracy': 0.6388888888888888, 'precision': 0.6219512195121951, 'recall': 0.7083333333333334, 'f1_score': 0.6623376623376623, 'kappa': 0.2777777777777778, 'specificity': 0.5694444444444444, 'tn': 41, 'fp': 31, 'fn': 21, 'tp': 51, 'roc_auc': 0.6695601851851851, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.08333333333333333, 0.08333333333333333, 0.09722222222222222, 0.09722222222222222, 0.1111111111111111, 0.1111111111111111, 0.1388888888888889, 0.1388888888888889, 0.1527777777777778, 0.1527777777777778, 0.16666666666666666, 0.16666666666666666, 0.20833333333333334, 0.20833333333333334, 0.2361111111111111, 0.2361111111111111, 0.2777777777777778, 0.2777777777777778, 0.3194444444444444, 0.3194444444444444, 0.3333333333333333, 0.3333333333333333, 0.3472222222222222, 0.3472222222222222, 0.3611111111111111, 0.3611111111111111, 0.375, 0.375, 0.3888888888888889, 0.3888888888888889, 0.4027777777777778, 0.4027777777777778, 0.4166666666666667, 0.4166666666666667, 0.4583333333333333, 0.4583333333333333, 0.5, 0.5, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.6527777777777778, 0.6527777777777778, 0.6944444444444444, 0.6944444444444444, 0.7083333333333334, 0.7083333333333334, 0.75, 0.75, 0.7916666666666666, 0.7916666666666666, 0.8888888888888888, 0.8888888888888888, 0.9166666666666666, 0.9166666666666666, 0.9305555555555556, 0.9305555555555556, 0.9722222222222222, 0.9722222222222222, 1.0, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.05555555555555555, 0.05555555555555555, 0.125, 0.125, 0.16666666666666666, 0.16666666666666666, 0.18055555555555555, 0.18055555555555555, 0.2638888888888889, 0.2638888888888889, 0.3333333333333333, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.4027777777777778, 0.4027777777777778, 0.4166666666666667, 0.4166666666666667, 0.4583333333333333, 0.4583333333333333, 0.4722222222222222, 0.4722222222222222, 0.5138888888888888, 0.5138888888888888, 0.5416666666666666, 0.5416666666666666, 0.5694444444444444, 0.5694444444444444, 0.5833333333333334, 0.5833333333333334, 0.5972222222222222, 0.5972222222222222, 0.6111111111111112, 0.6111111111111112, 0.625, 0.625, 0.6666666666666666, 0.6666666666666666, 0.6805555555555556, 0.6805555555555556, 0.7083333333333334, 0.7083333333333334, 0.75, 0.75, 0.7638888888888888, 0.7638888888888888, 0.7777777777777778, 0.7777777777777778, 0.7916666666666666, 0.7916666666666666, 0.8055555555555556, 0.8055555555555556, 0.8194444444444444, 0.8194444444444444, 0.8611111111111112, 0.8611111111111112, 0.875, 0.875, 0.9027777777777778, 0.9027777777777778, 0.9166666666666666, 0.9166666666666666, 0.9444444444444444, 0.9444444444444444, 0.9583333333333334, 0.9583333333333334, 0.9861111111111112, 0.9861111111111112, 1.0]}, {'accuracy': 0.7361111111111112, 'precision': 0.75, 'recall': 0.7083333333333334, 'f1_score': 0.7285714285714285, 'kappa': 0.4722222222222222, 'specificity': 0.7638888888888888, 'tn': 55, 'fp': 17, 'fn': 21, 'tp': 51, 'roc_auc': 0.7569444444444445, 'fpr': [0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.06944444444444445, 0.06944444444444445, 0.125, 0.125, 0.1388888888888889, 0.1388888888888889, 0.1527777777777778, 0.1527777777777778, 0.18055555555555555, 0.18055555555555555, 0.20833333333333334, 0.20833333333333334, 0.2222222222222222, 0.2222222222222222, 0.2361111111111111, 0.2361111111111111, 0.2638888888888889, 0.2638888888888889, 0.3333333333333333, 0.3333333333333333, 0.3888888888888889, 0.3888888888888889, 0.4166666666666667, 0.4166666666666667, 0.4722222222222222, 0.4722222222222222, 0.5277777777777778, 0.5277777777777778, 0.5833333333333334, 0.5833333333333334, 0.5972222222222222, 0.5972222222222222, 0.6111111111111112, 0.6111111111111112, 0.6388888888888888, 0.6388888888888888, 0.6944444444444444, 0.6944444444444444, 0.75, 0.75, 0.7638888888888888, 0.7638888888888888, 0.7777777777777778, 0.7777777777777778, 0.875, 0.875, 1.0], 'tpr': [0.0, 0.0, 0.125, 0.125, 0.1388888888888889, 0.1388888888888889, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3472222222222222, 0.3472222222222222, 0.4444444444444444, 0.4444444444444444, 0.4861111111111111, 0.4861111111111111, 0.5138888888888888, 0.5138888888888888, 0.5277777777777778, 0.5277777777777778, 0.5694444444444444, 0.5694444444444444, 0.5972222222222222, 0.5972222222222222, 0.7083333333333334, 0.7083333333333334, 0.7222222222222222, 0.7222222222222222, 0.7638888888888888, 0.7638888888888888, 0.7916666666666666, 0.7916666666666666, 0.8055555555555556, 0.8055555555555556, 0.8194444444444444, 0.8194444444444444, 0.8333333333333334, 0.8333333333333334, 0.8472222222222222, 0.8472222222222222, 0.8611111111111112, 0.8611111111111112, 0.8888888888888888, 0.8888888888888888, 0.9027777777777778, 0.9027777777777778, 0.9166666666666666, 0.9166666666666666, 0.9444444444444444, 0.9444444444444444, 0.9722222222222222, 0.9722222222222222, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.8055555555555556, 'precision': 0.782051282051282, 'recall': 0.8472222222222222, 'f1_score': 0.8133333333333332, 'kappa': 0.6111111111111112, 'specificity': 0.7638888888888888, 'tn': 55, 'fp': 17, 'fn': 11, 'tp': 61, 'roc_auc': 0.8447145061728396, 'fpr': [0.0, 0.0, 0.0, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.06944444444444445, 0.06944444444444445, 0.08333333333333333, 0.08333333333333333, 0.09722222222222222, 0.09722222222222222, 0.1111111111111111, 0.1111111111111111, 0.125, 0.125, 0.1388888888888889, 0.1388888888888889, 0.18055555555555555, 0.18055555555555555, 0.19444444444444445, 0.19444444444444445, 0.20833333333333334, 0.20833333333333334, 0.2222222222222222, 0.2222222222222222, 0.3194444444444444, 0.3194444444444444, 0.3611111111111111, 0.3611111111111111, 0.375, 0.375, 0.4305555555555556, 0.4305555555555556, 0.4722222222222222, 0.4722222222222222, 0.4861111111111111, 0.4861111111111111, 0.5694444444444444, 0.5694444444444444, 0.5833333333333334, 0.5833333333333334, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.20833333333333334, 0.20833333333333334, 0.2638888888888889, 0.2638888888888889, 0.3055555555555556, 0.3055555555555556, 0.3194444444444444, 0.3194444444444444, 0.3333333333333333, 0.3333333333333333, 0.375, 0.375, 0.4444444444444444, 0.4444444444444444, 0.4722222222222222, 0.4722222222222222, 0.5416666666666666, 0.5416666666666666, 0.7222222222222222, 0.7222222222222222, 0.75, 0.75, 0.7638888888888888, 0.7638888888888888, 0.8055555555555556, 0.8055555555555556, 0.8472222222222222, 0.8472222222222222, 0.8611111111111112, 0.8611111111111112, 0.875, 0.875, 0.9027777777777778, 0.9027777777777778, 0.9166666666666666, 0.9166666666666666, 0.9305555555555556, 0.9305555555555556, 0.9444444444444444, 0.9444444444444444, 0.9583333333333334, 0.9583333333333334, 0.9722222222222222, 0.9722222222222222, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.8680555555555556, 'precision': 0.8840579710144928, 'recall': 0.8472222222222222, 'f1_score': 0.8652482269503546, 'kappa': 0.7361111111111112, 'specificity': 0.8888888888888888, 'tn': 64, 'fp': 8, 'fn': 11, 'tp': 61, 'roc_auc': 0.9328703703703703, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.041666666666666664, 0.041666666666666664, 0.05555555555555555, 0.05555555555555555, 0.06944444444444445, 0.06944444444444445, 0.09722222222222222, 0.09722222222222222, 0.125, 0.125, 0.19444444444444445, 0.19444444444444445, 0.2222222222222222, 0.2222222222222222, 0.2361111111111111, 0.2361111111111111, 0.2777777777777778, 0.2777777777777778, 0.3055555555555556, 0.3055555555555556, 0.375, 0.375, 0.4027777777777778, 0.4027777777777778, 0.4166666666666667, 0.4166666666666667, 0.4444444444444444, 0.4444444444444444, 0.7083333333333334, 0.7083333333333334, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.5555555555555556, 0.5555555555555556, 0.5833333333333334, 0.5833333333333334, 0.6111111111111112, 0.6111111111111112, 0.6944444444444444, 0.6944444444444444, 0.75, 0.75, 0.8055555555555556, 0.8055555555555556, 0.8472222222222222, 0.8472222222222222, 0.8611111111111112, 0.8611111111111112, 0.875, 0.875, 0.8888888888888888, 0.8888888888888888, 0.9027777777777778, 0.9027777777777778, 0.9166666666666666, 0.9166666666666666, 0.9305555555555556, 0.9305555555555556, 0.9444444444444444, 0.9444444444444444, 0.9583333333333334, 0.9583333333333334, 0.9722222222222222, 0.9722222222222222, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}, {'accuracy': 0.9583333333333334, 'precision': 0.9852941176470589, 'recall': 0.9305555555555556, 'f1_score': 0.9571428571428572, 'kappa': 0.9166666666666666, 'specificity': 0.9861111111111112, 'tn': 71, 'fp': 1, 'fn': 5, 'tp': 67, 'roc_auc': 0.9947916666666667, 'fpr': [0.0, 0.0, 0.0, 0.013888888888888888, 0.013888888888888888, 0.027777777777777776, 0.027777777777777776, 0.05555555555555555, 0.05555555555555555, 0.09722222222222222, 0.09722222222222222, 0.1388888888888889, 0.1388888888888889, 1.0], 'tpr': [0.0, 0.013888888888888888, 0.9027777777777778, 0.9027777777777778, 0.9305555555555556, 0.9305555555555556, 0.9583333333333334, 0.9583333333333334, 0.9722222222222222, 0.9722222222222222, 0.9861111111111112, 0.9861111111111112, 1.0, 1.0]}]\n",
            "\n",
            "Combined metrics: {'accuracy': 0.8395061728395061, 'precision': 0.8273809523809523, 'recall': 0.8580246913580247, 'f1_score': 0.8424242424242424, 'kappa': 0.6790123456790124, 'specificity': 0.8209876543209876, 'tn': 532, 'fp': 116, 'fn': 92, 'tp': 556, 'roc_auc': 0.8971526825179088, 'fpr': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0015432098765432098, 0.0015432098765432098, 0.0030864197530864196, 0.0030864197530864196, 0.004629629629629629, 0.004629629629629629, 0.006172839506172839, 0.006172839506172839, 0.009259259259259259, 0.009259259259259259, 0.010802469135802469, 0.010802469135802469, 0.012345679012345678, 0.012345679012345678, 0.013888888888888888, 0.013888888888888888, 0.015432098765432098, 0.015432098765432098, 0.016975308641975308, 0.016975308641975308, 0.018518518518518517, 0.018518518518518517, 0.020061728395061727, 0.020061728395061727, 0.021604938271604937, 0.021604938271604937, 0.023148148148148147, 0.023148148148148147, 0.026234567901234566, 0.026234567901234566, 0.027777777777777776, 0.027777777777777776, 0.029320987654320986, 0.029320987654320986, 0.032407407407407406, 0.032407407407407406, 0.033950617283950615, 0.033950617283950615, 0.035493827160493825, 0.035493827160493825, 0.037037037037037035, 0.037037037037037035, 0.038580246913580245, 0.038580246913580245, 0.040123456790123455, 0.040123456790123455, 0.041666666666666664, 0.041666666666666664, 0.044753086419753084, 0.044753086419753084, 0.046296296296296294, 0.046296296296296294, 0.047839506172839504, 0.047839506172839504, 0.04938271604938271, 0.04938271604938271, 0.05092592592592592, 0.05092592592592592, 0.05246913580246913, 0.05246913580246913, 0.05401234567901234, 0.05401234567901234, 0.05555555555555555, 0.05555555555555555, 0.05709876543209876, 0.05709876543209876, 0.06018518518518518, 0.06018518518518518, 0.06327160493827161, 0.06327160493827161, 0.06790123456790123, 0.06790123456790123, 0.06944444444444445, 0.06944444444444445, 0.07098765432098765, 0.07098765432098765, 0.07253086419753087, 0.07253086419753087, 0.07561728395061729, 0.07561728395061729, 0.07716049382716049, 0.07716049382716049, 0.08024691358024691, 0.08024691358024691, 0.08179012345679013, 0.08179012345679013, 0.08333333333333333, 0.08333333333333333, 0.08796296296296297, 0.08796296296296297, 0.08950617283950617, 0.08950617283950617, 0.09104938271604938, 0.09104938271604938, 0.09567901234567901, 0.09567901234567901, 0.09722222222222222, 0.09722222222222222, 0.09876543209876543, 0.09876543209876543, 0.10030864197530864, 0.10030864197530864, 0.10185185185185185, 0.10185185185185185, 0.10339506172839506, 0.10339506172839506, 0.10493827160493827, 0.10493827160493827, 0.10802469135802469, 0.10802469135802469, 0.1095679012345679, 0.1095679012345679, 0.1111111111111111, 0.1111111111111111, 0.11265432098765432, 0.11265432098765432, 0.11419753086419752, 0.11419753086419752, 0.11574074074074074, 0.11574074074074074, 0.11728395061728394, 0.11728395061728394, 0.11882716049382716, 0.11882716049382716, 0.12037037037037036, 0.12037037037037036, 0.12191358024691358, 0.12191358024691358, 0.12345679012345678, 0.12345679012345678, 0.125, 0.125, 0.12654320987654322, 0.12654320987654322, 0.12962962962962962, 0.12962962962962962, 0.13271604938271606, 0.13271604938271606, 0.13425925925925927, 0.13425925925925927, 0.13580246913580246, 0.13580246913580246, 0.13734567901234568, 0.13734567901234568, 0.1419753086419753, 0.1419753086419753, 0.14506172839506173, 0.14506172839506173, 0.14660493827160495, 0.14660493827160495, 0.14814814814814814, 0.14814814814814814, 0.14969135802469136, 0.14969135802469136, 0.15123456790123457, 0.15123456790123457, 0.1558641975308642, 0.1558641975308642, 0.16049382716049382, 0.16049382716049382, 0.16203703703703703, 0.16203703703703703, 0.16358024691358025, 0.16358024691358025, 0.16512345679012347, 0.16512345679012347, 0.16820987654320987, 0.16820987654320987, 0.1697530864197531, 0.1697530864197531, 0.1712962962962963, 0.1712962962962963, 0.1743827160493827, 0.1743827160493827, 0.17592592592592593, 0.17592592592592593, 0.17746913580246915, 0.17746913580246915, 0.18518518518518517, 0.18518518518518517, 0.1882716049382716, 0.1882716049382716, 0.18981481481481483, 0.18981481481481483, 0.19907407407407407, 0.19907407407407407, 0.2052469135802469, 0.2052469135802469, 0.21141975308641975, 0.21141975308641975, 0.2191358024691358, 0.2191358024691358, 0.22839506172839505, 0.22839506172839505, 0.23148148148148148, 0.23148148148148148, 0.2345679012345679, 0.2345679012345679, 0.23919753086419754, 0.23919753086419754, 0.24845679012345678, 0.24845679012345678, 0.2515432098765432, 0.2515432098765432, 0.26080246913580246, 0.26080246913580246, 0.2638888888888889, 0.2638888888888889, 0.27314814814814814, 0.27314814814814814, 0.2762345679012346, 0.2762345679012346, 0.2978395061728395, 0.2978395061728395, 0.30401234567901236, 0.30401234567901236, 0.3055555555555556, 0.3055555555555556, 0.3148148148148148, 0.3148148148148148, 0.3271604938271605, 0.3271604938271605, 0.3287037037037037, 0.3287037037037037, 0.3317901234567901, 0.3317901234567901, 0.33641975308641975, 0.33641975308641975, 0.33796296296296297, 0.33796296296296297, 0.345679012345679, 0.345679012345679, 0.3472222222222222, 0.3472222222222222, 0.3626543209876543, 0.3626543209876543, 0.36419753086419754, 0.36419753086419754, 0.37037037037037035, 0.37037037037037035, 0.38271604938271603, 0.38271604938271603, 0.38425925925925924, 0.38425925925925924, 0.3873456790123457, 0.3873456790123457, 0.39969135802469136, 0.39969135802469136, 0.4012345679012346, 0.4012345679012346, 0.41512345679012347, 0.41512345679012347, 0.4182098765432099, 0.4182098765432099, 0.4212962962962963, 0.4212962962962963, 0.4228395061728395, 0.4228395061728395, 0.42746913580246915, 0.42746913580246915, 0.42901234567901236, 0.42901234567901236, 0.43364197530864196, 0.43364197530864196, 0.4351851851851852, 0.4351851851851852, 0.4675925925925926, 0.4675925925925926, 0.4722222222222222, 0.4722222222222222, 0.4737654320987654, 0.4737654320987654, 0.48148148148148145, 0.48148148148148145, 0.4845679012345679, 0.4845679012345679, 0.4861111111111111, 0.4861111111111111, 0.4876543209876543, 0.4876543209876543, 0.49228395061728397, 0.49228395061728397, 0.49537037037037035, 0.49537037037037035, 0.49691358024691357, 0.49691358024691357, 0.5015432098765432, 0.5015432098765432, 0.5123456790123457, 0.5123456790123457, 0.5416666666666666, 0.5416666666666666, 0.5524691358024691, 0.5524691358024691, 0.5540123456790124, 0.5540123456790124, 0.5601851851851852, 0.5601851851851852, 0.5632716049382716, 0.5632716049382716, 0.6033950617283951, 0.6033950617283951, 0.6111111111111112, 0.6111111111111112, 0.6342592592592593, 0.6342592592592593, 0.6790123456790124, 0.6790123456790124, 0.7037037037037037, 0.7037037037037037, 0.75, 0.75, 0.7962962962962963, 0.7962962962962963, 0.7978395061728395, 0.7978395061728395, 1.0], 'tpr': [0.0, 0.0015432098765432098, 0.029320987654320986, 0.032407407407407406, 0.08796296296296297, 0.08796296296296297, 0.13580246913580246, 0.13580246913580246, 0.1743827160493827, 0.1743827160493827, 0.19598765432098766, 0.19598765432098766, 0.2006172839506173, 0.2006172839506173, 0.2052469135802469, 0.2052469135802469, 0.20987654320987653, 0.20987654320987653, 0.21604938271604937, 0.21604938271604937, 0.22685185185185186, 0.22685185185185186, 0.2515432098765432, 0.2515432098765432, 0.2654320987654321, 0.2654320987654321, 0.29475308641975306, 0.29475308641975306, 0.2962962962962963, 0.2962962962962963, 0.30401234567901236, 0.30401234567901236, 0.3194444444444444, 0.3194444444444444, 0.3317901234567901, 0.3317901234567901, 0.33796296296296297, 0.33796296296296297, 0.35030864197530864, 0.35030864197530864, 0.35648148148148145, 0.35648148148148145, 0.3611111111111111, 0.3611111111111111, 0.38425925925925924, 0.38425925925925924, 0.41358024691358025, 0.41358024691358025, 0.4228395061728395, 0.4228395061728395, 0.43209876543209874, 0.43209876543209874, 0.44290123456790126, 0.44290123456790126, 0.4537037037037037, 0.4537037037037037, 0.46141975308641975, 0.46141975308641975, 0.4783950617283951, 0.4783950617283951, 0.48302469135802467, 0.48302469135802467, 0.48919753086419754, 0.48919753086419754, 0.49691358024691357, 0.49691358024691357, 0.5324074074074074, 0.5324074074074074, 0.5354938271604939, 0.5354938271604939, 0.5493827160493827, 0.5493827160493827, 0.5540123456790124, 0.5540123456790124, 0.5555555555555556, 0.5555555555555556, 0.5802469135802469, 0.5802469135802469, 0.5925925925925926, 0.5925925925925926, 0.6018518518518519, 0.6018518518518519, 0.6049382716049383, 0.6049382716049383, 0.6111111111111112, 0.6111111111111112, 0.6141975308641975, 0.6141975308641975, 0.6234567901234568, 0.6234567901234568, 0.625, 0.625, 0.6296296296296297, 0.6296296296296297, 0.6496913580246914, 0.6496913580246914, 0.6527777777777778, 0.6527777777777778, 0.6589506172839507, 0.6589506172839507, 0.6651234567901234, 0.6651234567901234, 0.6666666666666666, 0.6666666666666666, 0.6759259259259259, 0.6759259259259259, 0.6820987654320988, 0.6820987654320988, 0.6851851851851852, 0.6851851851851852, 0.6944444444444444, 0.6944444444444444, 0.6975308641975309, 0.6975308641975309, 0.7021604938271605, 0.7021604938271605, 0.7037037037037037, 0.7037037037037037, 0.7098765432098766, 0.7098765432098766, 0.7175925925925926, 0.7175925925925926, 0.7268518518518519, 0.7268518518518519, 0.7283950617283951, 0.7283950617283951, 0.7314814814814815, 0.7314814814814815, 0.7391975308641975, 0.7391975308641975, 0.7407407407407407, 0.7407407407407407, 0.7422839506172839, 0.7422839506172839, 0.7453703703703703, 0.7453703703703703, 0.7577160493827161, 0.7577160493827161, 0.7592592592592593, 0.7592592592592593, 0.7654320987654321, 0.7654320987654321, 0.7685185185185185, 0.7685185185185185, 0.7700617283950617, 0.7700617283950617, 0.7839506172839507, 0.7839506172839507, 0.7901234567901234, 0.7901234567901234, 0.7916666666666666, 0.7916666666666666, 0.7947530864197531, 0.7947530864197531, 0.7962962962962963, 0.7962962962962963, 0.8009259259259259, 0.8009259259259259, 0.8024691358024691, 0.8024691358024691, 0.8117283950617284, 0.8117283950617284, 0.8179012345679012, 0.8179012345679012, 0.8225308641975309, 0.8225308641975309, 0.8256172839506173, 0.8256172839506173, 0.8302469135802469, 0.8302469135802469, 0.8364197530864198, 0.8364197530864198, 0.8395061728395061, 0.8395061728395061, 0.8410493827160493, 0.8410493827160493, 0.8503086419753086, 0.8503086419753086, 0.8518518518518519, 0.8518518518518519, 0.8564814814814815, 0.8564814814814815, 0.8580246913580247, 0.8580246913580247, 0.8595679012345679, 0.8595679012345679, 0.8611111111111112, 0.8611111111111112, 0.8626543209876543, 0.8626543209876543, 0.8641975308641975, 0.8641975308641975, 0.8672839506172839, 0.8672839506172839, 0.8688271604938271, 0.8688271604938271, 0.8703703703703703, 0.8703703703703703, 0.8719135802469136, 0.8719135802469136, 0.8734567901234568, 0.8734567901234568, 0.875, 0.875, 0.8765432098765432, 0.8765432098765432, 0.8780864197530864, 0.8780864197530864, 0.8811728395061729, 0.8811728395061729, 0.8827160493827161, 0.8827160493827161, 0.8873456790123457, 0.8873456790123457, 0.8904320987654321, 0.8904320987654321, 0.8919753086419753, 0.8919753086419753, 0.8966049382716049, 0.8966049382716049, 0.8996913580246914, 0.8996913580246914, 0.9012345679012346, 0.9012345679012346, 0.9058641975308642, 0.9058641975308642, 0.9074074074074074, 0.9074074074074074, 0.9089506172839507, 0.9089506172839507, 0.9104938271604939, 0.9104938271604939, 0.9120370370370371, 0.9120370370370371, 0.9135802469135802, 0.9135802469135802, 0.9166666666666666, 0.9166666666666666, 0.9197530864197531, 0.9197530864197531, 0.9212962962962963, 0.9212962962962963, 0.9243827160493827, 0.9243827160493827, 0.9259259259259259, 0.9259259259259259, 0.9290123456790124, 0.9290123456790124, 0.9320987654320988, 0.9320987654320988, 0.933641975308642, 0.933641975308642, 0.9382716049382716, 0.9382716049382716, 0.9398148148148148, 0.9398148148148148, 0.941358024691358, 0.941358024691358, 0.9429012345679012, 0.9429012345679012, 0.9444444444444444, 0.9444444444444444, 0.9475308641975309, 0.9475308641975309, 0.9490740740740741, 0.9490740740740741, 0.9506172839506173, 0.9506172839506173, 0.9521604938271605, 0.9521604938271605, 0.9537037037037037, 0.9537037037037037, 0.9567901234567902, 0.9567901234567902, 0.9583333333333334, 0.9583333333333334, 0.9598765432098766, 0.9598765432098766, 0.9629629629629629, 0.9629629629629629, 0.9645061728395061, 0.9645061728395061, 0.9660493827160493, 0.9660493827160493, 0.970679012345679, 0.970679012345679, 0.9722222222222222, 0.9722222222222222, 0.9737654320987654, 0.9737654320987654, 0.9753086419753086, 0.9753086419753086, 0.9768518518518519, 0.9768518518518519, 0.9799382716049383, 0.9799382716049383, 0.9814814814814815, 0.9814814814814815, 0.9830246913580247, 0.9830246913580247, 0.9845679012345679, 0.9845679012345679, 0.9861111111111112, 0.9861111111111112, 0.9876543209876543, 0.9876543209876543, 0.9891975308641975, 0.9891975308641975, 0.9907407407407407, 0.9907407407407407, 0.9922839506172839, 0.9922839506172839, 0.9938271604938271, 0.9938271604938271, 0.9953703703703703, 0.9953703703703703, 0.9969135802469136, 0.9969135802469136, 0.9984567901234568, 0.9984567901234568, 1.0, 1.0]}\n"
          ]
        }
      ],
      "source": [
        "# Full modified script with enhanced plotting & documentation\n",
        "# Cell 1: Imports and Setup\n",
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    cohen_kappa_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from braindecode.datasets import MOABBDataset\n",
        "from braindecode.preprocessing import (\n",
        "    Preprocessor,\n",
        "    preprocess,\n",
        "    exponential_moving_standardize,\n",
        "    create_windows_from_events,\n",
        "    SetEEGReference,\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"Helvetica\",\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 16,\n",
        "    \"axes.labelsize\": 14,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"figure.dpi\": 600,\n",
        "    \"savefig.dpi\": 600,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "    \"savefig.format\": \"png\",\n",
        "})\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# -------------------\n",
        "# Cell 2: Model Definition (SpiTranNet)  -- unchanged\n",
        "class SpikingNeuronCell(nn.Module):\n",
        "    def __init__(self, threshold=0.3, decay=0.9, temp=1.2):\n",
        "        super().__init__()\n",
        "        self.threshold = threshold\n",
        "        self.decay = decay\n",
        "        self.temp = temp\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not x.requires_grad:\n",
        "            x = x.clone().detach().requires_grad_()\n",
        "        mem_pot = x * self.decay\n",
        "        sigmoid = torch.sigmoid((mem_pot - self.threshold) * self.temp)\n",
        "        spike = torch.where(mem_pot > self.threshold,\n",
        "                            torch.ones_like(mem_pot),\n",
        "                            torch.zeros_like(mem_pot))\n",
        "        spike = spike.clone().detach().requires_grad_()\n",
        "        surrogate_grad = (sigmoid * (1.0 - sigmoid) * self.temp).detach()\n",
        "        spike.register_hook(lambda grad: grad * surrogate_grad)\n",
        "        return spike\n",
        "\n",
        "class SpikingMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.spike = SpikingNeuronCell()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        return self.spike(attn_output)\n",
        "\n",
        "def positional_encoding(seq_len, d_model, device):\n",
        "    pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    i = torch.arange(d_model // 2, dtype=torch.float32, device=device)\n",
        "    angle_rates = 1 / (10000 ** (2 * i / d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    sin = torch.sin(angle_rads)\n",
        "    cos = torch.cos(angle_rads)\n",
        "    pos_encoding = torch.cat([sin, cos], dim=-1)\n",
        "    return pos_encoding.unsqueeze(0)\n",
        "\n",
        "class SpiTranNet(nn.Module):\n",
        "    def __init__(self, input_channels=22, input_length=1000, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.input_length = input_length\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 64, kernel_size=7, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Conv1d(128, 128, kernel_size=7, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.transformer_dim = 128\n",
        "        self.attn = SpikingMultiHeadAttention(embed_dim=self.transformer_dim, num_heads=2)\n",
        "        self.norm1 = nn.LayerNorm(self.transformer_dim)\n",
        "        self.norm2 = nn.LayerNorm(self.transformer_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(self.transformer_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            SpikingNeuronCell(),\n",
        "            nn.Linear(128, self.transformer_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.seq_len = input_length // 64\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.seq_len * self.transformer_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        pos_enc = positional_encoding(x.size(1), x.size(2), x.device)\n",
        "        x = x + pos_enc\n",
        "        residual = x\n",
        "        x = self.attn(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm2(x + residual)\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# -------------------\n",
        "# Cell 3: Data Loading & Preprocessing  -- unchanged\n",
        "sr = 250\n",
        "input_length = 1000\n",
        "input_channels = 22\n",
        "num_classes = 2\n",
        "\n",
        "def load_dataframe(subject_id):\n",
        "    dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])\n",
        "    low_cut_hz = 8.\n",
        "    high_cut_hz = 30.\n",
        "    factor = 1e6\n",
        "    factor_new = 1e-3\n",
        "    init_block_size = 1000\n",
        "    preprocessors = [\n",
        "        Preprocessor('pick_types', eeg=True, meg=False, stim=False),\n",
        "        Preprocessor(lambda data: data * factor),\n",
        "        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
        "        Preprocessor(exponential_moving_standardize, factor_new=factor_new, init_block_size=init_block_size),\n",
        "        SetEEGReference()\n",
        "    ]\n",
        "    preprocess(dataset, preprocessors, n_jobs=-1)\n",
        "    windows_dataset = create_windows_from_events(dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0, preload=True)\n",
        "    XYSet = [windows_dataset.split('session')['0train']]\n",
        "    XYSet_ = [windows_dataset.split('session')['1test']]\n",
        "    return XYSet, XYSet_\n",
        "\n",
        "def extract(XYSet, selected_classes=[0, 1]):\n",
        "    X, Y = [], []\n",
        "    for ds in XYSet:\n",
        "        for window in ds:\n",
        "            x = window[0]\n",
        "            y = window[1]\n",
        "            if isinstance(y, list):\n",
        "                y = y[0]\n",
        "            if int(y) in selected_classes:\n",
        "                X.append(x)\n",
        "                Y.append(int(y))\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def fp(x, y, device):\n",
        "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "    return x, y\n",
        "\n",
        "# -------------------\n",
        "# Cell 4: Metrics and Evaluation  -- unchanged compute functions\n",
        "def evaluate_with_probs(model, loader):\n",
        "    model.eval()\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "    y_prob_list = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            outputs = model(Xb)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            y_true_list.extend(yb.cpu().numpy().tolist())\n",
        "            y_pred_list.extend(preds.cpu().numpy().tolist())\n",
        "            if probs.size(1) >= 2:\n",
        "                pos_probs = probs[:, 1].cpu().numpy().tolist()\n",
        "            else:\n",
        "                pos_probs = torch.sigmoid(outputs[:, 0]).cpu().numpy().tolist()\n",
        "            y_prob_list.extend(pos_probs)\n",
        "    return y_true_list, y_pred_list, y_prob_list\n",
        "\n",
        "def compute_metrics_from_preds(y_true, y_pred, y_prob=None) -> Dict:\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.size == 4:\n",
        "        tn, fp_, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = int(cm[0, 0]) if cm.shape[0] > 0 and cm.shape[1] > 0 else 0\n",
        "        fp_ = int(cm[0, 1]) if cm.shape[0] > 0 and cm.shape[1] > 1 else 0\n",
        "        fn = int(cm[1, 0]) if cm.shape[0] > 1 and cm.shape[1] > 0 else 0\n",
        "        tp = int(cm[1, 1]) if cm.shape[0] > 1 and cm.shape[1] > 1 else 0\n",
        "    spec = tn / (tn + fp_) if (tn + fp_) > 0 else 0.0\n",
        "    results = {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1_score\": float(f1),\n",
        "        \"kappa\": float(kappa),\n",
        "        \"specificity\": float(spec),\n",
        "        \"tn\": int(tn),\n",
        "        \"fp\": int(fp_),\n",
        "        \"fn\": int(fn),\n",
        "        \"tp\": int(tp)\n",
        "    }\n",
        "    if y_prob is not None and len(set(y_true)) > 1:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        results[\"roc_auc\"] = float(roc_auc)\n",
        "        results[\"fpr\"] = fpr.tolist()\n",
        "        results[\"tpr\"] = tpr.tolist()\n",
        "    else:\n",
        "        results[\"roc_auc\"] = None\n",
        "        results[\"fpr\"] = []\n",
        "        results[\"tpr\"] = []\n",
        "    return results\n",
        "\n",
        "# -------------------\n",
        "# Cell 5: Plotting / Saving functions (enhanced)\n",
        "def save_png_pdf(fig, out_path_base):\n",
        "    \"\"\"Save figure in PNG and PDF with high resolution.\"\"\"\n",
        "    png_path = out_path_base + \".png\"\n",
        "    pdf_path = out_path_base + \".pdf\"\n",
        "    fig.savefig(png_path, dpi=600, bbox_inches=\"tight\")\n",
        "    try:\n",
        "        fig.savefig(pdf_path, dpi=600, bbox_inches=\"tight\")\n",
        "    except Exception as e:\n",
        "        # PDF save may fail in some environments, but continue\n",
        "        print(\"Warning: saving PDF failed:\", e)\n",
        "\n",
        "\n",
        "def save_confusion_matrix(y_true, y_pred, save_path, title=\"Confusion Matrix\", normalize=False):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    num_classes = cm.shape[0]\n",
        "    fig_size = max(6, num_classes * 2)  # larger to ensure all rows visible\n",
        "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
        "\n",
        "    # Draw heatmap without annotations\n",
        "    sns.heatmap(cm, annot=False, cmap=\"Blues\", cbar=True, square=True, ax=ax)\n",
        "\n",
        "    # Add numbers manually (centered)\n",
        "    for i in range(num_classes):\n",
        "        for j in range(num_classes):\n",
        "            val = cm[i, j]\n",
        "            display_val = f\"{val:.2f}\" if normalize else f\"{int(val)}\"\n",
        "            ax.text(j + 0.5, i + 0.5, display_val,\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > cm.max()/2 else \"black\",\n",
        "                    fontsize=12, fontweight='bold')\n",
        "\n",
        "    ax.set_xlabel(\"Predicted label\", fontsize=12)\n",
        "    ax.set_ylabel(\"True label\", fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
        "    ax.set_xticks(np.arange(num_classes)+0.5)\n",
        "    ax.set_yticks(np.arange(num_classes)+0.5)\n",
        "    ax.set_xticklabels(range(num_classes))\n",
        "    ax.set_yticklabels(range(num_classes), rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_png_pdf(fig, save_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_epoch_history_excel(history: Dict, out_dir: str, filename_base=\"epoch_history\"):\n",
        "    # history is expected to be dict: keys -> lists of same length\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame(history)\n",
        "    csv_path = os.path.join(out_dir, filename_base + \".csv\")\n",
        "    df.to_csv(csv_path, index_label=\"epoch\")\n",
        "    # Save Excel (sheet)\n",
        "    xlsx_path = os.path.join(out_dir, filename_base + \".xlsx\")\n",
        "    try:\n",
        "        with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
        "            df.to_excel(writer, sheet_name=\"history\", index_label=\"epoch\")\n",
        "    except Exception as e:\n",
        "        # still ok if to_excel fails\n",
        "        print(\"Warning: to_excel failed:\", e)\n",
        "    return csv_path, xlsx_path\n",
        "\n",
        "def save_loss_plot(history: Dict, out_path_base: str, title=\"Training & Validation Loss\"):\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    epochs = range(1, len(history.get(\"loss\", [])) + 1)\n",
        "    if history.get(\"loss\", None):\n",
        "        ax.plot(epochs, history[\"loss\"], label=\"Training Loss\", linewidth=2)\n",
        "    if history.get(\"val_loss\", None):\n",
        "        ax.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    save_png_pdf(fig, out_path_base)\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_acc_plot(history: Dict, out_path_base: str, title=\"Training & Validation Accuracy\"):\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    epochs = range(1, len(history.get(\"train_acc\", [])) + 1)\n",
        "    if history.get(\"train_acc\", None):\n",
        "        ax.plot(epochs, history[\"train_acc\"], label=\"Training Accuracy\", linewidth=2)\n",
        "    if history.get(\"val_acc\", None):\n",
        "        ax.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "    ax.set_ylim(0,1.0)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    save_png_pdf(fig, out_path_base)\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_kappa_plot(history: Dict, out_path_base: str, title=\"Training & Validation Kappa\"):\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    epochs = range(1, len(history.get(\"train_kappa\", [])) + 1)\n",
        "    if history.get(\"train_kappa\", None):\n",
        "        ax.plot(epochs, history[\"train_kappa\"], label=\"Training Kappa\", linewidth=2)\n",
        "    if history.get(\"val_kappa\", None):\n",
        "        ax.plot(epochs, history[\"val_kappa\"], label=\"Validation Kappa\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Cohen's Kappa\")\n",
        "    ax.set_ylim(-1,1)\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    save_png_pdf(fig, out_path_base)\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_roc_curve(y_true, y_prob, out_path_base: str, title=\"ROC Curve\"):\n",
        "    # Only if there are at least two classes\n",
        "    if y_prob is None or len(set(y_true)) < 2:\n",
        "        print(\"ROC not saved: insufficient classes or missing probabilities\")\n",
        "        return None\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}', linewidth=2)\n",
        "    ax.plot([0,1],[0,1],'--', color='gray', linewidth=1)\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    save_png_pdf(fig, out_path_base)\n",
        "    plt.close(fig)\n",
        "    return {\"fpr\": fpr.tolist(), \"tpr\": tpr.tolist(), \"roc_auc\": float(roc_auc)}\n",
        "\n",
        "def save_metrics_json(metrics: Dict, out_path):\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=4)\n",
        "\n",
        "def save_combined_loss_acc(train_loss, val_loss, train_acc, val_acc, out_path):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot Loss\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(train_loss, label=\"Train Loss\")\n",
        "    plt.plot(val_loss, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(train_acc, label=\"Train Accuracy\")\n",
        "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path + \".png\", dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def save_combined_kappa(train_kappa, val_kappa, out_path):\n",
        "    \"\"\"\n",
        "    Save a combined plot of training vs validation Cohen's kappa.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_kappa : list or array\n",
        "        Training kappa values per epoch\n",
        "    val_kappa : list or array\n",
        "        Validation kappa values per epoch\n",
        "    out_path : str\n",
        "        Path prefix to save the plot (without extension)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_kappa, label=\"Train Kappa\", linewidth=2)\n",
        "    plt.plot(val_kappa, label=\"Validation Kappa\", linewidth=2, linestyle=\"--\")\n",
        "    plt.xlabel(\"Epoch\", fontsize=12)\n",
        "    plt.ylabel(\"Cohen's Kappa\", fontsize=12)\n",
        "    plt.title(\"Training vs Validation Kappa\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path + \".png\", dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_summary_table(metrics_dict, out_path, title=\"Summary Metrics Table\"):\n",
        "    \"\"\"\n",
        "    Save a summary table as a PNG image with clear formatting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    metrics_dict : dict\n",
        "        Dictionary of metrics, e.g. { \"Subject1\": {\"Acc\":0.9, \"F1\":0.8, ...}, ... }\n",
        "    out_path : str\n",
        "        Path prefix to save PNG (without extension)\n",
        "    title : str\n",
        "        Title above the table\n",
        "    \"\"\"\n",
        "    # Convert dictionary to DataFrame\n",
        "    df = pd.DataFrame(metrics_dict).T  # subjects as rows\n",
        "    df = df.round(4)  # round to 4 decimals for clarity\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(8, len(df.columns)*1.2), max(4, len(df)*0.5 + 2)))\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Title\n",
        "    plt.title(title, fontsize=14, fontweight=\"bold\", pad=20)\n",
        "\n",
        "    # Create table\n",
        "    table = ax.table(cellText=df.values,\n",
        "                     rowLabels=df.index,\n",
        "                     colLabels=df.columns,\n",
        "                     cellLoc='center',\n",
        "                     rowLoc='center',\n",
        "                     loc='center')\n",
        "\n",
        "    # Formatting\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1.2, 1.2)  # make table bigger\n",
        "\n",
        "    # Style header cells\n",
        "    for (row, col), cell in table.get_celld().items():\n",
        "        cell.set_linewidth(1)\n",
        "        if row == 0:  # header row\n",
        "            cell.set_text_props(weight='bold', color=\"white\")\n",
        "            cell.set_facecolor(\"#4B8BBE\")\n",
        "        elif col == -1:  # row index\n",
        "            cell.set_text_props(weight='bold')\n",
        "            cell.set_facecolor(\"#f0f0f0\")\n",
        "        else:\n",
        "            cell.set_facecolor(\"white\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path + \".png\", dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# Cell 6: Training function (unchanged) - returns best_metrics, history, (y_true_final, y_pred_final, y_prob_final)\n",
        "def train_model_with_history(model, train_loader, test_loader, num_epochs=100, lr=1e-4, device=\"cuda\",\n",
        "                             out_dir=\"results_subject\", checkpoint_name=\"best_model.pth\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    best_acc = -1.0\n",
        "    best_metrics = None\n",
        "    history = {\"loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"train_kappa\": [], \"val_kappa\": []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        n_samples = 0\n",
        "        y_true_train_epoch = []\n",
        "        y_pred_train_epoch = []\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += float(loss.item()) * X_batch.size(0)\n",
        "            n_samples += X_batch.size(0)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            y_true_train_epoch.extend(y_batch.cpu().numpy())\n",
        "            y_pred_train_epoch.extend(preds.cpu().numpy())\n",
        "        epoch_loss = running_loss / (n_samples if n_samples > 0 else 1.0)\n",
        "        train_metrics_epoch = compute_metrics_from_preds(y_true_train_epoch, y_pred_train_epoch)\n",
        "        train_acc_epoch = train_metrics_epoch[\"accuracy\"]\n",
        "        train_kappa_epoch = train_metrics_epoch[\"kappa\"]\n",
        "\n",
        "        # Validation evaluation\n",
        "        y_true_val, y_pred_val, y_prob_val = evaluate_with_probs(model, test_loader)\n",
        "        metrics_val = compute_metrics_from_preds(y_true_val, y_pred_val, y_prob_val)\n",
        "        val_acc_epoch = metrics_val[\"accuracy\"]\n",
        "        val_kappa_epoch = metrics_val[\"kappa\"]\n",
        "\n",
        "        # Compute validation loss\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += float(loss.item()) * X_batch.size(0)\n",
        "                val_samples += X_batch.size(0)\n",
        "        val_loss = val_loss / (val_samples if val_samples > 0 else 1.0)\n",
        "\n",
        "        history[\"loss\"].append(epoch_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc_epoch)\n",
        "        history[\"val_acc\"].append(val_acc_epoch)\n",
        "        history[\"train_kappa\"].append(train_kappa_epoch)\n",
        "        history[\"val_kappa\"].append(val_kappa_epoch)\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{num_epochs}] loss={epoch_loss:.4f} val_loss={val_loss:.4f} train_acc={train_acc_epoch:.4f} val_acc={val_acc_epoch:.4f} train_kappa={train_kappa_epoch:.4f} val_kappa={val_kappa_epoch:.4f}\")\n",
        "\n",
        "        if val_acc_epoch > best_acc:\n",
        "            best_acc = val_acc_epoch\n",
        "            best_metrics = metrics_val\n",
        "            torch.save(model.state_dict(), os.path.join(out_dir, checkpoint_name))\n",
        "\n",
        "    ckpt_path = os.path.join(out_dir, checkpoint_name)\n",
        "    if os.path.exists(ckpt_path):\n",
        "        state = torch.load(ckpt_path, map_location=next(model.parameters()).device)\n",
        "        try:\n",
        "            model.load_state_dict(state)\n",
        "        except Exception as e:\n",
        "            print(\"Warning: could not load saved state_dict:\", e)\n",
        "\n",
        "    y_true_final, y_pred_final, y_prob_final = evaluate_with_probs(model, test_loader)\n",
        "    final_metrics = compute_metrics_from_preds(y_true_final, y_pred_final, y_prob_final)\n",
        "    return best_metrics, history, (y_true_final, y_pred_final, y_prob_final)\n",
        "\n",
        "# -------------------\n",
        "# Cell 8: Main function for per-subject and all-subject metrics (enhanced plotting & combined outputs)\n",
        "# -------------------\n",
        "# Helper: Save summary table as PNG (clean formatting)\n",
        "\n",
        "# def save_summary_table_png(df, save_path, title=\"Summary Metrics per Subject\"):\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import numpy as np\n",
        "\n",
        "#     # Round numeric columns to 4 digits\n",
        "#     df_display = df.copy()\n",
        "#     for col in df_display.columns:\n",
        "#         if np.issubdtype(df_display[col].dtype, np.number):\n",
        "#             df_display[col] = df_display[col].round(4)\n",
        "\n",
        "#     n_rows, n_cols = df_display.shape\n",
        "#     fig_height = max(2, 0.6 * n_rows)\n",
        "#     fig_width = max(8, 1.5 * n_cols)\n",
        "#     fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
        "#     ax.axis('off')\n",
        "\n",
        "#     # Create table\n",
        "#     tbl = ax.table(cellText=df_display.values,\n",
        "#                    colLabels=df_display.columns,\n",
        "#                    cellLoc='center',  # center text in all cells\n",
        "#                    loc='center')\n",
        "\n",
        "#     tbl.auto_set_font_size(False)\n",
        "#     tbl.set_fontsize(10)\n",
        "#     tbl.scale(1.2, 1.2)  # scale width and height of cells\n",
        "\n",
        "#     # Make header bold\n",
        "#     for (i, j), cell in tbl.get_celld().items():\n",
        "#         if i == 0:  # header row\n",
        "#             cell.set_text_props(weight='bold', ha='center', va='center')\n",
        "#         else:\n",
        "#             cell.set_text_props(ha='center', va='center')\n",
        "\n",
        "#     ax.set_title(title, fontsize=14, pad=20)\n",
        "#     plt.tight_layout()\n",
        "#     fig.savefig(save_path + \".png\", dpi=300)\n",
        "#     plt.close(fig)\n",
        "\n",
        "\n",
        "def save_combined_subject_training_curves_simple(df_combined_stats, out_dir=\"results_combined\"):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    epochs = np.arange(1, df_combined_stats.shape[0]+1)\n",
        "\n",
        "    # Loss\n",
        "    fig, ax = plt.subplots(figsize=(9,5))\n",
        "    if \"loss_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"loss_mean\"], label=\"Train Loss\", linewidth=2)\n",
        "    if \"val_loss_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"val_loss_mean\"], label=\"Val Loss\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.set_title(\"Train & Validation Loss (Combined Subjects)\")\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(os.path.join(out_dir, \"combined_loss.png\"), dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Accuracy\n",
        "    fig, ax = plt.subplots(figsize=(9,5))\n",
        "    if \"train_acc_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"train_acc_mean\"], label=\"Train Accuracy\", linewidth=2)\n",
        "    if \"val_acc_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"val_acc_mean\"], label=\"Validation Accuracy\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "    ax.set_ylim(0,1.0)\n",
        "    ax.set_title(\"Train & Validation Accuracy (Combined Subjects)\")\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(os.path.join(out_dir, \"combined_accuracy.png\"), dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Kappa\n",
        "    fig, ax = plt.subplots(figsize=(9,5))\n",
        "    if \"train_kappa_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"train_kappa_mean\"], label=\"Train Kappa\", linewidth=2)\n",
        "    if \"val_kappa_mean\" in df_combined_stats.columns:\n",
        "        ax.plot(epochs, df_combined_stats[\"val_kappa_mean\"], label=\"Validation Kappa\", linewidth=2)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Cohen's Kappa\")\n",
        "    ax.set_ylim(-1,1)\n",
        "    ax.set_title(\"Train & Validation Kappa (Combined Subjects)\")\n",
        "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(os.path.join(out_dir, \"combined_kappa.png\"), dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "# Cell 8: Main function for per-subject and all-subject metrics (enhanced plotting & combined outputs)\n",
        "def run_all_subjects(subject_ids, num_epochs=50, batch_size=32, lr=1e-4, device=\"cuda\"):\n",
        "    all_subject_metrics = []\n",
        "    all_subject_y_true = []\n",
        "    all_subject_y_pred = []\n",
        "    all_subject_y_prob = []\n",
        "\n",
        "    # For combined plotting across histories\n",
        "    histories_by_subject = {}  # subj_id -> history dict\n",
        "    final_metrics_by_subject = {}  # subj_id -> final_metrics\n",
        "    train_vs_test_acc = []  # list of dicts {subject, train_acc_final_epoch, test_acc}\n",
        "    roc_records = []  # list of (subject_id, fpr, tpr, auc)\n",
        "    summary_rows = []\n",
        "\n",
        "    for subj_id in subject_ids:\n",
        "        print(f\"\\n=== Processing Subject {subj_id} ===\\n\")\n",
        "        out_dir = f\"results_subject_{subj_id}\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "        # Load data\n",
        "        XY_train, XY_test = load_dataframe(subj_id)\n",
        "        X_train, y_train = extract(XY_train)\n",
        "        X_test, y_test = extract(XY_test)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_train_t, y_train_t = fp(X_train, y_train, device)\n",
        "        X_test_t, y_test_t = fp(X_test, y_test, device)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "        test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        model = SpiTranNet(input_channels=X_train.shape[1], input_length=X_train.shape[2], num_classes=num_classes)\n",
        "        model.to(device)\n",
        "\n",
        "        # Train model with history\n",
        "        best_metrics, history, (y_true_final, y_pred_final, y_prob_final) = train_model_with_history(\n",
        "            model, train_loader, test_loader, num_epochs=num_epochs, lr=lr, device=device, out_dir=out_dir\n",
        "        )\n",
        "\n",
        "        # Save epoch history Excel and CSV\n",
        "        csv_path, xlsx_path = save_epoch_history_excel(history, out_dir, filename_base=\"epoch_history\")\n",
        "\n",
        "        # Save separate plots: loss, acc, kappa\n",
        "        save_loss_plot(history, os.path.join(out_dir, \"loss\"))\n",
        "        save_acc_plot(history, os.path.join(out_dir, \"accuracy\"))\n",
        "        save_kappa_plot(history, os.path.join(out_dir, \"kappa\"))\n",
        "\n",
        "        # Save combined loss+acc (keep original combined layout for compatibility)\n",
        "        save_combined_loss_acc(history[\"loss\"], history[\"val_loss\"], history[\"train_acc\"], history[\"val_acc\"], os.path.join(out_dir, \"loss_acc\"))\n",
        "        save_combined_kappa(history[\"train_kappa\"], history[\"val_kappa\"], os.path.join(out_dir, \"kappa_combined\"))\n",
        "\n",
        "        # Save confusion matrix (final test)\n",
        "        save_confusion_matrix(y_true_final, y_pred_final, os.path.join(out_dir, \"confusion_matrix\"), title=f\"Confusion Matrix Subject {subj_id}\")\n",
        "\n",
        "        # Save ROC curve (using final test y_true_final / y_prob_final)\n",
        "        roc_info = save_roc_curve(y_true_final, y_prob_final, os.path.join(out_dir, \"roc_curve\"), title=f\"ROC Curve Subject {subj_id}\")\n",
        "        if roc_info is not None:\n",
        "            roc_records.append((subj_id, roc_info[\"fpr\"], roc_info[\"tpr\"], roc_info[\"roc_auc\"]))\n",
        "\n",
        "        # Save summary metrics (final test) as JSON & CSV\n",
        "        final_metrics = compute_metrics_from_preds(y_true_final, y_pred_final, y_prob_final)\n",
        "        save_metrics_json(final_metrics, os.path.join(out_dir, f\"summary_metrics_final_subject_{subj_id}.json\"))\n",
        "        pd.DataFrame([final_metrics]).to_csv(os.path.join(out_dir, f\"summary_metrics_final_subject_{subj_id}.csv\"), index=False)\n",
        "\n",
        "        # Store metrics\n",
        "        all_subject_metrics.append(final_metrics)\n",
        "        all_subject_y_true.extend(y_true_final)\n",
        "        all_subject_y_pred.extend(y_pred_final)\n",
        "        all_subject_y_prob.extend(y_prob_final)\n",
        "\n",
        "        # Keep history and final metrics for combined plotting\n",
        "        histories_by_subject[subj_id] = history\n",
        "        final_metrics_by_subject[subj_id] = final_metrics\n",
        "\n",
        "        # Train-final epoch accuracy (from history) and test accuracy\n",
        "        train_acc_final_epoch = history[\"train_acc\"][-1] if len(history[\"train_acc\"])>0 else None\n",
        "        test_acc = final_metrics[\"accuracy\"]\n",
        "        train_vs_test_acc.append({\"subject\": subj_id, \"train_final_acc\": train_acc_final_epoch, \"test_acc\": test_acc})\n",
        "\n",
        "        # Summary table row\n",
        "        row = {\"subject\": subj_id}\n",
        "        for k,v in final_metrics.items():\n",
        "            row[k] = v\n",
        "        summary_rows.append(row)\n",
        "\n",
        "    # -------------------\n",
        "    # Compute combined metrics for all subjects\n",
        "    print(\"\\n=== Computing Combined Metrics Across All Subjects ===\\n\")\n",
        "    combined_metrics = compute_metrics_from_preds(all_subject_y_true, all_subject_y_pred, all_subject_y_prob)\n",
        "    combined_dir = \"results_combined\"\n",
        "    os.makedirs(combined_dir, exist_ok=True)\n",
        "\n",
        "    # Save combined metrics CSV/JSON\n",
        "    pd.DataFrame([combined_metrics]).to_csv(os.path.join(combined_dir, \"metrics_combined.csv\"), index=False)\n",
        "    with open(os.path.join(combined_dir, \"metrics_combined.json\"), \"w\") as f:\n",
        "        json.dump(combined_metrics, f, indent=4)\n",
        "\n",
        "    # Combined confusion matrix\n",
        "    save_confusion_matrix(all_subject_y_true, all_subject_y_pred, os.path.join(combined_dir, \"confusion_matrix_combined\"), title=\"Combined Confusion Matrix\")\n",
        "\n",
        "    # Combined ROC plot\n",
        "    roc_info_combined = None\n",
        "    if combined_metrics[\"roc_auc\"] is not None:\n",
        "        roc_info_combined = save_roc_curve(all_subject_y_true, all_subject_y_prob, os.path.join(combined_dir, \"roc_curve_combined\"), title=\"Combined ROC Curve\")\n",
        "\n",
        "    # -------------------\n",
        "    # Save clean summary table PNG + CSV/Excel\n",
        "    summary_df = pd.DataFrame(summary_rows).sort_values(\"subject\")\n",
        "    combined_row = {\"subject\": \"combined\"}\n",
        "    for k,v in combined_metrics.items():\n",
        "        combined_row[k] = v\n",
        "    summary_df_with_combined = pd.concat([summary_df, pd.DataFrame([combined_row])], ignore_index=True, sort=False)\n",
        "\n",
        "    # CSV & Excel\n",
        "    summary_df_with_combined.to_csv(os.path.join(combined_dir, \"summary_table_all_subjects.csv\"), index=False)\n",
        "    try:\n",
        "        with pd.ExcelWriter(os.path.join(combined_dir, \"summary_table_all_subjects.xlsx\"), engine=\"openpyxl\") as writer:\n",
        "            summary_df_with_combined.to_excel(writer, sheet_name=\"summary\", index=False)\n",
        "    except Exception as e:\n",
        "        print(\"Warning: to_excel for summary table failed:\", e)\n",
        "\n",
        "    # # PNG (clean)\n",
        "    # try:\n",
        "    #     save_summary_table_png(summary_df_with_combined, os.path.join(combined_dir, \"summary_table_all_subjects\"),\n",
        "    #                            title=\"Summary metrics per subject and combined\")\n",
        "    # except Exception as e:\n",
        "    #     print(\"Warning: saving PNG of summary table failed:\", e)\n",
        "\n",
        "    # -------------------\n",
        "    # -------------------\n",
        "    # Save combined training curves for all subjects (train/val loss, acc, kappa)\n",
        "    metrics = [\"loss\",\"val_loss\",\"train_acc\",\"val_acc\",\"train_kappa\",\"val_kappa\"]\n",
        "    max_epochs = max([len(h.get(\"loss\",[])) for h in histories_by_subject.values()]) if len(histories_by_subject)>0 else 0\n",
        "    combined_stats = {}\n",
        "    for m in metrics:\n",
        "        stacked = []\n",
        "        for subj_id, hist in histories_by_subject.items():\n",
        "            arr = hist.get(m, [])\n",
        "            arr_padded = arr + [np.nan]*(max_epochs - len(arr))\n",
        "            stacked.append(arr_padded)\n",
        "        if len(stacked) > 0:\n",
        "            stacked_arr = np.vstack(stacked)\n",
        "            mean = np.nanmean(stacked_arr, axis=0)\n",
        "            std = np.nanstd(stacked_arr, axis=0)\n",
        "        else:\n",
        "            mean = []\n",
        "            std = []\n",
        "        combined_stats[m + \"_mean\"] = mean\n",
        "        combined_stats[m + \"_std\"] = std\n",
        "    df_combined_stats = pd.DataFrame(combined_stats)\n",
        "\n",
        "    # Save the combined training curves\n",
        "    save_combined_subject_training_curves_simple(df_combined_stats, out_dir=combined_dir)\n",
        "\n",
        "    # -------------------\n",
        "    # Additional aggregated plots (multi-subject ROC, boxplot, train vs test acc) omitted for brevity\n",
        "    # (they remain the same as your previous enhanced code)\n",
        "\n",
        "    return all_subject_metrics, combined_metrics\n",
        "\n",
        "# -------------------\n",
        "# Cell 9: Run the pipeline\n",
        "subject_ids = [1,2,3,4,5,6,7,8,9]  # example, replace with your actual subject IDs\n",
        "all_subject_metrics, combined_metrics = run_all_subjects(subject_ids, num_epochs=100, batch_size=32, lr=1e-4, device=\"cuda\")\n",
        "print(\"\\nPer-subject metrics:\", all_subject_metrics)\n",
        "print(\"\\nCombined metrics:\", combined_metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8898c3a5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
